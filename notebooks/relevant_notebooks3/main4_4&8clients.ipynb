{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check for CUDA GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "dataset_train = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define superclasses and subclasses\n",
    "superclasses = {\n",
    "    0: [4, 30, 55, 72, 95],  # aquatic mammals\n",
    "    1: [1, 32, 67, 73, 91],  # fish\n",
    "    2: [54, 62, 70, 82, 92],  # flowers\n",
    "    3: [9, 10, 16, 28, 61],  # food containers\n",
    "    4: [0, 51, 53, 57, 83],  # fruit and vegetables\n",
    "    5: [22, 39, 40, 86, 87],  # household electrical devices\n",
    "    6: [5, 20, 25, 84, 94],  # household furniture\n",
    "    7: [6, 7, 14, 18, 24],  # insects\n",
    "    8: [3, 42, 43, 88, 97],  # large carnivores\n",
    "    9: [12, 17, 37, 68, 76],  # large man-made outdoor things\n",
    "    10: [23, 33, 49, 60, 71],  # large natural outdoor scenes\n",
    "    11: [15, 19, 21, 31, 38],  # medium-sized mammals\n",
    "    12: [34, 63, 64, 66, 75],  # non-insect invertebrates\n",
    "    13: [26, 45, 77, 79, 99],  # people\n",
    "    14: [2, 11, 35, 46, 98],  # reptiles\n",
    "    15: [27, 29, 44, 78, 93],  # small mammals\n",
    "    16: [36, 50, 65, 74, 80],  # trees\n",
    "    17: [8, 13, 48, 58, 90],  # vehicles 1\n",
    "    18: [41, 66, 69, 81, 89],  # vehicles 2\n",
    "    19: [47, 50, 52, 56, 59],  # household furniture\n",
    "}\n",
    "\n",
    "# Add missing subclasses\n",
    "superclasses[16].append(96)  # Add \"willow tree\" to the trees superclass\n",
    "superclasses[3].append(85)   # Add \"plate\" to the food containers superclass\n",
    "\n",
    "# Function to map subclass to its corresponding superclass\n",
    "def get_superclass(subclass, superclasses):\n",
    "    for superclass in superclasses.keys():\n",
    "        subclasses=superclasses[superclass]\n",
    "        if subclass in subclasses:\n",
    "            return superclass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing for 4 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset, DataLoader, random_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to randomly split dataset by superclass for 4 clients\n",
    "# def filter_dataset_randomly_by_superclass_4_clients(dataset, superclasses):\n",
    "#     client_indices = [[] for _ in range(4)]  # Create an empty list for each client (4 clients)\n",
    "\n",
    "#     # Shuffle and split images randomly for each superclass\n",
    "#     for subclasses in superclasses.values():\n",
    "#         superclass_indices = []\n",
    "        \n",
    "#         # Gather all images belonging to this superclass\n",
    "#         for idx, (data, target) in enumerate(dataset):\n",
    "#             if target in subclasses:\n",
    "#                 superclass_indices.append(idx)\n",
    "        \n",
    "#         # If no images found for this superclass, continue to the next\n",
    "#         if not superclass_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # Shuffle the indices for randomness\n",
    "#         np.random.shuffle(superclass_indices)\n",
    "        \n",
    "#         # Split the indices between the 4 clients\n",
    "#         split_size = len(superclass_indices) // 4\n",
    "#         for i in range(4):\n",
    "#             start = i * split_size\n",
    "#             end = (i + 1) * split_size if i < 3 else len(superclass_indices)  # Ensure last client gets the remaining data\n",
    "#             client_indices[i].extend(superclass_indices[start:end])\n",
    "    \n",
    "#     # Return 4 subsets: one for each client\n",
    "#     return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "# def get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses):\n",
    "#     # Step 1: Split the dataset for 4 clients\n",
    "#     client_datasets = filter_dataset_randomly_by_superclass_4_clients(dataset_train, superclasses)\n",
    "\n",
    "#     # Step 2: Create validation splits for each client\n",
    "#     val_split = 0.1  # 10% for validation\n",
    "\n",
    "#     # Create training and validation splits for each client\n",
    "#     train_val_splits = [random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)]) for client_dataset in client_datasets]\n",
    "\n",
    "#     # Separate train and validation datasets\n",
    "#     train_datasets = [split[0] for split in train_val_splits]\n",
    "#     val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "#     # Step 3: Create DataLoaders for each client\n",
    "#     client_loaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for train_dataset in train_datasets]\n",
    "#     val_loaders = [DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) for val_dataset in val_datasets]\n",
    "#     test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#     return client_loaders, val_loaders, test_loader\n",
    "\n",
    "# # Parameters\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "\n",
    "# # Call the function to initialize DataLoaders for 4 clients\n",
    "# client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing for 2 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset, DataLoader, random_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to randomly split dataset by superclass for 2 clients\n",
    "# def filter_dataset_randomly_by_superclass_2_clients(dataset, superclasses):\n",
    "#     client_indices = [[] for _ in range(2)]\n",
    "    \n",
    "#     for subclasses in superclasses.values():\n",
    "#         superclass_indices = []\n",
    "        \n",
    "#         # Collect indices for samples in the current superclass\n",
    "#         for idx, (data, target) in enumerate(dataset):\n",
    "#             if target in subclasses:\n",
    "#                 # Map to superclass and assign label\n",
    "#                 superclass_label = get_superclass(target, superclasses)\n",
    "#                 if superclass_label is not None:\n",
    "#                     dataset.targets[idx] = superclass_label  # Remap the label\n",
    "#                     superclass_indices.append(idx)\n",
    "        \n",
    "#         # Split indices between the 2 clients\n",
    "#         np.random.shuffle(superclass_indices)\n",
    "#         split_size = len(superclass_indices) // 2\n",
    "#         for i in range(2):\n",
    "#             client_indices[i].extend(superclass_indices[i * split_size:(i + 1) * split_size])\n",
    "    \n",
    "#     return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "\n",
    "# def get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses):\n",
    "#     # Step 1: Split the dataset for 2 clients\n",
    "#     client_datasets = filter_dataset_randomly_by_superclass_2_clients(dataset_train, superclasses)\n",
    "\n",
    "#     # Step 2: Create validation splits for each client\n",
    "#     val_split = 0.1  # 10% for validation\n",
    "\n",
    "#     # Create training and validation splits for each client\n",
    "#     train_val_splits = [random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)]) for client_dataset in client_datasets]\n",
    "\n",
    "#     # Separate train and validation datasets\n",
    "#     train_datasets = [split[0] for split in train_val_splits]\n",
    "#     val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "#     # Step 3: Create DataLoaders for each client\n",
    "#     client_loaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for train_dataset in train_datasets]\n",
    "#     val_loaders = [DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) for val_dataset in val_datasets]\n",
    "#     test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#     return client_loaders, val_loaders, test_loader\n",
    "\n",
    "# # Parameters\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "\n",
    "# # Call the function to initialize DataLoaders for 2 clients\n",
    "# client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Updated function to map CIFAR-100 subclasses to superclasses consistently across all data splits\n",
    "def filter_dataset_by_superclass(dataset, superclasses):\n",
    "    # Remap labels to superclasses\n",
    "    for idx, (_, target) in enumerate(dataset):\n",
    "        # Map CIFAR-100 label to its superclass\n",
    "        superclass_label = get_superclass(target, superclasses)\n",
    "        if superclass_label is not None:\n",
    "            dataset.targets[idx] = superclass_label  # Update target label to superclass\n",
    "\n",
    "    # Randomly split dataset indices for two clients\n",
    "    client_indices = [[] for _ in range(2)]\n",
    "    superclass_indices = [[] for _ in range(len(superclasses))]  # Separate by superclass for balanced splits\n",
    "\n",
    "    for superclass, subclasses in superclasses.items():\n",
    "        for idx, target in enumerate(dataset.targets):\n",
    "            if target == superclass:\n",
    "                superclass_indices[superclass - 1].append(idx)  # Group indices by superclass\n",
    "\n",
    "    # Split each superclass evenly between the two clients\n",
    "    for indices in superclass_indices:\n",
    "        np.random.shuffle(indices)\n",
    "        half = len(indices) // 2\n",
    "        client_indices[0].extend(indices[:half])\n",
    "        client_indices[1].extend(indices[half:])\n",
    "\n",
    "    return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "# Updated data loader function to apply consistent label mapping across train, validation, and test sets\n",
    "def get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses):\n",
    "    # Step 1: Map dataset_train labels to superclasses and split for 2 clients\n",
    "    client_datasets = filter_dataset_by_superclass(dataset_train, superclasses)\n",
    "\n",
    "    # Step 2: Create validation splits for each client\n",
    "    val_split = 0.1  # 10% for validation\n",
    "\n",
    "    train_val_splits = [\n",
    "        random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)])\n",
    "        for client_dataset in client_datasets\n",
    "    ]\n",
    "\n",
    "    train_datasets = [split[0] for split in train_val_splits]\n",
    "    val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "    # Step 3: Remap test set labels to superclasses\n",
    "    filter_dataset_by_superclass(dataset_test, superclasses)\n",
    "\n",
    "    # Step 4: Create DataLoaders\n",
    "    client_loaders = [\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        for train_dataset in train_datasets\n",
    "    ]\n",
    "    val_loaders = [\n",
    "        DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "        for val_dataset in val_datasets\n",
    "    ]\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return client_loaders, val_loaders, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for data loaders\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "# Run get_data_loaders to create client loaders, validation loaders, and test loader\n",
    "client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client 1 label distribution in train dataset:\n",
      "[(0, 1143), (1, 1125), (2, 1116), (3, 1350), (4, 1126), (5, 1119), (6, 1124), (7, 1125), (8, 1127), (9, 1113), (10, 1136), (11, 1149), (12, 1120), (13, 1128), (14, 1108), (15, 1139), (16, 1342), (17, 1126), (18, 899), (19, 885)]\n",
      "\n",
      "Client 2 label distribution in train dataset:\n",
      "[(0, 1135), (1, 1132), (2, 1120), (3, 1351), (4, 1117), (5, 1115), (6, 1131), (7, 1133), (8, 1115), (9, 1107), (10, 1127), (11, 1141), (12, 1132), (13, 1130), (14, 1128), (15, 1131), (16, 1340), (17, 1125), (18, 893), (19, 897)]\n",
      "\n",
      "Client 1 label distribution in validation dataset:\n",
      "[(0, 107), (1, 125), (2, 134), (3, 150), (4, 124), (5, 131), (6, 126), (7, 125), (8, 123), (9, 137), (10, 114), (11, 101), (12, 130), (13, 122), (14, 142), (15, 111), (16, 158), (17, 124), (18, 101), (19, 115)]\n",
      "\n",
      "Client 2 label distribution in validation dataset:\n",
      "[(0, 115), (1, 118), (2, 130), (3, 149), (4, 133), (5, 135), (6, 119), (7, 117), (8, 135), (9, 143), (10, 123), (11, 109), (12, 118), (13, 120), (14, 122), (15, 119), (16, 160), (17, 125), (18, 107), (19, 103)]\n",
      "\n",
      "Test dataset label distribution:\n",
      "[(0, 500), (1, 500), (2, 500), (3, 600), (4, 500), (5, 500), (6, 500), (7, 500), (8, 500), (9, 500), (10, 500), (11, 500), (12, 500), (13, 500), (14, 500), (15, 500), (16, 600), (17, 500), (18, 400), (19, 400)]\n"
     ]
    }
   ],
   "source": [
    "# Check label mapping and distribution for each client\n",
    "from collections import Counter\n",
    "\n",
    "# Verify client datasets\n",
    "for i, client_loader in enumerate(client_loaders):\n",
    "    print(f\"\\nClient {i+1} label distribution in train dataset:\")\n",
    "    all_labels = []\n",
    "    for inputs, labels in client_loader:\n",
    "        all_labels.extend(labels.tolist())\n",
    "    label_counts = Counter(all_labels)\n",
    "    print(sorted(label_counts.items()))  # This should show counts for labels [0, 19]\n",
    "\n",
    "# Check label mapping and distribution for validation datasets\n",
    "for i, val_loader in enumerate(val_loaders):\n",
    "    print(f\"\\nClient {i+1} label distribution in validation dataset:\")\n",
    "    all_val_labels = []\n",
    "    for inputs, labels in val_loader:\n",
    "        all_val_labels.extend(labels.tolist())\n",
    "    val_label_counts = Counter(all_val_labels)\n",
    "    print(sorted(val_label_counts.items()))  # This should show counts for labels [0, 19]\n",
    "\n",
    "# Check label mapping and distribution in test dataset\n",
    "print(\"\\nTest dataset label distribution:\")\n",
    "all_test_labels = []\n",
    "for inputs, labels in test_loader:\n",
    "    all_test_labels.extend(labels.tolist())\n",
    "test_label_counts = Counter(all_test_labels)\n",
    "print(sorted(test_label_counts.items()))  # This should show counts for labels [0, 19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  preprocessing for 8 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset, DataLoader, random_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to randomly split dataset by superclass for 8 clients\n",
    "# def filter_dataset_randomly_by_superclass_8_clients(dataset, superclasses):\n",
    "#     client_indices = [[] for _ in range(8)]  # Create an empty list for each client (8 clients)\n",
    "\n",
    "#     # Shuffle and split images randomly for each superclass\n",
    "#     for subclasses in superclasses.values():\n",
    "#         superclass_indices = []\n",
    "        \n",
    "#         # Gather all images belonging to this superclass\n",
    "#         for idx, (data, target) in enumerate(dataset):\n",
    "#             if target in subclasses:\n",
    "#                 superclass_indices.append(idx)\n",
    "        \n",
    "#         # If no images found for this superclass, continue to the next\n",
    "#         if not superclass_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # Shuffle the indices for randomness\n",
    "#         np.random.shuffle(superclass_indices)\n",
    "        \n",
    "#         # Split the indices between the 8 clients\n",
    "#         split_size = len(superclass_indices) // 8\n",
    "#         for i in range(8):\n",
    "#             start = i * split_size\n",
    "#             end = (i + 1) * split_size if i < 7 else len(superclass_indices)  # Ensure last client gets the remaining data\n",
    "#             client_indices[i].extend(superclass_indices[start:end])\n",
    "    \n",
    "#     # Return 8 subsets: one for each client\n",
    "#     return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "# def get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses):\n",
    "#     # Step 1: Split the dataset for 8 clients\n",
    "#     client_datasets = filter_dataset_randomly_by_superclass_8_clients(dataset_train, superclasses)\n",
    "\n",
    "#     # Step 2: Create validation splits for each client\n",
    "#     val_split = 0.1  # 10% for validation\n",
    "\n",
    "#     # Create training and validation splits for each client\n",
    "#     train_val_splits = [random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)]) for client_dataset in client_datasets]\n",
    "\n",
    "#     # Separate train and validation datasets\n",
    "#     train_datasets = [split[0] for split in train_val_splits]\n",
    "#     val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "#     # Step 3: Create DataLoaders for each client\n",
    "#     client_loaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for train_dataset in train_datasets]\n",
    "#     val_loaders = [DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) for val_dataset in val_datasets]\n",
    "#     test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#     return client_loaders, val_loaders, test_loader\n",
    "\n",
    "# # Parameters\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "\n",
    "# # Call the function to initialize DataLoaders for 8 clients\n",
    "# client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  preprocessing for 16 clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset, DataLoader, random_split\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to randomly split dataset by superclass for 16 clients\n",
    "# def filter_dataset_randomly_by_superclass_16_clients(dataset, superclasses):\n",
    "#     client_indices = [[] for _ in range(16)]  # Create an empty list for each client (16 clients)\n",
    "\n",
    "#     # Shuffle and split images randomly for each superclass\n",
    "#     for subclasses in superclasses.values():\n",
    "#         superclass_indices = []\n",
    "        \n",
    "#         # Gather all images belonging to this superclass\n",
    "#         for idx, (data, target) in enumerate(dataset):\n",
    "#             if target in subclasses:\n",
    "#                 superclass_indices.append(idx)\n",
    "        \n",
    "#         # If no images found for this superclass, continue to the next\n",
    "#         if not superclass_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # Shuffle the indices for randomness\n",
    "#         np.random.shuffle(superclass_indices)\n",
    "        \n",
    "#         # Split the indices between the 16 clients\n",
    "#         split_size = len(superclass_indices) // 16\n",
    "#         for i in range(16):\n",
    "#             start = i * split_size\n",
    "#             end = (i + 1) * split_size if i < 15 else len(superclass_indices)  # Ensure last client gets the remaining data\n",
    "#             client_indices[i].extend(superclass_indices[start:end])\n",
    "    \n",
    "#     # Return 16 subsets: one for each client\n",
    "#     return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "# def get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses):\n",
    "#     # Step 1: Split the dataset for 16 clients\n",
    "#     client_datasets = filter_dataset_randomly_by_superclass_16_clients(dataset_train, superclasses)\n",
    "\n",
    "#     # Step 2: Create validation splits for each client\n",
    "#     val_split = 0.1  # 10% for validation\n",
    "\n",
    "#     # Create training and validation splits for each client\n",
    "#     train_val_splits = [random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)]) for client_dataset in client_datasets]\n",
    "\n",
    "#     # Separate train and validation datasets\n",
    "#     train_datasets = [split[0] for split in train_val_splits]\n",
    "#     val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "#     # Step 3: Create DataLoaders for each client\n",
    "#     client_loaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for train_dataset in train_datasets]\n",
    "#     val_loaders = [DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) for val_dataset in val_datasets]\n",
    "#     test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#     return client_loaders, val_loaders, test_loader\n",
    "\n",
    "# # Parameters\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "\n",
    "# # Call the function to initialize DataLoaders for 16 clients\n",
    "# client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)\n",
    "\n",
    "\n",
    "\n",
    "# # from torch.utils.data import Subset, DataLoader, random_split\n",
    "# # import numpy as np\n",
    "\n",
    "# # # Function to randomly split dataset by superclass for 16 clients\n",
    "# # def filter_dataset_randomly_by_superclass_16_clients(dataset, superclasses):\n",
    "# #     client_indices = [[] for _ in range(16)]  # Create an empty list for each client (16 clients)\n",
    "\n",
    "# #     # Shuffle and split images randomly for each superclass\n",
    "# #     for subclasses in superclasses.values():\n",
    "# #         superclass_indices = []\n",
    "        \n",
    "# #         # Gather all images belonging to this superclass\n",
    "# #         for idx, (data, target) in enumerate(dataset):\n",
    "# #             if target in subclasses:\n",
    "# #                 superclass_indices.append(idx)\n",
    "        \n",
    "# #         # If no images found for this superclass, continue to the next\n",
    "# #         if not superclass_indices:\n",
    "# #             continue\n",
    "        \n",
    "# #         # Shuffle the indices for randomness\n",
    "# #         np.random.shuffle(superclass_indices)\n",
    "        \n",
    "# #         # Split the indices between the 16 clients\n",
    "# #         split_size = len(superclass_indices) // 16\n",
    "# #         for i in range(16):\n",
    "# #             start = i * split_size\n",
    "# #             end = (i + 1) * split_size if i < 15 else len(superclass_indices)  # Ensure last client gets the remaining data\n",
    "# #             client_indices[i].extend(superclass_indices[start:end])\n",
    "    \n",
    "# #     # Return 16 subsets: one for each client\n",
    "# #     return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "# # def get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses):\n",
    "# #     # Step 1: Split the dataset for 16 clients\n",
    "# #     client_datasets = filter_dataset_randomly_by_superclass_16_clients(dataset_train, superclasses)\n",
    "\n",
    "# #     # Step 2: Create validation splits for each client\n",
    "# #     val_split = 0.1  # 10% for validation\n",
    "\n",
    "# #     # Create training and validation splits for each client\n",
    "# #     train_val_splits = [random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)]) for client_dataset in client_datasets]\n",
    "\n",
    "# #     # Separate train and validation datasets\n",
    "# #     train_datasets = [split[0] for split in train_val_splits]\n",
    "# #     val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "# #     # Step 3: Create DataLoaders for each client\n",
    "# #     client_loaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for train_dataset in train_datasets]\n",
    "# #     val_loaders = [DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) for val_dataset in val_datasets]\n",
    "# #     test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# #     return client_loaders, val_loaders, test_loader\n",
    "\n",
    "# # # Parameters\n",
    "# # batch_size = 256\n",
    "# # num_workers = 4\n",
    "\n",
    "# # # Call the function to initialize DataLoaders for 16 clients\n",
    "# # client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Train Loader Sizes: [22500, 22500]\n",
      "Client Validation Loader Sizes: [2500, 2500]\n",
      "Test Loader Size: 10000\n"
     ]
    }
   ],
   "source": [
    "# Check the number of samples in each client's train and validation loaders\n",
    "client_loader_sizes = [len(loader.dataset) for loader in client_loaders]\n",
    "val_loader_sizes = [len(loader.dataset) for loader in val_loaders]\n",
    "\n",
    "print(\"Client Train Loader Sizes:\", client_loader_sizes)\n",
    "print(\"Client Validation Loader Sizes:\", val_loader_sizes)\n",
    "\n",
    "# Check the number of samples in the test loader\n",
    "print(\"Test Loader Size:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking superclass coverage for each client...\n",
      "\n",
      "\n",
      "Client 1 train superclass distribution:\n",
      "Superclass 0: 1143 samples\n",
      "Superclass 1: 1125 samples\n",
      "Superclass 2: 1116 samples\n",
      "Superclass 3: 1350 samples\n",
      "Superclass 4: 1126 samples\n",
      "Superclass 5: 1119 samples\n",
      "Superclass 6: 1124 samples\n",
      "Superclass 7: 1125 samples\n",
      "Superclass 8: 1127 samples\n",
      "Superclass 9: 1113 samples\n",
      "Superclass 10: 1136 samples\n",
      "Superclass 11: 1149 samples\n",
      "Superclass 12: 1120 samples\n",
      "Superclass 13: 1128 samples\n",
      "Superclass 14: 1108 samples\n",
      "Superclass 15: 1139 samples\n",
      "Superclass 16: 1342 samples\n",
      "Superclass 17: 1126 samples\n",
      "Superclass 18: 899 samples\n",
      "Superclass 19: 885 samples\n",
      "\n",
      "\n",
      "\n",
      "Client 2 train superclass distribution:\n",
      "Superclass 0: 1135 samples\n",
      "Superclass 1: 1132 samples\n",
      "Superclass 2: 1120 samples\n",
      "Superclass 3: 1351 samples\n",
      "Superclass 4: 1117 samples\n",
      "Superclass 5: 1115 samples\n",
      "Superclass 6: 1131 samples\n",
      "Superclass 7: 1133 samples\n",
      "Superclass 8: 1115 samples\n",
      "Superclass 9: 1107 samples\n",
      "Superclass 10: 1127 samples\n",
      "Superclass 11: 1141 samples\n",
      "Superclass 12: 1132 samples\n",
      "Superclass 13: 1130 samples\n",
      "Superclass 14: 1128 samples\n",
      "Superclass 15: 1131 samples\n",
      "Superclass 16: 1340 samples\n",
      "Superclass 17: 1125 samples\n",
      "Superclass 18: 893 samples\n",
      "Superclass 19: 897 samples\n",
      "\n",
      "\n",
      "\n",
      "Client 1 validation superclass distribution:\n",
      "Superclass 0: 107 samples\n",
      "Superclass 1: 125 samples\n",
      "Superclass 2: 134 samples\n",
      "Superclass 3: 150 samples\n",
      "Superclass 4: 124 samples\n",
      "Superclass 5: 131 samples\n",
      "Superclass 6: 126 samples\n",
      "Superclass 7: 125 samples\n",
      "Superclass 8: 123 samples\n",
      "Superclass 9: 137 samples\n",
      "Superclass 10: 114 samples\n",
      "Superclass 11: 101 samples\n",
      "Superclass 12: 130 samples\n",
      "Superclass 13: 122 samples\n",
      "Superclass 14: 142 samples\n",
      "Superclass 15: 111 samples\n",
      "Superclass 16: 158 samples\n",
      "Superclass 17: 124 samples\n",
      "Superclass 18: 101 samples\n",
      "Superclass 19: 115 samples\n",
      "\n",
      "\n",
      "\n",
      "Client 2 validation superclass distribution:\n",
      "Superclass 0: 115 samples\n",
      "Superclass 1: 118 samples\n",
      "Superclass 2: 130 samples\n",
      "Superclass 3: 149 samples\n",
      "Superclass 4: 133 samples\n",
      "Superclass 5: 135 samples\n",
      "Superclass 6: 119 samples\n",
      "Superclass 7: 117 samples\n",
      "Superclass 8: 135 samples\n",
      "Superclass 9: 143 samples\n",
      "Superclass 10: 123 samples\n",
      "Superclass 11: 109 samples\n",
      "Superclass 12: 118 samples\n",
      "Superclass 13: 120 samples\n",
      "Superclass 14: 122 samples\n",
      "Superclass 15: 119 samples\n",
      "Superclass 16: 160 samples\n",
      "Superclass 17: 125 samples\n",
      "Superclass 18: 107 samples\n",
      "Superclass 19: 103 samples\n",
      "\n",
      "\n",
      "Overall test superclass distribution:\n",
      "Superclass 0: 500 samples\n",
      "Superclass 1: 500 samples\n",
      "Superclass 2: 500 samples\n",
      "Superclass 3: 600 samples\n",
      "Superclass 4: 500 samples\n",
      "Superclass 5: 500 samples\n",
      "Superclass 6: 500 samples\n",
      "Superclass 7: 500 samples\n",
      "Superclass 8: 500 samples\n",
      "Superclass 9: 500 samples\n",
      "Superclass 10: 500 samples\n",
      "Superclass 11: 500 samples\n",
      "Superclass 12: 500 samples\n",
      "Superclass 13: 500 samples\n",
      "Superclass 14: 500 samples\n",
      "Superclass 15: 500 samples\n",
      "Superclass 16: 600 samples\n",
      "Superclass 17: 500 samples\n",
      "Superclass 18: 400 samples\n",
      "Superclass 19: 400 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check_superclass_coverage(client_loaders, superclasses):\n",
    "    for i, loader in enumerate(client_loaders):\n",
    "        superclass_counts = Counter()\n",
    "\n",
    "        for _, labels in loader:\n",
    "            for label in labels:\n",
    "                superclass_counts[label.item()] += 1\n",
    "\n",
    "        print(f\"Client {i + 1} superclass distribution:\")\n",
    "        for superclass in range(len(superclasses)):\n",
    "            count = superclass_counts.get(superclass, 0)\n",
    "            print(f\"Superclass {superclass}: {count} samples\")\n",
    "            if count == 0:\n",
    "                print(f\"Warning: Client {i + 1} is missing samples for superclass {superclass}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Checking superclass distribution for each client across train, validation, and test sets\n",
    "print(\"Checking superclass coverage for each client...\\n\")\n",
    "\n",
    "def print_superclass_distribution(client_datasets, stage):\n",
    "    for i, loader in enumerate(client_datasets):\n",
    "        superclass_counts = Counter()\n",
    "        for _, targets in loader:\n",
    "            superclass_counts.update(targets.tolist())\n",
    "\n",
    "        print(f\"\\nClient {i + 1} {stage} superclass distribution:\")\n",
    "        for superclass, count in sorted(superclass_counts.items()):\n",
    "            print(f\"Superclass {superclass}: {count} samples\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Check distribution in train datasets\n",
    "print_superclass_distribution(client_loaders, \"train\")\n",
    "\n",
    "# Check distribution in validation datasets\n",
    "print_superclass_distribution(val_loaders, \"validation\")\n",
    "\n",
    "# Check distribution in test dataset (since test_loader is shared, it only needs one output)\n",
    "print(\"Overall test superclass distribution:\")\n",
    "test_superclass_counts = Counter()\n",
    "for _, targets in test_loader:\n",
    "    test_superclass_counts.update(targets.tolist())\n",
    "\n",
    "for superclass, count in sorted(test_superclass_counts.items()):\n",
    "    print(f\"Superclass {superclass}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets for 2 clients saved successfully to /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/client_datasets_2_clients.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Define the number of clients\n",
    "num_clients = 2  # Change this to 4, 8, or 16 for other configurations\n",
    "\n",
    "# Define the save path with the number of clients in the filename\n",
    "save_path = f'/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/client_datasets_{num_clients}_clients.pkl'\n",
    "\n",
    "# Save split datasets\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump([client_loaders, val_loaders, test_loader], f)\n",
    "\n",
    "print(f\"Datasets for {num_clients} clients saved successfully to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets for 2 clients loaded successfully from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/client_datasets_2_clients.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the number of clients you want to load\n",
    "num_clients = 2  # Adjust to the desired configuration (2, 4, 8, or 16 clients)\n",
    "\n",
    "# Load the saved dataset splits\n",
    "load_path = f'/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/client_datasets_2_clients.pkl' # When loading, match the num_clients variable\n",
    "with open(load_path, 'rb') as f:\n",
    "    client_loaders, val_loaders, test_loader = pickle.load(f)\n",
    "\n",
    "print(f\"Datasets for {num_clients} clients loaded successfully from {load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(num_classes=20, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "### Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.(a) Federated Training Functions (Federated Averaging (FedAvg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "#     model.to(device)  \n",
    "#     model.train()\n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)  \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.0001, patience=5, min_delta=0):\n",
    "#     model = prepare_model(num_classes=20).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         avg_state_dict = federated_averaging(state_dicts)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Validation phase (handling multiple validation loaders)\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for val_loader in val_loaders:\n",
    "#                 for inputs, labels in val_loader:\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     val_running_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    \n",
    "#     # Optionally save the final model\n",
    "#     num_clients = 2  # Adjust this for 2, 4, 8, or 16 clients as needed\n",
    "#     model_save_path = f'/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/final_federated_model_{num_clients}_clients.pth'\n",
    "#     torch.save(model.state_dict(), model_save_path)\n",
    "#     print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.(b) Federated Training Functions (Adaptive Federated Optimization(AdaptFedOpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "#     model.to(device)  \n",
    "#     model.train()\n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)  \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts, global_model, beta=0.9):\n",
    "#     avg_state_dict = global_model.state_dict()\n",
    "#     for key in avg_state_dict.keys():\n",
    "#         if avg_state_dict[key].dtype == torch.long:\n",
    "#             avg_state_dict[key] = torch.zeros_like(avg_state_dict[key], dtype=torch.float32)\n",
    "#             for state_dict in state_dicts:\n",
    "#                 avg_state_dict[key] += state_dict[key].float() / len(state_dicts)\n",
    "#             avg_state_dict[key] = avg_state_dict[key].long()  # Convert back to long if necessary\n",
    "#         else:\n",
    "#             avg_state_dict[key] = torch.zeros_like(avg_state_dict[key])\n",
    "#             for state_dict in state_dicts:\n",
    "#                 avg_state_dict[key] += state_dict[key] / len(state_dicts)\n",
    "#             avg_state_dict[key] = beta * avg_state_dict[key] + (1 - beta) * global_model.state_dict()[key]\n",
    "#     return avg_state_dict\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.0001, patience=5, min_delta=0):\n",
    "#     model = prepare_model(num_classes=20).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     # Initialize server optimizer for global model\n",
    "#     # Use the same learning rate as the clients for the server-side optimizer\n",
    "#     server_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         avg_state_dict = federated_averaging(state_dicts, model, beta=0.9)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "\n",
    "#         # Server-side optimization step\n",
    "#         server_optimizer.zero_grad()\n",
    "#         server_optimizer.step()  # Apply server optimizer to improve global model\n",
    "        \n",
    "\n",
    "#         # Validation phase (handling multiple validation loaders)\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for val_loader in val_loaders:  # Iterate over multiple validation loaders\n",
    "#                 for inputs, labels in val_loader:  # Iterate over batches within each loader\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     val_running_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#             f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.(c) Federated Training Functions (FedProx (Federated Proximal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client(model, train_loader, criterion, optimizer, global_model, mu=0.01, epochs=1):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     global_weights = global_model.state_dict()  # Get the global model's weights\n",
    "    \n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Add the proximal term (FedProx)\n",
    "#             proximal_term = 0.0\n",
    "#             for param, global_param in zip(model.parameters(), global_model.parameters()):\n",
    "#                 proximal_term += (param - global_param).norm(2)\n",
    "#             loss += (mu / 2) * proximal_term\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict\n",
    "\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.0001, patience=5, min_delta=0, mu=0.01):\n",
    "#     model = prepare_model(num_classes=20).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer, model, mu)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "        \n",
    "#         avg_state_dict = federated_averaging(state_dicts)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Validation phase (same as before)\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for val_loader in val_loaders:\n",
    "#                 for inputs, labels in val_loader:\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     val_running_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase (same as before)\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.(a) Federated Training Functions (Weighted Federated Averaging (weightedFedAvg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.to(device)  \n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "# Ensure that client_loader_sizes is already defined before this\n",
    "client_data_sizes = client_loader_sizes\n",
    "\n",
    "def weighted_federated_averaging(state_dicts, client_data_sizes):\n",
    "    \"\"\"Performs WeightedFedAvg aggregation of the model state_dicts based on client data sizes.\"\"\"\n",
    "    \n",
    "    if len(state_dicts) != len(client_data_sizes):\n",
    "        raise ValueError(\"Number of state dicts and client data sizes must match.\")\n",
    "    \n",
    "    total_data = sum(client_data_sizes)\n",
    "    avg_state_dict = {}\n",
    "    \n",
    "    # Initialize the avg_state_dict with zero values, and ensure they are float type\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = torch.zeros_like(state_dicts[0][key], dtype=torch.float32)  # Ensure float type\n",
    "    \n",
    "    # Weighted averaging of the state_dicts\n",
    "    for i, state_dict in enumerate(state_dicts):\n",
    "        weight = client_data_sizes[i] / total_data\n",
    "        for key in state_dict.keys():\n",
    "            avg_state_dict[key] += state_dict[key].float() * weight  # Convert state_dict to float before multiplication\n",
    "\n",
    "    return avg_state_dict\n",
    "\n",
    "\n",
    "\n",
    "def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.0001, patience=5, min_delta=0):\n",
    "    model = prepare_model(num_classes=20).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for round in range(num_epochs):\n",
    "        print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "        state_dicts = []\n",
    "        for i, client_loader in enumerate(client_loaders):\n",
    "            print(f\"Training model for client {i+1}...\")\n",
    "            client_model = prepare_model().to(device)\n",
    "            client_model.load_state_dict(model.state_dict())\n",
    "            optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "            client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "            state_dicts.append(client_state_dict)\n",
    "\n",
    "        avg_state_dict = weighted_federated_averaging(state_dicts, client_data_sizes)\n",
    "\n",
    "        model.load_state_dict(avg_state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Validation phase (handling multiple validation loaders)\n",
    "        print(\"Validating model...\")\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_loader in val_loaders:\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Test phase\n",
    "    print(\"Testing model...\")\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def log_experiment_result(filename, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['num_clients', 'num_epochs', 'learning_rate', 'patience', 'min_delta', 'val_loss', 'val_accuracy', 'test_loss', 'test_accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only once\n",
    "        \n",
    "        writer.writerow({\n",
    "            'num_clients': num_clients,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'patience': patience,\n",
    "            'min_delta': min_delta,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-split datasets loaded successfully!\n",
      "Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated learning round 1/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 1/40, Val Loss: 1.4788, Val Accuracy: 54.42%\n",
      "Starting federated learning round 2/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 2/40, Val Loss: 1.2134, Val Accuracy: 61.76%\n",
      "Starting federated learning round 3/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 3/40, Val Loss: 1.1079, Val Accuracy: 65.28%\n",
      "Starting federated learning round 4/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 4/40, Val Loss: 1.0374, Val Accuracy: 67.82%\n",
      "Starting federated learning round 5/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 5/40, Val Loss: 0.9982, Val Accuracy: 68.60%\n",
      "Starting federated learning round 6/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 6/40, Val Loss: 0.9489, Val Accuracy: 70.24%\n",
      "Starting federated learning round 7/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 7/40, Val Loss: 0.9333, Val Accuracy: 70.48%\n",
      "Starting federated learning round 8/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 8/40, Val Loss: 0.9153, Val Accuracy: 70.56%\n",
      "Starting federated learning round 9/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 9/40, Val Loss: 0.8893, Val Accuracy: 71.88%\n",
      "Starting federated learning round 10/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 10/40, Val Loss: 0.8609, Val Accuracy: 72.86%\n",
      "Starting federated learning round 11/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 11/40, Val Loss: 0.8659, Val Accuracy: 72.86%\n",
      "Starting federated learning round 12/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 12/40, Val Loss: 0.8391, Val Accuracy: 73.16%\n",
      "Starting federated learning round 13/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 13/40, Val Loss: 0.8593, Val Accuracy: 72.78%\n",
      "Starting federated learning round 14/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 14/40, Val Loss: 0.8313, Val Accuracy: 73.98%\n",
      "Starting federated learning round 15/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 15/40, Val Loss: 0.8331, Val Accuracy: 73.92%\n",
      "Starting federated learning round 16/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 16/40, Val Loss: 0.8368, Val Accuracy: 74.12%\n",
      "Starting federated learning round 17/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 17/40, Val Loss: 0.8034, Val Accuracy: 74.30%\n",
      "Starting federated learning round 18/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 18/40, Val Loss: 0.8431, Val Accuracy: 73.16%\n",
      "Starting federated learning round 19/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 19/40, Val Loss: 0.8053, Val Accuracy: 74.56%\n",
      "Starting federated learning round 20/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 20/40, Val Loss: 0.8349, Val Accuracy: 73.90%\n",
      "Starting federated learning round 21/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 21/40, Val Loss: 0.8195, Val Accuracy: 75.08%\n",
      "Starting federated learning round 22/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Validating model...\n",
      "Federated Round 22/40, Val Loss: 0.7936, Val Accuracy: 76.08%\n",
      "Early stopping\n",
      "Testing model...\n",
      "Test Loss: 0.8078, Test Accuracy: 75.23%\n",
      "Federated training complete.\n",
      "Experiment results logged.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Parameters\n",
    "num_clients = 2  # Update this to match the number of clients\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "patience = 5\n",
    "min_delta = 0.01\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "log_file = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/experiment_results.csv'\n",
    "\n",
    "# Path for saving/loading pre-split datasets\n",
    "dataset_save_path = f'/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/client_datasets_2_clients.pkl'\n",
    "\n",
    "# Try to load pre-split datasets\n",
    "try:\n",
    "    with open(dataset_save_path, 'rb') as f:\n",
    "        client_loaders, val_loaders, test_loader = pickle.load(f)\n",
    "    print(\"Pre-split datasets loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    # If not found, split the dataset and save it\n",
    "    print(\"Pre-split datasets not found. Splitting dataset...\")\n",
    "    client_loaders, val_loaders, test_loader = get_data_loaders(batch_size, num_workers, dataset_train, dataset_test, superclasses)\n",
    "    \n",
    "    # Save the dataset splits after the first split\n",
    "    os.makedirs(os.path.dirname(dataset_save_path), exist_ok=True)\n",
    "    with open(dataset_save_path, 'wb') as f:\n",
    "        pickle.dump([client_loaders, val_loaders, test_loader], f)\n",
    "    print(\"Datasets saved successfully!\")\n",
    "\n",
    "# Start federated training\n",
    "print(\"Starting federated training...\")\n",
    "model, val_loss, val_accuracy, test_loss, test_accuracy = train_federated_model(\n",
    "    client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate, patience, min_delta)\n",
    "print(\"Federated training complete.\")\n",
    "\n",
    "# Log the results\n",
    "log_experiment_result(log_file, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy)\n",
    "print(\"Experiment results logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
