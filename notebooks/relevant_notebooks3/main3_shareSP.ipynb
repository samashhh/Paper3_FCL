{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check for CUDA GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Client 1 dataset size: 22500\n",
      "Client 2 dataset size: 22500\n",
      "Client 3 dataset size: 19000\n",
      "Client 1 - Batch size: 256\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels shape: torch.Size([256])\n",
      "Labels: tensor([73, 82,  1,  5, 40, 28,  9, 86, 28, 73, 20, 24, 97, 51, 91, 16, 22, 30,\n",
      "        40, 55,  5, 18, 70, 42, 83, 57, 25, 18, 54,  9,  7,  4, 94, 73, 94, 82,\n",
      "         4,  1, 83, 70, 62,  1, 54, 83,  6, 67, 25,  5, 67, 97, 86, 53, 16,  6,\n",
      "        91, 54,  4, 53, 28, 83,  7, 67,  9, 25, 30, 28, 24, 92,  9, 97, 94, 55,\n",
      "         1, 30, 18, 16, 95, 70, 70,  5, 42, 20, 32, 86, 83, 82, 67, 82, 43, 28,\n",
      "        32,  9, 51,  4, 10,  0, 43,  3,  0, 30, 67, 67, 43,  5, 14,  1,  0, 91,\n",
      "        22, 32, 84,  9, 24,  7, 54, 95, 32, 32, 88,  9, 57, 32, 18, 91,  4, 73,\n",
      "        16,  4, 28, 54, 16, 24, 92, 32, 53,  3,  5, 73, 25, 22, 67, 82, 40, 73,\n",
      "        72,  1, 86, 57,  4, 24, 20, 25, 73, 61,  3, 92, 73, 39, 82, 61,  5,  4,\n",
      "        61, 25,  6, 53, 53, 51, 10, 42, 30, 53, 54,  4, 25, 57, 28, 62, 70, 53,\n",
      "        57,  1, 95, 22, 95,  7, 39, 88, 57, 61,  6, 86,  6, 16,  7,  9, 82, 22,\n",
      "         3, 87, 88,  9, 22, 91, 92, 43, 42, 57, 43, 55, 18, 86, 94, 88,  5,  0,\n",
      "        20, 18, 62,  4,  9, 20, 55,  5, 95, 86, 92,  0, 86, 20,  7,  6, 57, 14,\n",
      "        82, 88, 67, 72,  6, 16, 43, 40,  4,  0, 97, 61, 87, 55, 16, 94, 28, 40,\n",
      "        54, 53, 10, 51])\n",
      "Client 2 - Batch size: 256\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels shape: torch.Size([256])\n",
      "Labels: tensor([21, 42, 77,  3, 24, 97, 75, 14, 14, 45, 35, 79, 97, 68, 77, 60, 99, 75,\n",
      "        20, 88, 76, 18, 42,  6, 35, 79, 71,  3, 99, 77, 98, 79, 26, 12, 23, 84,\n",
      "        77, 20, 12, 97, 75, 37, 12, 38, 35, 33,  3, 45, 75, 63,  5, 46, 25, 88,\n",
      "        76, 25, 24, 97, 71, 38, 12, 97, 49, 38, 63, 79, 23, 63, 60, 94,  6, 99,\n",
      "        49,  2, 75, 37, 43, 42, 18, 26, 14, 15, 97, 60, 37, 43, 71, 42, 49, 23,\n",
      "         3, 66, 25, 19, 18, 17, 75, 63, 99, 60, 60, 35, 23, 24, 38, 15, 11, 37,\n",
      "        71, 37, 17, 15, 68, 19, 63, 19, 75, 25, 63, 99, 66, 42, 34, 21, 12, 25,\n",
      "        77, 46, 33, 14, 75, 68, 77, 18,  2, 71, 24, 98, 45, 14, 18, 35, 75, 99,\n",
      "        11, 14, 71, 75, 26, 49,  3, 88, 45, 17, 33, 42, 11, 34, 99, 18, 23, 66,\n",
      "        76, 42, 33, 84, 14, 77, 75, 11,  7, 43,  6, 79, 99, 18,  5, 77, 64, 66,\n",
      "        79, 43, 46, 60, 97, 24, 71,  2, 25, 71,  2, 23, 99, 18, 97, 43, 66,  7,\n",
      "        79, 19, 60, 42, 97, 42, 12, 31, 17, 35, 49, 77, 33, 14, 45, 35, 46, 46,\n",
      "        64, 63, 18, 75, 43, 64, 14, 23,  6, 97, 99,  2, 33, 34, 19, 14, 76, 19,\n",
      "        23, 12,  6, 42, 71, 35, 11, 18, 31, 21, 63,  2, 31,  7, 25, 15, 20,  3,\n",
      "        15, 71, 60, 66])\n",
      "Client 2 - Batch size: 256\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels shape: torch.Size([256])\n",
      "Labels: tensor([48, 58, 27, 48, 13, 93, 77, 52, 46,  2, 78, 93, 48, 79, 44, 59, 11, 52,\n",
      "        79, 46, 80, 11, 58, 29, 89, 98, 56, 58, 99, 44, 50, 27, 45, 44, 63, 93,\n",
      "        35, 64, 69, 52, 99, 93,  2, 47, 35, 13, 46, 29, 74, 79, 80, 77, 56, 80,\n",
      "         2, 52, 46, 80, 47, 29, 11, 11,  2, 80, 44, 89, 13, 79, 36, 27, 63, 78,\n",
      "        89, 75, 80, 58, 64,  8, 81, 77, 59, 74, 66, 80, 78, 90, 52, 81, 50, 50,\n",
      "        50, 98, 35, 65, 75, 90, 74, 74, 59, 59, 69, 64, 99, 81, 98, 99, 59, 66,\n",
      "        45, 47, 79, 80,  2, 80, 75,  8, 45, 56, 27, 58, 58, 29, 89, 79, 58, 56,\n",
      "        65, 11, 75, 78,  2, 45, 90, 74, 41, 45, 66, 98, 79, 90, 46, 50, 11, 56,\n",
      "        58, 90, 56, 29, 75, 44, 64, 26, 26, 26, 74, 35, 90, 46, 58, 36, 74, 47,\n",
      "        69, 77, 44, 66, 58, 59, 56, 81, 78, 81, 99, 47,  2, 36, 26, 13, 26, 77,\n",
      "        78, 79, 77, 63, 52, 64, 48, 80, 64, 93, 64, 93, 65, 27, 26, 80, 46, 47,\n",
      "        64, 26, 78, 79, 66, 59, 59, 56,  2,  2, 59, 74, 93, 58, 75, 81, 66, 90,\n",
      "        58, 29, 47, 75,  2, 79, 65, 63,  8, 34, 90, 46, 26, 99, 81, 98, 44, 99,\n",
      "        89, 34, 64, 78, 98,  8, 80, 59, 48, 99, 36, 77, 50, 69, 63, 35, 11, 89,\n",
      "        58, 78, 65, 56])\n",
      "Client 1 Validation - Batch size: 256\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels shape: torch.Size([256])\n",
      "Labels: tensor([18, 18, 97,  7, 88, 57, 43,  4, 28, 95, 95, 57, 70, 87,  9, 53, 87,  0,\n",
      "        95, 70, 86, 57, 86, 14, 95, 28, 20, 91,  0, 72,  6, 61, 55, 16, 51, 16,\n",
      "        55, 16, 22, 22,  1, 53, 61, 42, 14, 14,  4, 40, 16, 16, 22, 25, 91,  6,\n",
      "        10, 57, 57, 18, 91, 92, 16, 61,  0, 55, 42, 54,  4, 22, 22, 67, 28, 72,\n",
      "        24, 20, 30, 30, 62,  6, 40, 55, 24,  7, 16,  0,  3, 53, 92, 32, 84, 24,\n",
      "         9, 91, 24, 20,  4, 30,  5, 28, 92,  3, 88, 95, 95, 30,  6, 97, 53, 43,\n",
      "        92,  9, 94, 88,  3, 86, 20,  0, 40, 28, 53, 42, 62,  1,  7, 83, 95, 39,\n",
      "        73,  4, 67, 70,  1, 91,  9, 10, 95, 91, 82, 22, 24,  1, 40,  5, 54, 14,\n",
      "         0, 54, 82, 73, 39, 14, 70, 88, 70,  4,  7, 57, 67, 92, 42, 22, 70, 32,\n",
      "        39, 51,  7, 55, 72, 51,  7, 40, 86, 87, 20, 91, 61, 24, 97, 51, 92, 53,\n",
      "        57, 32,  4,  0,  4, 92, 61, 72, 94, 25,  4, 95, 55, 22, 16, 14, 24,  7,\n",
      "        25,  9,  1, 67, 91, 86, 24,  7, 14, 72, 61, 20, 24, 40, 88, 57,  4, 39,\n",
      "        43, 20, 94, 97, 18, 94, 16, 42, 42, 51, 10, 61, 70, 86, 42, 28, 72, 84,\n",
      "        83, 42, 91, 40, 94, 39,  6,  0, 28, 88, 16,  0, 20, 32,  4,  3, 42,  6,\n",
      "        55, 51, 22, 67])\n",
      "Client 2 Validation - Batch size: 256\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels shape: torch.Size([256])\n",
      "Labels: tensor([ 5, 88, 26, 38, 43, 99, 45,  7, 84, 35, 31, 98, 34, 64, 37, 23, 20, 37,\n",
      "        99, 26, 12,  5, 98,  6, 66,  7, 68, 23, 60, 33, 25,  7, 66, 66, 31,  7,\n",
      "        17, 31, 43, 46, 20, 88, 31,  2, 43, 21, 33, 12, 26, 75, 66,  2, 98, 75,\n",
      "        18, 79, 75, 38, 66, 79, 99, 94, 20, 76, 66, 34, 35, 17,  3, 31, 46, 64,\n",
      "        66, 77,  5, 19, 68, 26, 14, 45, 46, 33, 43, 46, 46, 63, 14, 99, 94, 71,\n",
      "        98, 37, 14, 46, 84, 21, 34, 38, 94, 12, 66, 77, 88, 11, 11,  7, 21, 94,\n",
      "        68, 76, 88, 19, 23, 45, 37, 14, 77,  5, 66, 94, 38, 77, 88, 71, 60, 20,\n",
      "        38,  6, 37, 98, 60, 60, 31, 34, 12, 37, 75,  5, 68, 63, 71,  2, 14, 14,\n",
      "        77, 15, 17, 49, 98, 75, 35, 34, 94, 14, 68,  3, 19,  3, 71,  5, 71, 38,\n",
      "        20, 66, 77, 14, 76,  6, 94,  2,  5, 84, 60, 49, 14,  5, 31, 64, 38,  3,\n",
      "        60,  2, 63, 34, 18,  5, 11, 14, 68, 37,  3,  5, 20, 66, 49, 14, 23, 38,\n",
      "        66, 66, 60, 17, 12, 68, 99,  2, 23, 76, 79, 12, 19, 33, 25, 64, 63, 66,\n",
      "        98, 19, 24, 98, 68, 79, 42, 77, 43, 14, 84,  3, 71, 14, 25, 35, 84, 98,\n",
      "        17, 18, 97, 71, 88, 45, 76, 76, 26, 33, 38,  3, 38, 98, 98,  7, 46, 24,\n",
      "        77, 49, 38, 37])\n",
      "Client 3 Validation - Batch size: 256\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels shape: torch.Size([256])\n",
      "Labels: tensor([11, 29, 45, 58, 36, 46, 48, 74, 11, 59, 75, 46, 26, 36, 27, 50, 79, 77,\n",
      "        48, 59, 35, 74, 59, 46, 89, 74, 36, 98, 66, 41, 44,  2, 79, 27, 99, 99,\n",
      "        99, 74, 75, 11,  2, 66, 69, 69, 63, 26, 34, 52, 48, 13, 69, 64, 93, 79,\n",
      "        89, 11, 58, 29, 35, 93, 26, 47, 93, 41, 41, 66, 13, 11, 56, 35, 13, 64,\n",
      "        41, 45, 74, 47,  8, 81, 81, 99, 81, 29, 75, 79, 64, 26, 27, 74, 13, 27,\n",
      "        89, 58, 89, 34, 89, 65, 29, 36, 80, 48, 58, 45, 26,  8, 58, 66, 74, 79,\n",
      "        27, 90, 50, 47, 50, 47, 27, 77, 34, 59, 46, 98, 52, 89, 56, 69, 75, 78,\n",
      "         8, 65, 45,  8, 78, 66, 78, 74, 27, 44,  2, 41, 75, 52,  2, 77, 77, 74,\n",
      "        80,  2, 41, 34, 34, 80, 26, 35, 80, 77, 66, 78, 44, 66, 34, 65, 46, 13,\n",
      "        69, 41, 58, 93, 56, 90, 45, 50, 46, 45, 75,  2,  2, 36, 63, 77, 13, 98,\n",
      "        45, 26, 41, 81, 81, 35, 48,  2, 56, 50, 98, 77, 48, 13, 63, 35, 90, 26,\n",
      "        93, 80, 46, 47, 56, 81, 45, 35, 77, 34, 50, 48, 26, 34, 90, 56, 44, 89,\n",
      "        80, 11, 46, 66, 11, 80, 89, 81, 52, 66, 45, 81, 44, 80, 69, 90, 44, 74,\n",
      "        64, 45, 59, 75, 52, 75, 27,  8, 66, 35, 77, 29, 90, 78, 41, 81, 36, 13,\n",
      "        93, 65, 50, 35])\n",
      "Preprocessing for federated learning with specific superclasses and validation sets completed successfully.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "dataset_train = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define superclasses and subclasses\n",
    "superclasses = {\n",
    "    1: [4, 30, 55, 72, 95],  # aquatic mammals\n",
    "    2: [1, 32, 67, 73, 91],  # fish\n",
    "    3: [54, 62, 70, 82, 92],  # flowers\n",
    "    4: [9, 10, 16, 28, 61],  # food containers\n",
    "    5: [0, 51, 53, 57, 83],  # fruit and vegetables\n",
    "    6: [22, 39, 40, 86, 87],  # household electrical devices\n",
    "    7: [5, 20, 25, 84, 94],  # household furniture\n",
    "    8: [6, 7, 14, 18, 24],  # insects\n",
    "    9: [3, 42, 43, 88, 97],  # large carnivores\n",
    "    10: [12, 17, 37, 68, 76],  # large man-made outdoor things\n",
    "    11: [23, 33, 49, 60, 71],  # large natural outdoor scenes\n",
    "    12: [15, 19, 21, 31, 38],  # medium-sized mammals\n",
    "    13: [34, 63, 64, 66, 75],  # non-insect invertebrates\n",
    "    14: [26, 45, 77, 79, 99],  # people\n",
    "    15: [2, 11, 35, 46, 98],  # reptiles\n",
    "    16: [27, 29, 44, 78, 93],  # small mammals\n",
    "    17: [36, 50, 65, 74, 80],  # trees\n",
    "    18: [8, 13, 48, 58, 90],  # vehicles 1\n",
    "    19: [41, 66, 69, 81, 89],  # vehicles 2\n",
    "    20: [47, 50, 52, 56, 59],  # household furniture\n",
    "}\n",
    "\n",
    "# Function to map subclass to its corresponding superclass\n",
    "def get_superclass(subclass, superclasses):\n",
    "    for superclass, subclasses in superclasses.items():\n",
    "        if subclass in subclasses:\n",
    "            return superclass\n",
    "    return None\n",
    "\n",
    "# Function to filter dataset by superclasses\n",
    "def filter_dataset_by_superclass(dataset, superclasses, selected_superclasses):\n",
    "    selected_indices = []\n",
    "    for idx, (data, target) in enumerate(dataset):\n",
    "        subclass = target\n",
    "        superclass = get_superclass(subclass, superclasses)\n",
    "        if superclass in selected_superclasses:\n",
    "            selected_indices.append(idx)\n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "# # Define the superclasses for each client\n",
    "# client1_superclasses = list(range(1, 13))  # Superclasses 1-12 for Client 1\n",
    "# client2_superclasses = list(range(8, 21))  # Superclasses 9-20 for Client 2\n",
    "\n",
    "\n",
    "# Superclass ranges for each client\n",
    "client1_superclasses = list(range(1, 10))   # Superclasses 1 to 9\n",
    "client2_superclasses = list(range(7, 16))   # Superclasses 7 to 15\n",
    "client3_superclasses = list(range(13, 21))  # Superclasses 13 to 20\n",
    "\n",
    "# Filter the dataset for each client\n",
    "client1_dataset = filter_dataset_by_superclass(dataset_train, superclasses, client1_superclasses)\n",
    "client2_dataset = filter_dataset_by_superclass(dataset_train, superclasses, client2_superclasses)\n",
    "client3_dataset = filter_dataset_by_superclass(dataset_train, superclasses, client3_superclasses)\n",
    "\n",
    "# Print dataset sizes to verify\n",
    "print(f'Client 1 dataset size: {len(client1_dataset)}')\n",
    "print(f'Client 2 dataset size: {len(client2_dataset)}')\n",
    "print(f'Client 3 dataset size: {len(client3_dataset)}')\n",
    "\n",
    "# Create DataLoaders for each client and the validation set\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "# Split each client dataset into training and validation sets\n",
    "val_split = 0.1  # 10% for validation\n",
    "\n",
    "# For client1\n",
    "val_size_client1 = int(val_split * len(client1_dataset))\n",
    "train_size_client1 = len(client1_dataset) - val_size_client1\n",
    "train_dataset_client1, val_dataset_client1 = random_split(client1_dataset, [train_size_client1, val_size_client1])\n",
    "\n",
    "# For client2\n",
    "val_size_client2 = int(val_split * len(client2_dataset))\n",
    "train_size_client2 = len(client2_dataset) - val_size_client2\n",
    "train_dataset_client2, val_dataset_client2 = random_split(client2_dataset, [train_size_client2, val_size_client2])\n",
    "\n",
    "# For client3\n",
    "val_size_client3 = int(val_split * len(client3_dataset))\n",
    "train_size_client3 = len(client3_dataset) - val_size_client3\n",
    "train_dataset_client3, val_dataset_client3 = random_split(client3_dataset, [train_size_client3, val_size_client3])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "client_loaders = [\n",
    "    DataLoader(train_dataset_client1, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    DataLoader(train_dataset_client2, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    DataLoader(train_dataset_client3, batch_size=batch_size, shuffle=True, num_workers=num_workers),   \n",
    "]\n",
    "\n",
    "val_loaders = [\n",
    "    DataLoader(val_dataset_client1, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n",
    "    DataLoader(val_dataset_client2, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n",
    "    DataLoader(val_dataset_client3, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "]\n",
    "\n",
    "# Test DataLoader (common for all clients)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Checking the first client loader to see if everything works correctly\n",
    "for images, labels in client_loaders[0]:\n",
    "    print(f'Client 1 - Batch size: {images.size(0)}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Labels: {labels}')\n",
    "    break\n",
    "\n",
    "# Checking the second client loader to see if everything works correctly\n",
    "for images, labels in client_loaders[1]:\n",
    "    print(f'Client 2 - Batch size: {images.size(0)}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Labels: {labels}')\n",
    "    break\n",
    "\n",
    "# Checking the third client loader to see if everything works correctly\n",
    "for images, labels in client_loaders[2]:\n",
    "    print(f'Client 2 - Batch size: {images.size(0)}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Labels: {labels}')\n",
    "    break\n",
    "\n",
    "# Checking the first validation loader\n",
    "for images, labels in val_loaders[0]:\n",
    "    print(f'Client 1 Validation - Batch size: {images.size(0)}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Labels: {labels}')\n",
    "    break\n",
    "\n",
    "# Checking the second validation loader\n",
    "for images, labels in val_loaders[1]:\n",
    "    print(f'Client 2 Validation - Batch size: {images.size(0)}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Labels: {labels}')\n",
    "    break\n",
    "\n",
    "# Checking the third validation loader\n",
    "for images, labels in val_loaders[2]:\n",
    "    print(f'Client 3 Validation - Batch size: {images.size(0)}')\n",
    "    print(f'Images shape: {images.shape}')\n",
    "    print(f'Labels shape: {labels.shape}')\n",
    "    print(f'Labels: {labels}')\n",
    "    break\n",
    "\n",
    "print(\"Preprocessing for federated learning with specific superclasses and validation sets completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 superclass distribution:\n",
      "Superclass 1: 2500 images\n",
      "Superclass 2: 2500 images\n",
      "Superclass 3: 2500 images\n",
      "Superclass 4: 2500 images\n",
      "Superclass 5: 2500 images\n",
      "Superclass 6: 2500 images\n",
      "Superclass 7: 2500 images\n",
      "Superclass 8: 2500 images\n",
      "Superclass 9: 2500 images\n",
      "Superclass 10: 0 images\n",
      "Superclass 11: 0 images\n",
      "Superclass 12: 0 images\n",
      "Superclass 13: 0 images\n",
      "Superclass 14: 0 images\n",
      "Superclass 15: 0 images\n",
      "Superclass 16: 0 images\n",
      "Superclass 17: 0 images\n",
      "Superclass 18: 0 images\n",
      "Superclass 19: 0 images\n",
      "Superclass 20: 0 images\n",
      "\n",
      "Client 2 superclass distribution:\n",
      "Superclass 1: 0 images\n",
      "Superclass 2: 0 images\n",
      "Superclass 3: 0 images\n",
      "Superclass 4: 0 images\n",
      "Superclass 5: 0 images\n",
      "Superclass 6: 0 images\n",
      "Superclass 7: 2500 images\n",
      "Superclass 8: 2500 images\n",
      "Superclass 9: 2500 images\n",
      "Superclass 10: 2500 images\n",
      "Superclass 11: 2500 images\n",
      "Superclass 12: 2500 images\n",
      "Superclass 13: 2500 images\n",
      "Superclass 14: 2500 images\n",
      "Superclass 15: 2500 images\n",
      "Superclass 16: 0 images\n",
      "Superclass 17: 0 images\n",
      "Superclass 18: 0 images\n",
      "Superclass 19: 0 images\n",
      "Superclass 20: 0 images\n",
      "\n",
      "Client 3 superclass distribution:\n",
      "Superclass 1: 0 images\n",
      "Superclass 2: 0 images\n",
      "Superclass 3: 0 images\n",
      "Superclass 4: 0 images\n",
      "Superclass 5: 0 images\n",
      "Superclass 6: 0 images\n",
      "Superclass 7: 0 images\n",
      "Superclass 8: 0 images\n",
      "Superclass 9: 0 images\n",
      "Superclass 10: 0 images\n",
      "Superclass 11: 0 images\n",
      "Superclass 12: 0 images\n",
      "Superclass 13: 2500 images\n",
      "Superclass 14: 2500 images\n",
      "Superclass 15: 2500 images\n",
      "Superclass 16: 2500 images\n",
      "Superclass 17: 2500 images\n",
      "Superclass 18: 2500 images\n",
      "Superclass 19: 2000 images\n",
      "Superclass 20: 2000 images\n",
      "Client 1 Training superclass distribution:\n",
      "Superclass 1: 2500 images\n",
      "Superclass 2: 2500 images\n",
      "Superclass 3: 2500 images\n",
      "Superclass 4: 2500 images\n",
      "Superclass 5: 2500 images\n",
      "Superclass 6: 2500 images\n",
      "Superclass 7: 2500 images\n",
      "Superclass 8: 2500 images\n",
      "Superclass 9: 2500 images\n",
      "Superclass 10: 0 images\n",
      "Superclass 11: 0 images\n",
      "Superclass 12: 0 images\n",
      "Superclass 13: 0 images\n",
      "Superclass 14: 0 images\n",
      "Superclass 15: 0 images\n",
      "Superclass 16: 0 images\n",
      "Superclass 17: 0 images\n",
      "Superclass 18: 0 images\n",
      "Superclass 19: 0 images\n",
      "Superclass 20: 0 images\n",
      "\n",
      "Client 1 Validation superclass distribution:\n",
      "Superclass 1: 255 images\n",
      "Superclass 2: 271 images\n",
      "Superclass 3: 233 images\n",
      "Superclass 4: 250 images\n",
      "Superclass 5: 264 images\n",
      "Superclass 6: 253 images\n",
      "Superclass 7: 233 images\n",
      "Superclass 8: 258 images\n",
      "Superclass 9: 233 images\n",
      "Superclass 10: 0 images\n",
      "Superclass 11: 0 images\n",
      "Superclass 12: 0 images\n",
      "Superclass 13: 0 images\n",
      "Superclass 14: 0 images\n",
      "Superclass 15: 0 images\n",
      "Superclass 16: 0 images\n",
      "Superclass 17: 0 images\n",
      "Superclass 18: 0 images\n",
      "Superclass 19: 0 images\n",
      "Superclass 20: 0 images\n",
      "\n",
      "Client 2 Training superclass distribution:\n",
      "Superclass 1: 0 images\n",
      "Superclass 2: 0 images\n",
      "Superclass 3: 0 images\n",
      "Superclass 4: 0 images\n",
      "Superclass 5: 0 images\n",
      "Superclass 6: 0 images\n",
      "Superclass 7: 2500 images\n",
      "Superclass 8: 2500 images\n",
      "Superclass 9: 2500 images\n",
      "Superclass 10: 2500 images\n",
      "Superclass 11: 2500 images\n",
      "Superclass 12: 2500 images\n",
      "Superclass 13: 2500 images\n",
      "Superclass 14: 2500 images\n",
      "Superclass 15: 2500 images\n",
      "Superclass 16: 0 images\n",
      "Superclass 17: 0 images\n",
      "Superclass 18: 0 images\n",
      "Superclass 19: 0 images\n",
      "Superclass 20: 0 images\n",
      "\n",
      "Client 2 Validation superclass distribution:\n",
      "Superclass 1: 0 images\n",
      "Superclass 2: 0 images\n",
      "Superclass 3: 0 images\n",
      "Superclass 4: 0 images\n",
      "Superclass 5: 0 images\n",
      "Superclass 6: 0 images\n",
      "Superclass 7: 264 images\n",
      "Superclass 8: 269 images\n",
      "Superclass 9: 244 images\n",
      "Superclass 10: 249 images\n",
      "Superclass 11: 234 images\n",
      "Superclass 12: 245 images\n",
      "Superclass 13: 269 images\n",
      "Superclass 14: 238 images\n",
      "Superclass 15: 238 images\n",
      "Superclass 16: 0 images\n",
      "Superclass 17: 0 images\n",
      "Superclass 18: 0 images\n",
      "Superclass 19: 0 images\n",
      "Superclass 20: 0 images\n",
      "\n",
      "Client 3 Training superclass distribution:\n",
      "Superclass 1: 0 images\n",
      "Superclass 2: 0 images\n",
      "Superclass 3: 0 images\n",
      "Superclass 4: 0 images\n",
      "Superclass 5: 0 images\n",
      "Superclass 6: 0 images\n",
      "Superclass 7: 0 images\n",
      "Superclass 8: 0 images\n",
      "Superclass 9: 0 images\n",
      "Superclass 10: 0 images\n",
      "Superclass 11: 0 images\n",
      "Superclass 12: 0 images\n",
      "Superclass 13: 2500 images\n",
      "Superclass 14: 2500 images\n",
      "Superclass 15: 2500 images\n",
      "Superclass 16: 2500 images\n",
      "Superclass 17: 2500 images\n",
      "Superclass 18: 2500 images\n",
      "Superclass 19: 2000 images\n",
      "Superclass 20: 2000 images\n",
      "\n",
      "Client 3 Validation superclass distribution:\n",
      "Superclass 1: 0 images\n",
      "Superclass 2: 0 images\n",
      "Superclass 3: 0 images\n",
      "Superclass 4: 0 images\n",
      "Superclass 5: 0 images\n",
      "Superclass 6: 0 images\n",
      "Superclass 7: 0 images\n",
      "Superclass 8: 0 images\n",
      "Superclass 9: 0 images\n",
      "Superclass 10: 0 images\n",
      "Superclass 11: 0 images\n",
      "Superclass 12: 0 images\n",
      "Superclass 13: 251 images\n",
      "Superclass 14: 234 images\n",
      "Superclass 15: 240 images\n",
      "Superclass 16: 241 images\n",
      "Superclass 17: 269 images\n",
      "Superclass 18: 267 images\n",
      "Superclass 19: 210 images\n",
      "Superclass 20: 188 images\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to count occurrences of each superclass in a dataset\n",
    "def count_superclasses(dataset, superclasses):\n",
    "    superclass_counts = {k: 0 for k in superclasses.keys()}\n",
    "    \n",
    "    for _, target in dataset:\n",
    "        subclass = target\n",
    "        superclass = get_superclass(subclass, superclasses)\n",
    "        if superclass is not None:\n",
    "            superclass_counts[superclass] += 1\n",
    "            \n",
    "    return superclass_counts\n",
    "\n",
    "# After filtering the dataset for each client\n",
    "client1_superclass_counts = count_superclasses(client1_dataset, superclasses)\n",
    "client2_superclass_counts = count_superclasses(client2_dataset, superclasses)\n",
    "client3_superclass_counts = count_superclasses(client3_dataset, superclasses)\n",
    "\n",
    "# Print the distribution of superclasses for each client\n",
    "print(\"Client 1 superclass distribution:\")\n",
    "for superclass, count in client1_superclass_counts.items():\n",
    "    print(f\"Superclass {superclass}: {count} images\")\n",
    "\n",
    "print(\"\\nClient 2 superclass distribution:\")\n",
    "for superclass, count in client2_superclass_counts.items():\n",
    "    print(f\"Superclass {superclass}: {count} images\")\n",
    "    \n",
    "print(\"\\nClient 3 superclass distribution:\")\n",
    "for superclass, count in client3_superclass_counts.items():\n",
    "    print(f\"Superclass {superclass}: {count} images\")\n",
    "    \n",
    "    \n",
    "# Count and print the distribution of superclasses for a given dataset\n",
    "def print_superclass_distribution(dataset_name, dataset, superclasses):\n",
    "    counts = count_superclasses(dataset, superclasses)\n",
    "    print(f\"{dataset_name} superclass distribution:\")\n",
    "    for superclass, count in counts.items():\n",
    "        print(f\"Superclass {superclass}: {count} images\")\n",
    "    print()\n",
    "\n",
    "# After filtering the dataset for each client and validation sets\n",
    "print_superclass_distribution(\"Client 1 Training\", client1_dataset, superclasses)\n",
    "print_superclass_distribution(\"Client 1 Validation\", val_dataset_client1, superclasses)\n",
    "print_superclass_distribution(\"Client 2 Training\", client2_dataset, superclasses)\n",
    "print_superclass_distribution(\"Client 2 Validation\", val_dataset_client2, superclasses)\n",
    "print_superclass_distribution(\"Client 3 Training\", client3_dataset, superclasses)\n",
    "print_superclass_distribution(\"Client 3 Validation\", val_dataset_client3, superclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Federated Training Functions (Federated Averaging (FedAvg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.to(device)  \n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def federated_averaging(state_dicts):\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "    return avg_state_dict\n",
    "\n",
    "def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.001, patience=5, min_delta=0):\n",
    "    model = prepare_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for round in range(num_epochs):\n",
    "        print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "        state_dicts = []\n",
    "        for i, client_loader in enumerate(client_loaders):\n",
    "            print(f\"Training model for client {i+1}...\")\n",
    "            client_model = prepare_model().to(device)\n",
    "            client_model.load_state_dict(model.state_dict())\n",
    "            optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "            client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "            state_dicts.append(client_state_dict)\n",
    "\n",
    "        avg_state_dict = federated_averaging(state_dicts)\n",
    "        model.load_state_dict(avg_state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Validation phase (handling multiple validation loaders)\n",
    "        print(\"Validating model...\")\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_loader in val_loaders:\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Test phase\n",
    "    print(\"Testing model...\")\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Federated Training Functions (Adaptive Federated Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "#     model.to(device)  \n",
    "#     model.train()\n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)  \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts, global_model, beta=0.9):\n",
    "#     avg_state_dict = global_model.state_dict()\n",
    "#     for key in avg_state_dict.keys():\n",
    "#         if avg_state_dict[key].dtype == torch.long:\n",
    "#             avg_state_dict[key] = torch.zeros_like(avg_state_dict[key], dtype=torch.float32)\n",
    "#             for state_dict in state_dicts:\n",
    "#                 avg_state_dict[key] += state_dict[key].float() / len(state_dicts)\n",
    "#             avg_state_dict[key] = avg_state_dict[key].long()  # Convert back to long if necessary\n",
    "#         else:\n",
    "#             avg_state_dict[key] = torch.zeros_like(avg_state_dict[key])\n",
    "#             for state_dict in state_dicts:\n",
    "#                 avg_state_dict[key] += state_dict[key] / len(state_dicts)\n",
    "#             avg_state_dict[key] = beta * avg_state_dict[key] + (1 - beta) * global_model.state_dict()[key]\n",
    "#     return avg_state_dict\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate=0.001, patience=5, min_delta=0):\n",
    "#     model = prepare_model().to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         avg_state_dict = federated_averaging(state_dicts, model)  # Call to modified function\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Validation phase\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_running_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / len(val_loader)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define the Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def log_experiment_result(filename, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['num_clients', 'num_epochs', 'learning_rate', 'patience', 'min_delta', 'val_loss', 'val_accuracy', 'test_loss', 'test_accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only once\n",
    "        \n",
    "        writer.writerow({\n",
    "            'num_clients': num_clients,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'patience': patience,\n",
    "            'min_delta': min_delta,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loaders:  [<torch.utils.data.dataloader.DataLoader object at 0x7f5b744e4c40>, <torch.utils.data.dataloader.DataLoader object at 0x7f5b744e4b20>, <torch.utils.data.dataloader.DataLoader object at 0x7f5b744dcd90>]\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Loaders: \", val_loaders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Federated Learning Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated learning round 1/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 1/40, Val Loss: 3.7057, Val Accuracy: 20.50%\n",
      "Starting federated learning round 2/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 2/40, Val Loss: 3.4868, Val Accuracy: 26.89%\n",
      "Starting federated learning round 3/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 3/40, Val Loss: 3.5577, Val Accuracy: 28.75%\n",
      "Starting federated learning round 4/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 4/40, Val Loss: 3.3289, Val Accuracy: 31.38%\n",
      "Starting federated learning round 5/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 5/40, Val Loss: 3.2063, Val Accuracy: 34.30%\n",
      "Starting federated learning round 6/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 6/40, Val Loss: 3.0630, Val Accuracy: 35.55%\n",
      "Starting federated learning round 7/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 7/40, Val Loss: 3.0328, Val Accuracy: 36.33%\n",
      "Starting federated learning round 8/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 8/40, Val Loss: 2.8712, Val Accuracy: 38.23%\n",
      "Starting federated learning round 9/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 9/40, Val Loss: 2.7766, Val Accuracy: 39.17%\n",
      "Starting federated learning round 10/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 10/40, Val Loss: 2.7213, Val Accuracy: 41.41%\n",
      "Starting federated learning round 11/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 11/40, Val Loss: 2.6556, Val Accuracy: 41.55%\n",
      "Starting federated learning round 12/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 12/40, Val Loss: 2.5725, Val Accuracy: 42.89%\n",
      "Starting federated learning round 13/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 13/40, Val Loss: 2.4936, Val Accuracy: 43.88%\n",
      "Starting federated learning round 14/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 14/40, Val Loss: 2.3990, Val Accuracy: 45.66%\n",
      "Starting federated learning round 15/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 15/40, Val Loss: 2.4512, Val Accuracy: 46.52%\n",
      "Starting federated learning round 16/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 16/40, Val Loss: 2.3050, Val Accuracy: 46.91%\n",
      "Starting federated learning round 17/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 17/40, Val Loss: 2.3658, Val Accuracy: 47.19%\n",
      "Starting federated learning round 18/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 18/40, Val Loss: 2.2365, Val Accuracy: 48.83%\n",
      "Starting federated learning round 19/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 19/40, Val Loss: 2.1444, Val Accuracy: 50.61%\n",
      "Starting federated learning round 20/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 20/40, Val Loss: 2.1902, Val Accuracy: 49.72%\n",
      "Starting federated learning round 21/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 21/40, Val Loss: 2.1998, Val Accuracy: 49.53%\n",
      "Starting federated learning round 22/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 22/40, Val Loss: 2.1699, Val Accuracy: 51.12%\n",
      "Starting federated learning round 23/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 23/40, Val Loss: 2.0382, Val Accuracy: 53.22%\n",
      "Starting federated learning round 24/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 24/40, Val Loss: 2.0959, Val Accuracy: 52.16%\n",
      "Starting federated learning round 25/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 25/40, Val Loss: 2.0736, Val Accuracy: 54.00%\n",
      "Starting federated learning round 26/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 26/40, Val Loss: 2.0007, Val Accuracy: 53.31%\n",
      "Starting federated learning round 27/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 27/40, Val Loss: 1.9582, Val Accuracy: 54.33%\n",
      "Starting federated learning round 28/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 28/40, Val Loss: 1.9075, Val Accuracy: 55.95%\n",
      "Starting federated learning round 29/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 29/40, Val Loss: 1.9155, Val Accuracy: 55.12%\n",
      "Starting federated learning round 30/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 30/40, Val Loss: 1.8465, Val Accuracy: 56.17%\n",
      "Starting federated learning round 31/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 31/40, Val Loss: 1.8311, Val Accuracy: 56.12%\n",
      "Starting federated learning round 32/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 32/40, Val Loss: 1.8338, Val Accuracy: 56.56%\n",
      "Starting federated learning round 33/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 33/40, Val Loss: 1.7942, Val Accuracy: 58.16%\n",
      "Starting federated learning round 34/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 34/40, Val Loss: 1.7675, Val Accuracy: 57.67%\n",
      "Starting federated learning round 35/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 35/40, Val Loss: 1.7224, Val Accuracy: 58.67%\n",
      "Starting federated learning round 36/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 36/40, Val Loss: 1.7724, Val Accuracy: 58.02%\n",
      "Starting federated learning round 37/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 37/40, Val Loss: 1.8190, Val Accuracy: 57.09%\n",
      "Starting federated learning round 38/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 38/40, Val Loss: 1.7050, Val Accuracy: 59.42%\n",
      "Starting federated learning round 39/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 39/40, Val Loss: 1.6520, Val Accuracy: 60.58%\n",
      "Starting federated learning round 40/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Validating model...\n",
      "Federated Round 40/40, Val Loss: 1.6717, Val Accuracy: 60.78%\n",
      "Testing model...\n",
      "Test Loss: 2.4625, Test Accuracy: 50.61%\n",
      "Federated training complete.\n",
      "Experiment results logged.\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_clients = 3\n",
    "num_epochs = 40\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "min_delta = 0.01\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "log_file = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/experiment_results.csv'\n",
    "\n",
    "\n",
    "# Initialize Data Loaders\n",
    "#client_loaders, val_loader = get_data_loaders(batch_size, num_workers, num_clients)\n",
    "#test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(\"Starting federated training...\")\n",
    "model, val_loss, val_accuracy, test_loss, test_accuracy = train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate, patience, min_delta)\n",
    "print(\"Federated training complete.\")\n",
    "\n",
    "# Log the results\n",
    "log_experiment_result(log_file, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy)\n",
    "print(\"Experiment results logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
