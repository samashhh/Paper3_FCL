{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Check for CUDA GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "dataset_train = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define superclasses and subclasses\n",
    "superclasses = {\n",
    "    1: [4, 30, 55, 72, 95],  # aquatic mammals\n",
    "    2: [1, 32, 67, 73, 91],  # fish\n",
    "    3: [54, 62, 70, 82, 92],  # flowers\n",
    "    4: [9, 10, 16, 28, 61],  # food containers\n",
    "    5: [0, 51, 53, 57, 83],  # fruit and vegetables\n",
    "    6: [22, 39, 40, 86, 87],  # household electrical devices\n",
    "    7: [5, 20, 25, 84, 94],  # household furniture\n",
    "    8: [6, 7, 14, 18, 24],  # insects\n",
    "    9: [3, 42, 43, 88, 97],  # large carnivores\n",
    "    10: [12, 17, 37, 68, 76],  # large man-made outdoor things\n",
    "    11: [23, 33, 49, 60, 71],  # large natural outdoor scenes\n",
    "    12: [15, 19, 21, 31, 38],  # medium-sized mammals\n",
    "    13: [34, 63, 64, 66, 75],  # non-insect invertebrates\n",
    "    14: [26, 45, 77, 79, 99],  # people\n",
    "    15: [2, 11, 35, 46, 98],  # reptiles\n",
    "    16: [27, 29, 44, 78, 93],  # small mammals\n",
    "    17: [36, 50, 65, 74, 80],  # trees\n",
    "    18: [8, 13, 48, 58, 90],  # vehicles 1\n",
    "    19: [41, 66, 69, 81, 89],  # vehicles 2\n",
    "    20: [47, 50, 52, 56, 59],  # household furniture\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to map subclass to its corresponding superclass\n",
    "# def get_superclass(subclass, superclasses):\n",
    "#     for superclass in superclasses.keys():\n",
    "#         subclasses=superclasses[superclass]\n",
    "#         if subclass in subclasses:\n",
    "#             return superclass\n",
    "#     return None\n",
    "\n",
    "# # Function to filter dataset by superclasses using dataset.targets directly\n",
    "# def filter_dataset_by_superclass(dataset, superclasses, selected_superclasses):\n",
    "#     selected_indices = []\n",
    "    \n",
    "#     # Directly access dataset.targets, assuming the dataset is labeled (like CIFAR-100)\n",
    "#     for idx, target in enumerate(dataset.targets):  # Access only targets, not full data\n",
    "#         subclass = target\n",
    "#         superclass = get_superclass(subclass, superclasses)\n",
    "        \n",
    "#         if superclass in selected_superclasses:\n",
    "#             selected_indices.append(idx)\n",
    "\n",
    "#     return Subset(dataset, selected_indices)\n",
    "\n",
    "# # Define the superclasses for each client\n",
    "# client1_superclasses = list(range(1, 11))\n",
    "# client2_superclasses = list(range(11, 21))\n",
    "\n",
    "# # Filter the dataset for each client\n",
    "# client1_dataset = filter_dataset_by_superclass(dataset_train, superclasses, client1_superclasses)\n",
    "# client2_dataset = filter_dataset_by_superclass(dataset_train, superclasses, client2_superclasses)\n",
    "\n",
    "# # Print dataset sizes to verify\n",
    "# print(f'Client 1 dataset size: {len(client1_dataset)}')\n",
    "# print(f'Client 2 dataset size: {len(client2_dataset)}')\n",
    "\n",
    "# # Create DataLoaders for each client\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "\n",
    "# client_loaders = [\n",
    "#     DataLoader(client1_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "#     DataLoader(client2_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "# ]\n",
    "\n",
    "# # Test DataLoader (common for all clients)\n",
    "# test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# # Checking the first client loader to see if everything works correctly\n",
    "# for images, labels in client_loaders[0]:\n",
    "#     print(f'Client 1 - Batch size: {images.size(0)}')\n",
    "#     print(f'Images shape: {images.shape}')\n",
    "#     print(f'Labels shape: {labels.shape}')\n",
    "#     print(f'Labels: {labels}')\n",
    "#     break\n",
    "\n",
    "# # Checking the second client loader to see if everything works correctly\n",
    "# for images, labels in client_loaders[1]:\n",
    "#     print(f'Client 2 - Batch size: {images.size(0)}')\n",
    "#     print(f'Images shape: {images.shape}')\n",
    "#     print(f'Labels shape: {labels.shape}')\n",
    "#     print(f'Labels: {labels}')\n",
    "#     break\n",
    "\n",
    "# print(\"Preprocessing for federated learning with specific superclasses completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 training set size: 18000\n",
      "Client 1 validation set size: 2000\n",
      "Client 2 training set size: 26100\n",
      "Client 2 validation set size: 2900\n",
      "Client data sizes: [18000, 26100]\n"
     ]
    }
   ],
   "source": [
    "# Function to randomly split subclasses between two clients, ensuring all superclasses are present in both\n",
    "def split_subclasses_for_both_clients(superclasses):\n",
    "    client1_subclasses = []\n",
    "    client2_subclasses = []\n",
    "    \n",
    "    for superclass, subclasses in superclasses.items():\n",
    "        random.shuffle(subclasses)\n",
    "        half = len(subclasses) // 2\n",
    "        # Both clients get some subclasses from each superclass\n",
    "        client1_subclasses.extend(subclasses[:half])\n",
    "        client2_subclasses.extend(subclasses[half:])\n",
    "        \n",
    "    return client1_subclasses, client2_subclasses\n",
    "\n",
    "# Function to filter dataset by subclasses\n",
    "def filter_dataset_by_subclasses(dataset, subclasses):\n",
    "    selected_indices = []\n",
    "    for idx, target in enumerate(dataset.targets):\n",
    "        if target in subclasses:\n",
    "            selected_indices.append(idx)\n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "# Split subclasses between the two clients (ensuring both clients have all superclasses)\n",
    "client1_subclasses, client2_subclasses = split_subclasses_for_both_clients(superclasses)\n",
    "\n",
    "# Filter dataset for each client based on the subclasses they received\n",
    "client1_dataset = filter_dataset_by_subclasses(dataset_train, client1_subclasses)\n",
    "client2_dataset = filter_dataset_by_subclasses(dataset_train, client2_subclasses)\n",
    "\n",
    "# Function to create train/validation split using PyTorch's random_split\n",
    "def split_train_val(dataset, val_size=0.1):\n",
    "    val_len = int(len(dataset) * val_size)\n",
    "    train_len = len(dataset) - val_len\n",
    "    return random_split(dataset, [train_len, val_len])\n",
    "\n",
    "# Create train/validation split for each client using PyTorch's random_split\n",
    "client1_train, client1_val = split_train_val(client1_dataset, val_size=0.1)\n",
    "client2_train, client2_val = split_train_val(client2_dataset, val_size=0.1)\n",
    "\n",
    "# Print dataset sizes to verify\n",
    "print(f'Client 1 training set size: {len(client1_train)}')\n",
    "print(f'Client 1 validation set size: {len(client1_val)}')\n",
    "print(f'Client 2 training set size: {len(client2_train)}')\n",
    "print(f'Client 2 validation set size: {len(client2_val)}')\n",
    "\n",
    "# Set client_data_sizes based on the training set sizes\n",
    "client_data_sizes = [len(client1_train), len(client2_train)]\n",
    "print(f\"Client data sizes: {client_data_sizes}\")  # This should print [17550, 26550]\n",
    "\n",
    "\n",
    "# Create DataLoaders for each client\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "client1_loader = DataLoader(client1_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "client1_val_loader = DataLoader(client1_val, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "client2_loader = DataLoader(client2_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "client2_val_loader = DataLoader(client2_val, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Test DataLoader (common for all clients)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Create a list of DataLoaders for both clients\n",
    "client_loaders = [client1_loader, client2_loader]\n",
    "\n",
    "# Similarly, create a list of validation loaders\n",
    "val_loaders = [client1_val_loader, client2_val_loader]\n",
    "\n",
    "\n",
    "# Test DataLoader (common for all clients)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 superclasses present: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "Client 2 superclasses present: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n"
     ]
    }
   ],
   "source": [
    "# Function to map subclasses back to their superclasses\n",
    "def get_superclass_mapping(superclasses):\n",
    "    subclass_to_superclass = {}\n",
    "    for superclass, subclasses in superclasses.items():\n",
    "        for subclass in subclasses:\n",
    "            subclass_to_superclass[subclass] = superclass\n",
    "    return subclass_to_superclass\n",
    "\n",
    "# Check if all superclasses are present in the dataset\n",
    "def check_superclasses_present(dataset, subclass_to_superclass):\n",
    "    superclasses_present = set()\n",
    "    for idx in dataset.indices:\n",
    "        subclass = dataset.dataset.targets[idx]\n",
    "        superclass = subclass_to_superclass[subclass]\n",
    "        superclasses_present.add(superclass)\n",
    "    return superclasses_present\n",
    "\n",
    "# Get mapping from subclasses to superclasses\n",
    "subclass_to_superclass = get_superclass_mapping(superclasses)\n",
    "\n",
    "# Check for Client 1\n",
    "client1_superclasses_present = check_superclasses_present(client1_dataset, subclass_to_superclass)\n",
    "print(f\"Client 1 superclasses present: {sorted(client1_superclasses_present)}\")\n",
    "\n",
    "# Check for Client 2\n",
    "client2_superclasses_present = check_superclasses_present(client2_dataset, subclass_to_superclass)\n",
    "print(f\"Client 2 superclasses present: {sorted(client2_superclasses_present)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Federated Training Functions (Federated Averaging (FedAvg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.to(device)  \n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Federated Averaging function\n",
    "def weighted_federated_averaging(state_dicts, client_data_sizes):\n",
    "    total_data_points = sum(client_data_sizes)  # Total number of data points across all clients\n",
    "    print(f\"Total data points: {total_data_points}\")\n",
    "    \n",
    "    avg_state_dict = {}\n",
    "\n",
    "    # Initialize with zeros\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = torch.zeros_like(state_dicts[0][key])\n",
    "\n",
    "    # Weighted sum of client updates\n",
    "    for i, state_dict in enumerate(state_dicts):\n",
    "        client_weight = client_data_sizes[i] / total_data_points\n",
    "        for key in state_dict.keys():\n",
    "            # Only perform weighted sum in float for float-like tensors\n",
    "            if avg_state_dict[key].dtype in [torch.float32, torch.float64]:\n",
    "                avg_state_dict[key] += state_dict[key].float() * client_weight\n",
    "            else:  # For integer types (e.g., LongTensors), just add them directly\n",
    "                avg_state_dict[key] += state_dict[key]\n",
    "\n",
    "    return avg_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.001, patience=5, min_delta=0):\n",
    "#     model = prepare_model().to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         avg_state_dict = federated_averaging(state_dicts)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Validation phase\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for val_loader in val_loaders:\n",
    "#                 for inputs, labels in val_loader:\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     val_running_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_data_sizes = [len(client1_train), len(client2_train)]\n",
    "\n",
    "\n",
    "# Modify train_federated_model to accept client_data_sizes and use weighted averaging\n",
    "def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, client_data_sizes, learning_rate=0.0001, patience=5, min_delta=0):\n",
    "    \n",
    "    # Debugging print to ensure the function is called and client_data_sizes is passed\n",
    "    print(f'Train Federated Model function called with client_data_sizes: {client_data_sizes}')\n",
    "    \n",
    "    model = prepare_model().to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for round in range(num_epochs):\n",
    "        print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "        state_dicts = []\n",
    "        for i, client_loader in enumerate(client_loaders):\n",
    "            print(f\"Training model for client {i+1}...\")\n",
    "            client_model = prepare_model().to(device)\n",
    "            client_model.load_state_dict(model.state_dict())\n",
    "            optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "            client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "            state_dicts.append(client_state_dict)\n",
    "            \n",
    "        # Check client_data_sizes before passing it to weighted_federated_averaging\n",
    "        print(f'Client data sizes: {client_data_sizes}')\n",
    "\n",
    "        # Use the new weighted federated averaging\n",
    "        avg_state_dict = weighted_federated_averaging(state_dicts, client_data_sizes)\n",
    "        model.load_state_dict(avg_state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Validation phase\n",
    "        print(\"Validating model...\")\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_loader in val_loaders:\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Test phase\n",
    "    print(\"Testing model...\")\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Federated Training Functions (Adaptive Federated Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "#     model.to(device)  \n",
    "#     model.train()\n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)  \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts, global_model, beta=0.9):\n",
    "#     avg_state_dict = global_model.state_dict()\n",
    "#     for key in avg_state_dict.keys():\n",
    "#         if avg_state_dict[key].dtype == torch.long:\n",
    "#             avg_state_dict[key] = torch.zeros_like(avg_state_dict[key], dtype=torch.float32)\n",
    "#             for state_dict in state_dicts:\n",
    "#                 avg_state_dict[key] += state_dict[key].float() / len(state_dicts)\n",
    "#             avg_state_dict[key] = avg_state_dict[key].long()  # Convert back to long if necessary\n",
    "#         else:\n",
    "#             avg_state_dict[key] = torch.zeros_like(avg_state_dict[key])\n",
    "#             for state_dict in state_dicts:\n",
    "#                 avg_state_dict[key] += state_dict[key] / len(state_dicts)\n",
    "#             avg_state_dict[key] = beta * avg_state_dict[key] + (1 - beta) * global_model.state_dict()[key]\n",
    "#     return avg_state_dict\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate=0.001, patience=5, min_delta=0):\n",
    "#     model = prepare_model().to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         avg_state_dict = federated_averaging(state_dicts, model)  # Call to modified function\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "\n",
    "#         # Validation phase\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_running_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / len(val_loader)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define the Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_clients = 2\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "patience = 5\n",
    "min_delta = 0.01\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "log_file = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/experiment_results.csv'\n",
    "\n",
    "\n",
    "def log_experiment_result(filename, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['num_clients', 'num_epochs', 'learning_rate', 'patience', 'min_delta', 'val_loss', 'val_accuracy', 'test_loss', 'test_accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only once\n",
    "        \n",
    "        writer.writerow({\n",
    "            'num_clients': num_clients,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'patience': patience,\n",
    "            'min_delta': min_delta,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18000, 26100]\n"
     ]
    }
   ],
   "source": [
    "# Correctly define client_data_sizes based on the size of each client's dataset\n",
    "client_data_sizes = [len(client1_train), len(client2_train)]  # Size of the training sets for both clients|\n",
    "\n",
    "print(client_data_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Federated Learning Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training...\n",
      "Train Federated Model function called with client_data_sizes: [18000, 26100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated learning round 1/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 1/40, Val Loss: 3.3013, Val Accuracy: 31.16%\n",
      "Starting federated learning round 2/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 2/40, Val Loss: 2.7653, Val Accuracy: 36.14%\n",
      "Starting federated learning round 3/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 3/40, Val Loss: 2.5211, Val Accuracy: 39.41%\n",
      "Starting federated learning round 4/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 4/40, Val Loss: 2.4000, Val Accuracy: 41.80%\n",
      "Starting federated learning round 5/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 5/40, Val Loss: 2.2964, Val Accuracy: 43.24%\n",
      "Starting federated learning round 6/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 6/40, Val Loss: 2.2186, Val Accuracy: 44.73%\n",
      "Starting federated learning round 7/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 7/40, Val Loss: 2.1218, Val Accuracy: 46.43%\n",
      "Starting federated learning round 8/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 8/40, Val Loss: 2.0293, Val Accuracy: 48.16%\n",
      "Starting federated learning round 9/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 9/40, Val Loss: 1.9999, Val Accuracy: 49.24%\n",
      "Starting federated learning round 10/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 10/40, Val Loss: 1.9159, Val Accuracy: 50.14%\n",
      "Starting federated learning round 11/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 11/40, Val Loss: 1.8343, Val Accuracy: 52.35%\n",
      "Starting federated learning round 12/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 12/40, Val Loss: 1.7960, Val Accuracy: 53.43%\n",
      "Starting federated learning round 13/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 13/40, Val Loss: 1.7484, Val Accuracy: 54.96%\n",
      "Starting federated learning round 14/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 14/40, Val Loss: 1.7182, Val Accuracy: 55.18%\n",
      "Starting federated learning round 15/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 15/40, Val Loss: 1.6740, Val Accuracy: 56.80%\n",
      "Starting federated learning round 16/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 16/40, Val Loss: 1.6292, Val Accuracy: 57.31%\n",
      "Starting federated learning round 17/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 17/40, Val Loss: 1.6355, Val Accuracy: 56.29%\n",
      "Starting federated learning round 18/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 18/40, Val Loss: 1.6040, Val Accuracy: 57.41%\n",
      "Starting federated learning round 19/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 19/40, Val Loss: 1.5787, Val Accuracy: 58.41%\n",
      "Starting federated learning round 20/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 20/40, Val Loss: 1.5541, Val Accuracy: 59.55%\n",
      "Starting federated learning round 21/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 21/40, Val Loss: 1.5394, Val Accuracy: 59.14%\n",
      "Starting federated learning round 22/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 22/40, Val Loss: 1.5369, Val Accuracy: 59.14%\n",
      "Starting federated learning round 23/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 23/40, Val Loss: 1.5514, Val Accuracy: 58.35%\n",
      "Starting federated learning round 24/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 24/40, Val Loss: 1.5226, Val Accuracy: 59.78%\n",
      "Starting federated learning round 25/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 25/40, Val Loss: 1.5182, Val Accuracy: 59.71%\n",
      "Starting federated learning round 26/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 26/40, Val Loss: 1.4942, Val Accuracy: 61.06%\n",
      "Starting federated learning round 27/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 27/40, Val Loss: 1.4938, Val Accuracy: 60.82%\n",
      "Starting federated learning round 28/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 28/40, Val Loss: 1.5128, Val Accuracy: 59.65%\n",
      "Starting federated learning round 29/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 29/40, Val Loss: 1.4954, Val Accuracy: 60.76%\n",
      "Starting federated learning round 30/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 30/40, Val Loss: 1.4878, Val Accuracy: 60.53%\n",
      "Starting federated learning round 31/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 31/40, Val Loss: 1.4637, Val Accuracy: 61.65%\n",
      "Starting federated learning round 32/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 32/40, Val Loss: 1.4881, Val Accuracy: 61.24%\n",
      "Starting federated learning round 33/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 33/40, Val Loss: 1.4437, Val Accuracy: 62.29%\n",
      "Starting federated learning round 34/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 34/40, Val Loss: 1.4564, Val Accuracy: 62.12%\n",
      "Starting federated learning round 35/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 35/40, Val Loss: 1.4210, Val Accuracy: 61.84%\n",
      "Starting federated learning round 36/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 36/40, Val Loss: 1.4555, Val Accuracy: 61.57%\n",
      "Starting federated learning round 37/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 37/40, Val Loss: 1.4763, Val Accuracy: 61.57%\n",
      "Starting federated learning round 38/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 38/40, Val Loss: 1.5079, Val Accuracy: 60.08%\n",
      "Starting federated learning round 39/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 39/40, Val Loss: 1.4423, Val Accuracy: 61.45%\n",
      "Starting federated learning round 40/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Client data sizes: [18000, 26100]\n",
      "Total data points: 44100\n",
      "Validating model...\n",
      "Federated Round 40/40, Val Loss: 1.4259, Val Accuracy: 62.92%\n",
      "Early stopping\n",
      "Testing model...\n",
      "Test Loss: 1.8364, Test Accuracy: 61.91%\n",
      "Federated training complete.\n",
      "Experiment results logged.\n"
     ]
    }
   ],
   "source": [
    "# Federated training\n",
    "print(\"Starting federated training...\")\n",
    "\n",
    "# Call train_federated_model with correct parameters\n",
    "model, val_loss, val_accuracy, test_loss, test_accuracy = train_federated_model(\n",
    "    client_loaders, val_loaders, test_loader, num_clients, num_epochs, client_data_sizes, learning_rate, patience, min_delta\n",
    ")\n",
    "\n",
    "print(\"Federated training complete.\")\n",
    "\n",
    "# Log the results\n",
    "log_experiment_result(\n",
    "    log_file, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy\n",
    ")\n",
    "\n",
    "print(\"Experiment results logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
