{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Check for CUDA GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "dataset_train = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code performs two different preprocessing methods for splitting the CIFAR-100 dataset between two clients, simulating two sorting plants for plastic recycling:\n",
    "\n",
    "### Preprocessing Method 1: Major/Minor Superclass Split\n",
    "- Each client handles **different superclasses** as \"major\" and \"minor.\"\n",
    "- Client 1 gets superclasses 1-10 as major and 11-20 as minor.\n",
    "- Client 2 gets superclasses 11-20 as major and 1-10 as minor.\n",
    "- Ensures non-overlapping data between clients based on superclasses., ensuring no object (image) is shared between clients, reflecting real-world sorting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Transformations\n",
    "# # transform = transforms.Compose([\n",
    "# #     transforms.RandomResizedCrop(224),\n",
    "# #     transforms.RandomHorizontalFlip(),\n",
    "# #     transforms.ToTensor(),\n",
    "# #     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# # ])\n",
    "\n",
    "# # # Load CIFAR-100 dataset\n",
    "# # dataset_train = datasets.CIFAR100(root='../data', train=True, download=True, transform=transform)\n",
    "# # dataset_test = datasets.CIFAR100(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# # Define superclasses and subclasses\n",
    "# superclasses = {\n",
    "#     1: [4, 30, 55, 72, 95],  # aquatic mammals\n",
    "#     2: [1, 32, 67, 73, 91],  # fish\n",
    "#     3: [54, 62, 70, 82, 92],  # flowers\n",
    "#     4: [9, 10, 16, 28, 61],  # food containers\n",
    "#     5: [0, 51, 53, 57, 83],  # fruit and vegetables\n",
    "#     6: [22, 39, 40, 86, 87],  # household electrical devices\n",
    "#     7: [5, 20, 25, 84, 94],  # household furniture\n",
    "#     8: [6, 7, 14, 18, 24],  # insects\n",
    "#     9: [3, 42, 43, 88, 97],  # large carnivores\n",
    "#     10: [12, 17, 37, 68, 76],  # large man-made outdoor things\n",
    "#     11: [23, 33, 49, 60, 71],  # large natural outdoor scenes\n",
    "#     12: [15, 19, 21, 31, 38],  # medium-sized mammals\n",
    "#     13: [34, 63, 64, 66, 75],  # non-insect invertebrates\n",
    "#     14: [26, 45, 77, 79, 99],  # people\n",
    "#     15: [2, 11, 35, 46, 98],  # reptiles\n",
    "#     16: [27, 29, 44, 78, 93],  # small mammals\n",
    "#     17: [36, 50, 65, 74, 80],  # trees\n",
    "#     18: [8, 13, 48, 58, 90],  # vehicles 1\n",
    "#     19: [41, 66, 69, 81, 89],  # vehicles 2\n",
    "#     20: [47, 50, 52, 56, 59],  # household furniture\n",
    "# }\n",
    "\n",
    "# # Function to map subclass to its corresponding superclass\n",
    "# def get_superclass(subclass, superclasses):\n",
    "#     for superclass, subclasses in superclasses.items():\n",
    "#         if subclass in subclasses:\n",
    "#             return superclass\n",
    "#     return None\n",
    "\n",
    "# # Function to filter dataset by superclasses\n",
    "# def filter_dataset_by_superclass(dataset, superclasses, major_classes, minor_classes):\n",
    "#     major_indices = []\n",
    "#     minor_indices = []\n",
    "#     for idx, (data, target) in enumerate(dataset):\n",
    "#         subclass = target\n",
    "#         superclass = get_superclass(subclass, superclasses)\n",
    "#         if superclass in major_classes and major_classes[superclass] > 0:\n",
    "#             major_indices.append(idx)\n",
    "#             major_classes[superclass] -= 1\n",
    "#         elif superclass in minor_classes and minor_classes[superclass] > 0:\n",
    "#             minor_indices.append(idx)\n",
    "#             minor_classes[superclass] -= 1\n",
    "\n",
    "#     major_subset = Subset(dataset, major_indices)\n",
    "#     minor_subset = Subset(dataset, minor_indices)\n",
    "#     return major_subset, minor_subset\n",
    "\n",
    "\n",
    "\n",
    "# ###### use for 80/20 split\n",
    "# # # Define the number of images to sample from each superclass for both clients\n",
    "# # major_count = 2000\n",
    "# # minor_count = 500\n",
    "\n",
    "\n",
    "# # ###### use for 50/50 split\n",
    "# # # 50/50 Split for Major and Minor Classes\n",
    "# # # Each superclass will contribute 50% to major and 50% to minor superclasses\n",
    "# major_count = 1250  # 50% of 2500 (total images per superclass)\n",
    "# minor_count = 1250  # 50% of 2500\n",
    "\n",
    "\n",
    "\n",
    "# # Define which superclasses are major for each client\n",
    "# client1_major_classes = {k: major_count for k in range(1, 11)}\n",
    "# client1_minor_classes = {k: minor_count for k in range(11, 21)}\n",
    "\n",
    "# client2_major_classes = {k: major_count for k in range(11, 21)}\n",
    "# client2_minor_classes = {k: minor_count for k in range(1, 11)}\n",
    "\n",
    "# # Filter dataset for each client\n",
    "# client1_major, client1_minor = filter_dataset_by_superclass(dataset_train, superclasses, client1_major_classes, client1_minor_classes)\n",
    "# client2_major, client2_minor = filter_dataset_by_superclass(dataset_train, superclasses, client2_major_classes, client2_minor_classes)\n",
    "\n",
    "# # Combine major and minor subsets for each client\n",
    "# client1_indices = list(client1_major.indices) + list(client1_minor.indices)\n",
    "# client2_indices = list(client2_major.indices) + list(client2_minor.indices)\n",
    "\n",
    "# client1_dataset = Subset(dataset_train, client1_indices)\n",
    "# client2_dataset = Subset(dataset_train, client2_indices)\n",
    "\n",
    "# # Split a portion of client1_dataset for validation\n",
    "# val_split = 0.1  # 10% for validation\n",
    "\n",
    "\n",
    "# # val_size = int(val_split * len(client1_dataset))\n",
    "# # train_size = len(client1_dataset) - val_size\n",
    "\n",
    "# # train_dataset, val_dataset = random_split(client1_dataset, [train_size, val_size])\n",
    "\n",
    "# # # Update client1_dataset to exclude validation data\n",
    "# # client1_dataset = train_dataset\n",
    "\n",
    "\n",
    "# # For client1\n",
    "# val_size_client1 = int(val_split * len(client1_dataset))\n",
    "# train_size_client1 = len(client1_dataset) - val_size_client1\n",
    "# train_dataset_client1, val_dataset_client1 = random_split(client1_dataset, [train_size_client1, val_size_client1])\n",
    "\n",
    "# # Update client1_dataset to exclude validation data\n",
    "# client1_dataset = train_dataset_client1\n",
    "\n",
    "# # For client2\n",
    "# val_size_client2 = int(val_split * len(client2_dataset))\n",
    "# train_size_client2 = len(client2_dataset) - val_size_client2\n",
    "# train_dataset_client2, val_dataset_client2 = random_split(client2_dataset, [train_size_client2, val_size_client2])\n",
    "\n",
    "# # Update client2_dataset to exclude validation data\n",
    "# client2_dataset = train_dataset_client2\n",
    "\n",
    "\n",
    "# # Create DataLoaders for each client and the validation set\n",
    "# batch_size = 64\n",
    "# num_workers = 4\n",
    "\n",
    "# client_loaders = [\n",
    "#     DataLoader(client1_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "#     DataLoader(client2_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "# ]\n",
    "\n",
    "# val_loaders = [\n",
    "#     DataLoader(val_dataset_client1, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n",
    "#     DataLoader(val_dataset_client2, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# ]\n",
    "\n",
    "# test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# # Print statements to verify the data distribution\n",
    "# print(f'Client 1 dataset size (after validation split): {len(client1_dataset)}')\n",
    "# print(f'Client 2 dataset size (after validation split): {len(client2_dataset)}')\n",
    "# print(f'Client 1 validation dataset size: {len(val_dataset_client1)}')\n",
    "# print(f'Client 2 validation dataset size: {len(val_dataset_client2)}')\n",
    "# print(f'Test dataset size: {len(dataset_test)}')\n",
    "\n",
    "# # Check some samples from each client to ensure the data is correctly assigned\n",
    "# for i, (images, labels) in enumerate(client_loaders[0]):\n",
    "#     print(f'Client 1 batch {i+1}:')\n",
    "#     print(f'Images shape: {images.shape}')\n",
    "#     print(f'Labels: {labels}')\n",
    "#     break\n",
    "\n",
    "# for i, (images, labels) in enumerate(client_loaders[1]):\n",
    "#     print(f'Client 2 batch {i+1}:')\n",
    "#     print(f'Images shape: {images.shape}')\n",
    "#     print(f'Labels: {labels}')\n",
    "#     break\n",
    "\n",
    "# print(\"Preprocessing Method 1: Major/Minor Superclass Split completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Method 2: Random Subclass Image Assignment\n",
    "- Both clients handle **all 20 superclasses**.\n",
    "- Images within each superclass are **randomly split** between the clients.\n",
    "- Ensures that both clients see all subclasses, but the specific images within each subclass are unique to each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 dataset size (after validation split): 22500\n",
      "Client 2 dataset size (after validation split): 22500\n",
      "Client 1 validation dataset size: 2500\n",
      "Client 2 validation dataset size: 2500\n",
      "Test dataset size: 10000\n",
      "Client 1 batch 1:\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels: tensor([ 7, 34, 28, 86,  8, 74, 87, 53, 66,  3, 80, 68, 13, 61, 92, 10,  0, 99,\n",
      "        23, 83, 50, 14, 50, 60, 36, 76, 62, 83, 15, 60, 56, 61,  4, 30, 64, 44,\n",
      "        51,  7, 88, 16, 77, 20, 52, 75, 23, 82, 44, 15, 92, 82, 93, 36, 49, 92,\n",
      "        44, 34, 29,  3, 93, 94, 17, 71, 24, 32, 34, 99, 42,  0, 70, 61,  4, 61,\n",
      "        88, 41, 76, 84, 74, 79,  4, 48, 93, 34, 54, 76, 27, 53, 29, 63, 52, 48,\n",
      "        95, 90, 55,  9, 23,  9, 19, 11, 52, 90,  8, 59, 82, 73, 77, 66, 71, 13,\n",
      "        63, 19, 94, 15, 97, 31, 12, 29,  7, 14, 32, 69, 86, 70,  4,  3, 68, 61,\n",
      "         7, 76,  3,  2, 93, 18, 83, 75, 11, 54, 63, 64, 41, 31,  5, 50, 14, 18,\n",
      "        37, 87, 51, 66, 50, 15, 90, 57, 28, 18, 18, 66, 97, 75, 27, 64,  8, 78,\n",
      "        63, 92,  2, 42, 14, 30, 36, 27, 58, 14, 70, 69, 42,  9, 84,  0, 95, 78,\n",
      "        19, 65, 46, 39, 65, 53, 45, 58, 69, 25, 52, 43, 69,  2, 40, 54,  1, 53,\n",
      "        81, 53, 87, 78, 19, 68, 81, 28, 11, 91, 95, 54, 42,  8, 67, 67, 66, 59,\n",
      "        83, 34, 84, 83, 14,  1, 98, 43,  3,  6, 55, 79,  5, 12, 97, 23, 15, 70,\n",
      "        45, 46, 59, 31, 53, 43, 67,  6, 60, 73, 80, 61, 97, 87, 20, 37,  2, 77,\n",
      "        51,  9, 89, 86])\n",
      "Client 2 batch 1:\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels: tensor([40, 18, 80, 91, 71, 77,  6, 98, 74, 30, 31, 39, 36, 81, 58, 46, 59, 14,\n",
      "        72, 66, 31, 46, 83, 89, 16, 11, 56, 21, 84, 55, 53,  9, 71, 98, 56, 31,\n",
      "        35, 17, 29, 87, 75, 89, 14, 84, 19, 43, 84, 22, 33,  2, 77,  5, 28, 47,\n",
      "         3, 79, 94,  9, 66, 71, 81, 67, 59, 46,  5, 92,  6, 21, 25, 22, 29, 88,\n",
      "         0, 66, 45, 72,  5, 98, 82,  9, 23, 71, 26, 28, 53, 54, 66, 75, 81, 95,\n",
      "        53, 62, 78, 12, 78, 66, 57, 29, 16, 25, 72,  7, 59, 82, 46, 16, 23, 44,\n",
      "        80, 73, 14, 71, 15, 16, 22, 54, 76,  6, 58, 49, 86,  2, 31, 65, 62, 95,\n",
      "        35, 37, 60, 27, 55, 88, 98, 37,  2, 21, 56,  7, 71, 62,  4, 16, 33, 32,\n",
      "        40, 45, 28, 32, 49, 44, 69, 79, 93, 66, 57, 71, 24, 41, 37, 62, 17, 48,\n",
      "        90, 83, 37, 67, 65,  0, 23, 75, 67, 10, 11, 88, 15, 28, 86, 80, 16, 61,\n",
      "        15, 94, 65, 33,  6, 47, 42, 64, 23, 19, 52, 43, 55, 50, 24, 73, 30, 94,\n",
      "        26, 83, 83, 33, 95, 24, 99,  1, 77, 89, 14, 23, 98, 53, 45, 13, 99, 18,\n",
      "        37, 23, 60, 90, 37, 80, 10, 31, 52, 38,  6, 89, 72, 54, 91, 59, 81,  9,\n",
      "        48, 29, 74, 18, 59, 98,  6,  7, 57, 22, 42, 20, 57, 46, 89, 88, 93, 53,\n",
      "        37, 89, 18,  9])\n",
      "Preprocessing Method 2: Random Subclass Image Assignment completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# # Define superclasses and subclasses\n",
    "# superclasses = {\n",
    "#     1: [4, 30, 55, 72, 95],  # aquatic mammals\n",
    "#     2: [1, 32, 67, 73, 91],  # fish\n",
    "#     3: [54, 62, 70, 82, 92],  # flowers\n",
    "#     4: [9, 10, 16, 28, 61],  # food containers\n",
    "#     5: [0, 51, 53, 57, 83],  # fruit and vegetables\n",
    "#     6: [22, 39, 40, 86, 87],  # household electrical devices\n",
    "#     7: [5, 20, 25, 84, 94],  # household furniture\n",
    "#     8: [6, 7, 14, 18, 24],  # insects\n",
    "#     9: [3, 42, 43, 88, 97],  # large carnivores\n",
    "#     10: [12, 17, 37, 68, 76],  # large man-made outdoor things\n",
    "#     11: [23, 33, 49, 60, 71],  # large natural outdoor scenes\n",
    "#     12: [15, 19, 21, 31, 38],  # medium-sized mammals\n",
    "#     13: [34, 63, 64, 66, 75],  # non-insect invertebrates\n",
    "#     14: [26, 45, 77, 79, 99],  # people\n",
    "#     15: [2, 11, 35, 46, 98],  # reptiles\n",
    "#     16: [27, 29, 44, 78, 93],  # small mammals\n",
    "#     17: [36, 50, 65, 74, 80],  # trees\n",
    "#     18: [8, 13, 48, 58, 90],  # vehicles 1\n",
    "#     19: [41, 66, 69, 81, 89],  # vehicles 2\n",
    "#     20: [47, 50, 52, 56, 59],  # household furniture\n",
    "# }\n",
    "\n",
    "# # Function to map subclass to its corresponding superclass\n",
    "# def get_superclass(subclass, superclasses):\n",
    "#     for superclass, subclasses in superclasses.items():\n",
    "#         if subclass in subclasses:\n",
    "#             return superclass\n",
    "#     return None\n",
    "\n",
    "# # Function to randomly split dataset by superclass\n",
    "# def filter_dataset_randomly_by_superclass(dataset, superclasses):\n",
    "#     client1_indices = []\n",
    "#     client2_indices = []\n",
    "    \n",
    "#     # Shuffle and split images randomly for each superclass\n",
    "#     for superclass, subclasses in superclasses.items():\n",
    "#         superclass_indices = []\n",
    "        \n",
    "#         # Gather all images belonging to this superclass\n",
    "#         for idx, (data, target) in enumerate(dataset):\n",
    "#             if target in subclasses:\n",
    "#                 superclass_indices.append(idx)\n",
    "        \n",
    "#         # If no images found for this superclass, continue to the next\n",
    "#         if not superclass_indices:\n",
    "#             continue\n",
    "        \n",
    "#         # Shuffle the indices for randomness\n",
    "#         np.random.shuffle(superclass_indices)\n",
    "        \n",
    "#         # Split the indices between the two clients\n",
    "#         half_split = len(superclass_indices) // 2\n",
    "#         client1_indices.extend(superclass_indices[:half_split])\n",
    "#         client2_indices.extend(superclass_indices[half_split:])\n",
    "    \n",
    "#     # Return two subsets: one for each client\n",
    "#     return Subset(dataset, client1_indices), Subset(dataset, client2_indices)\n",
    "\n",
    "\n",
    "\n",
    "# # Apply the function to split the dataset randomly for both clients\n",
    "# client1_dataset, client2_dataset = filter_dataset_randomly_by_superclass(dataset_train, superclasses)\n",
    "\n",
    "# # Split a portion of client1_dataset for validation\n",
    "# val_split = 0.1  # 10% for validation\n",
    "\n",
    "# # For client1\n",
    "# val_size_client1 = int(val_split * len(client1_dataset))\n",
    "# train_size_client1 = len(client1_dataset) - val_size_client1\n",
    "# train_dataset_client1, val_dataset_client1 = random_split(client1_dataset, [train_size_client1, val_size_client1])\n",
    "\n",
    "# # For client2\n",
    "# val_size_client2 = int(val_split * len(client2_dataset))\n",
    "# train_size_client2 = len(client2_dataset) - val_size_client2\n",
    "# train_dataset_client2, val_dataset_client2 = random_split(client2_dataset, [train_size_client2, val_size_client2])\n",
    "\n",
    "# # Create DataLoaders for each client and the validation set\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "\n",
    "# client_loaders = [\n",
    "#     DataLoader(train_dataset_client1, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "#     DataLoader(train_dataset_client2, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "# ]\n",
    "\n",
    "# val_loaders = [\n",
    "#     DataLoader(val_dataset_client1, batch_size=batch_size, shuffle=False, num_workers=num_workers),\n",
    "#     DataLoader(val_dataset_client2, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "# ]\n",
    "\n",
    "# test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# # Print statements to verify the data distribution\n",
    "# print(f'Client 1 dataset size (after validation split): {len(train_dataset_client1)}')\n",
    "# print(f'Client 2 dataset size (after validation split): {len(train_dataset_client2)}')\n",
    "# print(f'Client 1 validation dataset size: {len(val_dataset_client1)}')\n",
    "# print(f'Client 2 validation dataset size: {len(val_dataset_client2)}')\n",
    "# print(f'Test dataset size: {len(dataset_test)}')\n",
    "\n",
    "# # Check some samples from each client to ensure the data is correctly assigned\n",
    "# for i, (images, labels) in enumerate(client_loaders[0]):\n",
    "#     print(f'Client 1 batch {i+1}:')\n",
    "#     print(f'Images shape: {images.shape}')\n",
    "#     print(f'Labels: {labels}')\n",
    "#     break\n",
    "\n",
    "# for i, (images, labels) in enumerate(client_loaders[1]):\n",
    "#     print(f'Client 2 batch {i+1}:')\n",
    "#     print(f'Images shape: {images.shape}')\n",
    "#     print(f'Labels: {labels}')\n",
    "#     break\n",
    "\n",
    "# print(\"Preprocessing Method 2: Random Subclass Image Assignment completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 dataset size (after validation split): 11250\n",
      "Client 1 validation dataset size: 1250\n",
      "Client 2 dataset size (after validation split): 11250\n",
      "Client 2 validation dataset size: 1250\n",
      "Client 3 dataset size (after validation split): 11250\n",
      "Client 3 validation dataset size: 1250\n",
      "Client 4 dataset size (after validation split): 11250\n",
      "Client 4 validation dataset size: 1250\n",
      "Test dataset size: 10000\n",
      "Client 1 batch 1:\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels: tensor([33, 57, 89, 91, 66, 92, 67, 55, 35, 42, 41, 69, 21, 67, 63, 18, 13, 62,\n",
      "        41, 20, 54, 32, 46, 17, 48, 74, 94, 81, 59, 47,  5, 35, 72,  8, 27, 45,\n",
      "        34,  7, 92, 54, 30,  1, 15, 50, 98,  5,  5, 19, 83, 66, 66, 52, 30, 31,\n",
      "        14, 92, 42, 14, 14, 99, 76, 56, 39, 49, 84, 20, 84, 60, 68, 77, 83, 44,\n",
      "        78, 11, 70, 77, 17, 82, 40, 45, 80, 43, 39, 73, 88, 47, 50, 42, 75, 13,\n",
      "        84, 10, 66, 44, 19, 82, 86, 42, 23,  2, 47, 98, 93, 10, 22, 75,  0, 21,\n",
      "        99, 21, 81, 73, 26, 53, 19, 29, 33, 14, 97, 64, 31, 13, 32, 88, 81, 56,\n",
      "        97, 36, 36, 98, 39, 83,  0,  8, 93,  9, 26, 64, 29, 61, 75, 57, 27, 72,\n",
      "         6, 35,  6, 69,  2,  1, 92, 11, 16, 33, 55, 21, 29, 68, 56, 69, 94, 75,\n",
      "        62, 36, 30, 76,  0, 82,  8,  8, 65, 41, 92, 43, 26, 49, 90, 71, 33, 34,\n",
      "        16, 46, 10, 27, 99, 21, 41, 35, 12, 79, 16, 12, 88,  5, 32, 86, 32, 51,\n",
      "        72, 64, 55, 49, 54, 13,  3, 52, 68, 50, 44, 50, 35, 46, 84, 27, 88, 32,\n",
      "        98, 30, 45, 66, 45, 18, 28, 86, 93, 22, 59, 33, 51, 65, 22, 45, 15, 38,\n",
      "        44, 18, 91, 18, 82, 71,  2, 56, 92, 75, 33, 77, 10,  1, 55, 75, 12, 97,\n",
      "        91, 62, 86,  7])\n",
      "Client 2 batch 1:\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels: tensor([74,  2, 40, 89, 66, 41, 91,  3, 44, 55, 84, 97, 66, 29, 95, 69, 62, 99,\n",
      "        11, 50, 45, 55, 80, 76, 61, 53, 68, 61,  8, 13, 42, 51, 92, 49, 94, 44,\n",
      "        91, 74, 18, 98, 63, 23, 24, 94, 90, 42, 58, 27, 93, 23, 44,  6, 32, 10,\n",
      "        33, 40, 61, 23, 18, 54, 54, 51, 68, 46, 37, 15, 15, 22, 74, 14, 51, 38,\n",
      "        40, 14, 74, 66,  0,  8, 99, 71, 33,  7, 19, 34, 32, 25, 37, 22, 71, 12,\n",
      "        94, 86, 57,  8, 55, 23, 26, 59, 98, 40, 47,  5, 24, 83, 15, 16, 54, 71,\n",
      "        72, 79,  4, 61, 66, 18, 98, 22, 50, 93, 11, 26, 95, 86, 26, 63, 57, 82,\n",
      "        64, 44, 42, 43,  6, 61, 74, 91, 69,  6, 79, 24, 13, 87, 12, 50, 94, 65,\n",
      "        63, 67,  4, 31, 94, 67, 31, 38, 37, 56, 60, 17, 90, 21,  4,  3, 27, 15,\n",
      "        35,  1, 31, 99,  8, 84, 97, 83, 25, 75, 83, 80, 38, 91, 60, 74,  2, 97,\n",
      "        84, 70, 47,  6, 72, 88,  2, 46, 95, 27,  2,  1, 33, 76, 73, 40,  6, 16,\n",
      "        89, 99, 90, 93, 63, 62, 57, 26, 21, 50, 52,  7, 35,  1, 32, 83, 51, 82,\n",
      "        65, 18, 14, 59,  0, 15,  2, 89, 20, 82, 99, 76, 38, 54, 22,  7, 21, 67,\n",
      "        99, 86, 16, 66, 29, 13, 10, 57, 75, 56, 64, 28, 23, 36, 38, 59, 65, 79,\n",
      "        99, 11, 39, 72])\n",
      "Client 3 batch 1:\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels: tensor([50, 75, 28, 44, 37, 36,  8, 26, 77, 40, 48,  8, 61, 98, 87, 75, 18, 12,\n",
      "        49, 58, 89, 57, 55, 65, 69, 42, 36, 66, 91, 17,  1, 11, 39, 39, 89, 92,\n",
      "        41, 87, 79, 10, 76,  5, 98, 52, 21, 21, 83, 95, 36, 48, 86, 84, 14, 70,\n",
      "        88, 50, 78, 51, 89, 19, 80, 41, 28,  9, 17, 44, 75, 65, 33,  8, 69, 74,\n",
      "        10, 49, 29, 14, 89, 10, 47, 88, 57, 30, 15,  1, 10, 51, 75, 77, 43, 86,\n",
      "        63, 61, 82, 79, 55, 74, 86,  8, 12, 91, 59, 53,  4, 88, 49, 75, 10, 15,\n",
      "        33, 61, 12, 43, 48, 93, 66, 57, 56, 64, 67, 37, 37, 60, 48,  3, 54, 39,\n",
      "        18, 39,  5, 39, 12, 53, 58, 17,  2, 58, 74, 72, 49, 79,  5, 78, 35, 29,\n",
      "        29, 77, 59,  0, 92, 53, 92, 22, 17, 11, 72, 71, 73,  7, 57, 66, 21, 51,\n",
      "        38, 86, 42,  5, 38, 40, 34, 29, 41, 99, 41, 38, 93, 38, 72, 12, 18, 59,\n",
      "        76, 12, 88,  5, 57, 92,  4, 52, 32, 18, 64, 19, 47, 34, 50, 28, 13, 24,\n",
      "        19, 89,  5, 98, 79, 65, 41, 15, 87, 11, 44, 23, 43, 34, 71, 84, 56, 55,\n",
      "        75, 87, 58, 38, 20, 60, 14, 86, 82, 77, 72, 12, 76, 38,  4, 26, 88, 94,\n",
      "        57, 54, 50, 24, 29, 17, 29, 95, 67, 41, 99, 27, 97, 48, 10, 57, 74, 68,\n",
      "        69, 25, 37,  4])\n",
      "Client 4 batch 1:\n",
      "Images shape: torch.Size([256, 3, 224, 224])\n",
      "Labels: tensor([ 2, 26, 95, 14, 12, 50, 23, 81, 95, 46, 74, 51, 88, 20, 51, 57, 60, 94,\n",
      "         5, 31, 80, 45, 46, 36, 33, 61, 16, 19, 92, 50, 33, 91, 32, 25, 22,  0,\n",
      "        66, 60, 97, 19, 97, 63, 64, 66, 36, 77,  6, 40, 35, 77, 35, 19, 89, 94,\n",
      "        90, 15, 48,  2, 70, 93, 53, 49, 69, 26, 53, 92,  0, 64, 80, 33, 18,  4,\n",
      "        54, 46, 66, 86, 44,  8,  4,  1, 40, 90, 70, 28, 68, 19, 60, 99, 53, 34,\n",
      "        90, 42, 30, 67, 73, 57, 21, 98, 88, 86,  5,  7, 21, 54, 98, 12, 80, 59,\n",
      "        50, 23, 30, 45, 25, 46, 88, 46, 73, 53,  5, 29, 34, 64, 74, 68, 45, 68,\n",
      "        46, 29, 94, 25, 38, 34, 59, 77, 43, 79, 36, 88, 78,  9, 65, 23, 65,  0,\n",
      "        78, 29, 26, 75, 17, 36, 47, 32, 92, 94, 41, 81, 55, 19, 22,  6, 23, 57,\n",
      "        31, 92, 21, 14, 66, 10, 18, 52, 33, 36, 66,  5, 18, 52, 10, 89, 95, 45,\n",
      "        16,  3, 87, 80, 47, 90, 65, 66, 92, 67, 67, 89, 59, 66, 14, 30, 82, 74,\n",
      "        56, 29, 41, 73, 44, 81, 34, 38, 36, 20, 54, 20, 67, 73, 33, 27, 39, 84,\n",
      "        61, 60,  5, 40, 29,  8, 82, 42, 68, 45, 16, 57, 11, 49, 82, 50, 66, 30,\n",
      "        38, 48, 86, 94, 30, 15, 97, 17, 61, 39, 99, 58, 80, 49, 33, 81, 22, 43,\n",
      "        73, 48, 81, 23])\n",
      "Preprocessing Method for 4 Clients completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Define superclasses and subclasses\n",
    "superclasses = {\n",
    "    1: [4, 30, 55, 72, 95],  # aquatic mammals\n",
    "    2: [1, 32, 67, 73, 91],  # fish\n",
    "    3: [54, 62, 70, 82, 92],  # flowers\n",
    "    4: [9, 10, 16, 28, 61],  # food containers\n",
    "    5: [0, 51, 53, 57, 83],  # fruit and vegetables\n",
    "    6: [22, 39, 40, 86, 87],  # household electrical devices\n",
    "    7: [5, 20, 25, 84, 94],  # household furniture\n",
    "    8: [6, 7, 14, 18, 24],  # insects\n",
    "    9: [3, 42, 43, 88, 97],  # large carnivores\n",
    "    10: [12, 17, 37, 68, 76],  # large man-made outdoor things\n",
    "    11: [23, 33, 49, 60, 71],  # large natural outdoor scenes\n",
    "    12: [15, 19, 21, 31, 38],  # medium-sized mammals\n",
    "    13: [34, 63, 64, 66, 75],  # non-insect invertebrates\n",
    "    14: [26, 45, 77, 79, 99],  # people\n",
    "    15: [2, 11, 35, 46, 98],  # reptiles\n",
    "    16: [27, 29, 44, 78, 93],  # small mammals\n",
    "    17: [36, 50, 65, 74, 80],  # trees\n",
    "    18: [8, 13, 48, 58, 90],  # vehicles 1\n",
    "    19: [41, 66, 69, 81, 89],  # vehicles 2\n",
    "    20: [47, 50, 52, 56, 59],  # household furniture\n",
    "}\n",
    "\n",
    "# Function to randomly split dataset by superclass for 4 clients\n",
    "def filter_dataset_randomly_by_superclass(dataset, superclasses):\n",
    "    client_indices = [[] for _ in range(4)]  # Create a list to store indices for 4 clients\n",
    "    \n",
    "    # Shuffle and split images randomly for each superclass\n",
    "    for superclass, subclasses in superclasses.items():\n",
    "        superclass_indices = []\n",
    "        \n",
    "        # Gather all images belonging to this superclass\n",
    "        for idx, (data, target) in enumerate(dataset):\n",
    "            if target in subclasses:\n",
    "                superclass_indices.append(idx)\n",
    "        \n",
    "        # If no images found for this superclass, continue to the next\n",
    "        if not superclass_indices:\n",
    "            continue\n",
    "        \n",
    "        # Shuffle the indices for randomness\n",
    "        np.random.shuffle(superclass_indices)\n",
    "        \n",
    "        # Split the indices between the 4 clients\n",
    "        split_size = len(superclass_indices) // 4\n",
    "        for i in range(4):\n",
    "            start = i * split_size\n",
    "            end = (i + 1) * split_size if i < 3 else len(superclass_indices)  # Ensure the last client gets the remaining data\n",
    "            client_indices[i].extend(superclass_indices[start:end])\n",
    "    \n",
    "    # Return four subsets: one for each client\n",
    "    return [Subset(dataset, indices) for indices in client_indices]\n",
    "\n",
    "# Apply the function to split the dataset randomly for 4 clients\n",
    "client_datasets = filter_dataset_randomly_by_superclass(dataset_train, superclasses)\n",
    "\n",
    "# Split a portion of each client dataset for validation\n",
    "val_split = 0.1  # 10% for validation\n",
    "\n",
    "train_val_splits = [random_split(client_dataset, [int(len(client_dataset) * (1 - val_split)), int(len(client_dataset) * val_split)]) for client_dataset in client_datasets]\n",
    "\n",
    "# Separate train and validation datasets\n",
    "train_datasets = [split[0] for split in train_val_splits]\n",
    "val_datasets = [split[1] for split in train_val_splits]\n",
    "\n",
    "# Create DataLoaders for each client and the validation set\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "client_loaders = [DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for train_dataset in train_datasets]\n",
    "val_loaders = [DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers) for val_dataset in val_datasets]\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Print statements to verify the data distribution\n",
    "for i in range(4):\n",
    "    print(f'Client {i+1} dataset size (after validation split): {len(train_datasets[i])}')\n",
    "    print(f'Client {i+1} validation dataset size: {len(val_datasets[i])}')\n",
    "\n",
    "print(f'Test dataset size: {len(dataset_test)}')\n",
    "\n",
    "# Check some samples from each client to ensure the data is correctly assigned\n",
    "for i in range(4):\n",
    "    for j, (images, labels) in enumerate(client_loaders[i]):\n",
    "        print(f'Client {i+1} batch {j+1}:')\n",
    "        print(f'Images shape: {images.shape}')\n",
    "        print(f'Labels: {labels}')\n",
    "        break  # Break to only display the first batch for verification\n",
    "\n",
    "print(\"Preprocessing Method for 4 Clients completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 - Images shape: torch.Size([256, 3, 224, 224]), Labels: torch.Size([256])\n",
      "Client 1 Validation - Images shape: torch.Size([256, 3, 224, 224]), Labels: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in client_loaders[0]:\n",
    "    print(f'Client 1 - Images shape: {images.shape}, Labels: {labels.shape}')\n",
    "    break\n",
    "\n",
    "for images, labels in val_loaders[0]:\n",
    "    print(f'Client 1 Validation - Images shape: {images.shape}, Labels: {labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Federated Training Functions (Federated Averaging (FedAvg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "#     model.to(device)  \n",
    "#     model.train()\n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)  \n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.001, patience=5, min_delta=0):\n",
    "#     model = prepare_model().to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         avg_state_dict = federated_averaging(state_dicts)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Validation phase (handling multiple validation loaders)\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for val_loader in val_loaders:\n",
    "#                 for inputs, labels in val_loader:\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     val_running_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Federated Training Functions (Adaptive Federated Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.to(device)  \n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def federated_averaging(state_dicts, global_model, beta=0.9):\n",
    "    avg_state_dict = global_model.state_dict()\n",
    "    for key in avg_state_dict.keys():\n",
    "        if avg_state_dict[key].dtype == torch.long:\n",
    "            avg_state_dict[key] = torch.zeros_like(avg_state_dict[key], dtype=torch.float32)\n",
    "            for state_dict in state_dicts:\n",
    "                avg_state_dict[key] += state_dict[key].float() / len(state_dicts)\n",
    "            avg_state_dict[key] = avg_state_dict[key].long()  # Convert back to long if necessary\n",
    "        else:\n",
    "            avg_state_dict[key] = torch.zeros_like(avg_state_dict[key])\n",
    "            for state_dict in state_dicts:\n",
    "                avg_state_dict[key] += state_dict[key] / len(state_dicts)\n",
    "            avg_state_dict[key] = beta * avg_state_dict[key] + (1 - beta) * global_model.state_dict()[key]\n",
    "    return avg_state_dict\n",
    "\n",
    "def train_federated_model(client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate=0.001, patience=5, min_delta=0):\n",
    "    model = prepare_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "\n",
    "    for round in range(num_epochs):\n",
    "        print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "        state_dicts = []\n",
    "        for i, client_loader in enumerate(client_loaders):\n",
    "            print(f\"Training model for client {i+1}...\")\n",
    "            client_model = prepare_model().to(device)\n",
    "            client_model.load_state_dict(model.state_dict())\n",
    "            optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "            client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "            state_dicts.append(client_state_dict)\n",
    "\n",
    "        avg_state_dict = federated_averaging(state_dicts, model)  # Call to modified function\n",
    "        model.load_state_dict(avg_state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        # Validation phase (handling multiple validation loaders)\n",
    "        print(\"Validating model...\")\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for val_loader in val_loaders:  # Iterate over multiple validation loaders\n",
    "                for inputs, labels in val_loader:  # Iterate over batches within each loader\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_running_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "            f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Test phase\n",
    "    print(\"Testing model...\")\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedProx (Federated Proximal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# def train_client_prox(model, train_loader, criterion, optimizer, global_model, mu=0.01, epochs=1):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     global_weights = global_model.state_dict()  # Get the global model's weights\n",
    "    \n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Add the proximal term (FedProx)\n",
    "#             proximal_term = 0.0\n",
    "#             for param, global_param in zip(model.parameters(), global_model.parameters()):\n",
    "#                 proximal_term += (param - global_param).norm(2)\n",
    "#             loss += (mu / 2) * proximal_term\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "    \n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict\n",
    "\n",
    "\n",
    "# def train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate=0.01, patience=5, min_delta=0, mu=0.01):\n",
    "#     model = prepare_model().to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model().to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client_prox(client_model, client_loader, criterion, optimizer, model, mu)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "        \n",
    "#         avg_state_dict = federated_averaging(state_dicts)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Validation phase (same as before)\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for val_loader in val_loaders:\n",
    "#                 for inputs, labels in val_loader:\n",
    "#                     inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#                     val_running_loss += loss.item()\n",
    "#                     _, predicted = torch.max(outputs.data, 1)\n",
    "#                     val_total += labels.size(0)\n",
    "#                     val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / sum(len(loader) for loader in val_loaders)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test phase (same as before)\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define the Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def log_experiment_result(filename, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['num_clients', 'num_epochs', 'learning_rate', 'patience', 'min_delta', 'val_loss', 'val_accuracy', 'test_loss', 'test_accuracy']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()  # Write header only once\n",
    "        \n",
    "        writer.writerow({\n",
    "            'num_clients': num_clients,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'patience': patience,\n",
    "            'min_delta': min_delta,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Federated Learning Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training...\n",
      "Starting federated learning round 1/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 1/40, Val Loss: 2.9437, Val Accuracy: 30.94%\n",
      "Starting federated learning round 2/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 2/40, Val Loss: 2.1303, Val Accuracy: 44.36%\n",
      "Starting federated learning round 3/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 3/40, Val Loss: 1.9388, Val Accuracy: 48.56%\n",
      "Starting federated learning round 4/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 4/40, Val Loss: 1.8062, Val Accuracy: 51.90%\n",
      "Starting federated learning round 5/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 5/40, Val Loss: 1.7003, Val Accuracy: 53.92%\n",
      "Starting federated learning round 6/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 6/40, Val Loss: 1.6067, Val Accuracy: 56.72%\n",
      "Starting federated learning round 7/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 7/40, Val Loss: 1.5841, Val Accuracy: 57.22%\n",
      "Starting federated learning round 8/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 8/40, Val Loss: 1.5330, Val Accuracy: 58.76%\n",
      "Starting federated learning round 9/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 9/40, Val Loss: 1.5160, Val Accuracy: 59.30%\n",
      "Starting federated learning round 10/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 10/40, Val Loss: 1.4439, Val Accuracy: 60.80%\n",
      "Starting federated learning round 11/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 11/40, Val Loss: 1.4263, Val Accuracy: 61.50%\n",
      "Starting federated learning round 12/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 12/40, Val Loss: 1.4089, Val Accuracy: 61.86%\n",
      "Starting federated learning round 13/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 13/40, Val Loss: 1.3850, Val Accuracy: 62.98%\n",
      "Starting federated learning round 14/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 14/40, Val Loss: 1.3986, Val Accuracy: 62.46%\n",
      "Starting federated learning round 15/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 15/40, Val Loss: 1.3547, Val Accuracy: 62.62%\n",
      "Starting federated learning round 16/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 16/40, Val Loss: 1.3515, Val Accuracy: 63.38%\n",
      "Starting federated learning round 17/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 17/40, Val Loss: 1.3547, Val Accuracy: 63.14%\n",
      "Starting federated learning round 18/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 18/40, Val Loss: 1.2915, Val Accuracy: 65.36%\n",
      "Starting federated learning round 19/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 19/40, Val Loss: 1.2849, Val Accuracy: 64.80%\n",
      "Starting federated learning round 20/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 20/40, Val Loss: 1.2948, Val Accuracy: 64.32%\n",
      "Starting federated learning round 21/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 21/40, Val Loss: 1.2632, Val Accuracy: 65.08%\n",
      "Starting federated learning round 22/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 22/40, Val Loss: 1.2663, Val Accuracy: 65.76%\n",
      "Starting federated learning round 23/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 23/40, Val Loss: 1.2833, Val Accuracy: 65.08%\n",
      "Starting federated learning round 24/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 24/40, Val Loss: 1.2281, Val Accuracy: 66.18%\n",
      "Starting federated learning round 25/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 25/40, Val Loss: 1.2642, Val Accuracy: 65.16%\n",
      "Starting federated learning round 26/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 26/40, Val Loss: 1.2409, Val Accuracy: 66.56%\n",
      "Starting federated learning round 27/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 27/40, Val Loss: 1.2951, Val Accuracy: 65.34%\n",
      "Starting federated learning round 28/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 28/40, Val Loss: 1.2162, Val Accuracy: 66.70%\n",
      "Starting federated learning round 29/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 29/40, Val Loss: 1.2133, Val Accuracy: 67.04%\n",
      "Starting federated learning round 30/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 30/40, Val Loss: 1.2024, Val Accuracy: 67.42%\n",
      "Starting federated learning round 31/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 31/40, Val Loss: 1.2357, Val Accuracy: 66.78%\n",
      "Starting federated learning round 32/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 32/40, Val Loss: 1.2058, Val Accuracy: 67.32%\n",
      "Starting federated learning round 33/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 33/40, Val Loss: 1.2518, Val Accuracy: 67.06%\n",
      "Starting federated learning round 34/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 34/40, Val Loss: 1.1962, Val Accuracy: 67.74%\n",
      "Starting federated learning round 35/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 35/40, Val Loss: 1.1892, Val Accuracy: 68.56%\n",
      "Starting federated learning round 36/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 36/40, Val Loss: 1.1881, Val Accuracy: 67.38%\n",
      "Starting federated learning round 37/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 37/40, Val Loss: 1.2030, Val Accuracy: 67.88%\n",
      "Starting federated learning round 38/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 38/40, Val Loss: 1.2082, Val Accuracy: 68.38%\n",
      "Starting federated learning round 39/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 39/40, Val Loss: 1.2164, Val Accuracy: 67.68%\n",
      "Starting federated learning round 40/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 40/40, Val Loss: 1.2197, Val Accuracy: 67.70%\n",
      "Early stopping\n",
      "Testing model...\n",
      "Test Loss: 1.9100, Test Accuracy: 66.31%\n",
      "Federated training complete.\n",
      "Experiment results logged.\n"
     ]
    }
   ],
   "source": [
    "num_clients = 4\n",
    "num_epochs = 40\n",
    "learning_rate = 0.001\n",
    "patience = 5\n",
    "min_delta = 0.01\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "log_file = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/experiment_results.csv'\n",
    "\n",
    "\n",
    "# Initialize Data Loaders\n",
    "#client_loaders, val_loader = get_data_loaders(batch_size, num_workers, num_clients)\n",
    "#test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(\"Starting federated training...\")\n",
    "model, val_loss, val_accuracy, test_loss, test_accuracy = train_federated_model(client_loaders, val_loaders, test_loader, num_clients, num_epochs, learning_rate, patience, min_delta)\n",
    "print(\"Federated training complete.\")\n",
    "\n",
    "# Log the results\n",
    "log_experiment_result(log_file, num_clients, num_epochs, learning_rate, patience, min_delta, val_loss, val_accuracy, test_loss, test_accuracy)\n",
    "print(\"Experiment results logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
