{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "\n",
    "# Check for CUDA GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 742 original images for BigBag2_1_PET\n",
      "Loaded 1403 original images for BigBag2_2_PP\n",
      "Loaded 1203 original images for BigBag2_3_PE\n",
      "Loaded 192 original images for BigBag2_4_Tetra\n",
      "Loaded 4 original images for BigBag2_5_PVC\n",
      "Loaded 227 original images for BigBag2_6_PS\n",
      "Loaded 1268 original images for BigBag2_7_Other\n",
      "Loaded 1350 augmented images for BigBag2_4_Tetra_Augmented\n",
      "Loaded 1694 augmented images for BigBag2_6_PS_Augmented\n",
      "Loaded 904 original images for BigBag4_1_PET\n",
      "Loaded 1483 original images for BigBag4_2_PP\n",
      "Loaded 833 original images for BigBag4_3_PE\n",
      "Loaded 173 original images for BigBag4_4_Tetra\n",
      "Loaded 254 original images for BigBag4_6_PS\n",
      "Loaded 3 original images for BigBag4_5_PVC\n",
      "Loaded 1373 original images for BigBag4_7_Other\n",
      "Loaded 458 original images for BigBag1_1_PET\n",
      "Loaded 414 original images for BigBag1_2_PP\n",
      "Loaded 518 original images for BigBag1_3_PE\n",
      "Loaded 21 original images for BigBag1_4_Tetra\n",
      "Loaded 47 original images for BigBag1_6_PS\n",
      "Loaded 984 original images for BigBag1_7_Other\n",
      "Loaded 391 original images for BigBag3_PET\n",
      "Loaded 599 original images for BigBag3_2_PP\n",
      "Loaded 345 original images for BigBag3_PE\n",
      "Loaded 70 original images for BigBag3_TETRA\n",
      "Loaded 178 original images for BigBag3_6_PS\n",
      "Loaded 702 original images for BigBag3_Other\n",
      "Loaded 647 original images for DWRL7_extension_2_PVC\n",
      "Loaded 1962 augmented images for BigBag2_5_PVC_Augmented\n",
      "Initial ClassCounts after augmentation: Counter({6: 4327, 1: 3899, 2: 2899, 5: 2616, 0: 2495, 4: 2400, 3: 1806})\n"
     ]
    }
   ],
   "source": [
    "#### With class 'Other'\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Updated class mapping to combine classes into unique classes (including 'Augmented') \n",
    "# put PVC and Other to one class\n",
    "class_mapping = {\n",
    "    #BigBag2\n",
    "    'BigBag2_1_PET': 0,  # PET\n",
    "    'BigBag2_2_PP': 1,   # PP\n",
    "    'BigBag2_3_PE': 2,   # PE\n",
    "    'BigBag2_4_Tetra': 3, # Tetra\n",
    "    'BigBag2_5_PVC': 5, # PVC\n",
    "    'BigBag2_6_PS': 4,   # PS\n",
    "    'BigBag2_7_Other': 6, # Other\n",
    "    'BigBag2_4_Tetra_Augmented': 3,  # Augmented Tetra\n",
    "    'BigBag2_6_PS_Augmented': 4,  # Augmented PS\n",
    "    \n",
    "    #BigBag4\n",
    "    'BigBag4_1_PET': 0,  # PET\n",
    "    'BigBag4_2_PP': 1,   # PP\n",
    "    'BigBag4_3_PE': 2,   # PE\n",
    "    'BigBag4_4_Tetra': 3, # Tetra\n",
    "    'BigBag4_6_PS': 4,   # PS\n",
    "    'BigBag4_5_PVC': 5, # PVC\n",
    "    'BigBag4_7_Other': 6, # Other\n",
    "    \n",
    "    #BigBag1\n",
    "    'BigBag1_1_PET': 0,  # PET\n",
    "    'BigBag1_2_PP': 1,   # PP\n",
    "    'BigBag1_3_PE': 2,   # PE\n",
    "    'BigBag1_4_Tetra': 3, # Tetra\n",
    "    #'BigBag2_4_Tetra_Augmented': 3,  # Augmented Tetra\n",
    "    #'BigBag2_5_PVC': 5, # PVC\n",
    "    'BigBag1_6_PS': 4,   # PS\n",
    "    'BigBag1_7_Other': 6, # Other\n",
    "    #'BigBag2_6_PS_Augmented': 4,  # Augmented PS\n",
    "    \n",
    "    #BigBag3\n",
    "    'BigBag3_PET': 0,  # PET\n",
    "    'BigBag3_2_PP': 1,   # PP\n",
    "    'BigBag3_PE': 2,   # PE\n",
    "    'BigBag3_TETRA': 3, # Tetra\n",
    "    #'BigBag3_PVC': 5, # PVC\n",
    "    'BigBag3_6_PS': 4,   # PS\n",
    "    'BigBag3_Other': 6, # Other\n",
    "    \n",
    "    'DWRL7_extension_2_PVC': 5, # \n",
    "    'BigBag2_5_PVC_Augmented': 5,  # Augmented PVC\n",
    "}\n",
    "\n",
    "class CustomPlasticDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_mapping, transform, tetra_transform=None, ps_transform=None, pvc_transform=None, diverse_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.class_mapping = class_mapping\n",
    "        self.transform = transform\n",
    "        self.tetra_transform = tetra_transform\n",
    "        self.ps_transform = ps_transform\n",
    "        self.pvc_transform = pvc_transform\n",
    "        self.diverse_transform = diverse_transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Gather image paths and labels\n",
    "        for class_folder in class_mapping.keys():\n",
    "            # Load original images\n",
    "            image_dir = os.path.join(root_dir, class_folder, 'images_cutout')\n",
    "            if os.path.exists(image_dir):\n",
    "                image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "                print(f\"Loaded {len(image_files)} original images for {class_folder}\")\n",
    "                self.image_paths.extend([os.path.join(image_dir, img) for img in image_files])\n",
    "                self.labels.extend([class_mapping[class_folder]] * len(image_files))\n",
    "\n",
    "            # Load augmented images if they exist\n",
    "            augmented_dir = os.path.join(root_dir, class_folder)  # Path to the augmented class\n",
    "            if os.path.exists(augmented_dir) and 'Augmented' in class_folder:\n",
    "                augmented_files = [f for f in os.listdir(augmented_dir) if f.endswith(('.jpg', '.png'))]\n",
    "                print(f\"Loaded {len(augmented_files)} augmented images for {class_folder}\")\n",
    "                self.image_paths.extend([os.path.join(augmented_dir, img) for img in augmented_files])\n",
    "                self.labels.extend([class_mapping[class_folder]] * len(augmented_files))  # Map to the same class label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply specific transformations based on class\n",
    "        if label == 3:  # Tetra\n",
    "            image = self.tetra_transform(image)\n",
    "        elif label == 4:  # PS\n",
    "            image = self.ps_transform(image)\n",
    "        elif label == 5:  # PVC\n",
    "            image = self.pvc_transform(image)\n",
    "            \n",
    "        else:\n",
    "            # Use diverse_transform with a 50% chance for other classes\n",
    "            if self.diverse_transform is not None and random.random() > 0.5:\n",
    "                image = self.diverse_transform(image)\n",
    "\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Augmentation for TETRA (Moderate)\n",
    "tetra_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Augmentation for PS (Light)\n",
    "ps_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Regular transform for other classes\n",
    "regular_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "])\n",
    "    \n",
    "# Augmentation for PVC (Light)\n",
    "pvc_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define more diverse transformations for augmentation\n",
    "diverse_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=30),  # Random rotation by up to 30 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # Random crop with resizing\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def augment_and_save_images(original_dataset, class_label, transform, target_dir, num_augmented_images):\n",
    "    os.makedirs(target_dir, exist_ok=True)  # Create the target directory if it doesn't exist\n",
    "    class_images = [original_dataset.image_paths[idx] for idx, label in enumerate(original_dataset.labels) if label == class_label]\n",
    "\n",
    "    for img_path in tqdm(class_images, desc=f'Augmenting Class {class_label}'):\n",
    "        img = Image.open(img_path)\n",
    "        for i in range(num_augmented_images):\n",
    "            augmented_img = transform(img)  # Apply the transformation\n",
    "            augmented_img = transforms.ToPILImage()(augmented_img)  # Convert back to PIL Image\n",
    "            \n",
    "            # Create a unique filename using the original filename and the index\n",
    "            base_filename = os.path.basename(img_path).split('.')[0]  # Get the original filename without extension\n",
    "            augmented_img.save(os.path.join(target_dir, f'augmented_{base_filename}_{i}.png'))  # Save with a unique name\n",
    "\n",
    "# Function to count images in each class\n",
    "def count_images_in_classes(base_dir, class_mapping):\n",
    "    class_counts = {class_name: 0 for class_name in class_mapping.keys()}\n",
    "\n",
    "    for class_name in class_mapping.keys():\n",
    "        # Check for original images\n",
    "        image_dir = os.path.join(base_dir, class_name, 'images_cutout')  # Original images path\n",
    "        if os.path.exists(image_dir):\n",
    "            image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "            class_counts[class_name] += len(image_files)\n",
    "\n",
    "        # Check for augmented images in their respective folders\n",
    "        if 'Augmented' in class_name:\n",
    "            augmented_dir = os.path.join(base_dir, class_name)  # Path to the augmented class\n",
    "            if os.path.exists(augmented_dir):\n",
    "                augmented_files = [f for f in os.listdir(augmented_dir) if f.endswith(('.jpg', '.png'))]\n",
    "                class_counts[class_name] += len(augmented_files)  # Count augmented images\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Directory where all class folders are stored\n",
    "data_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/DWRL7/data'\n",
    "\n",
    "# Create dataset instance\n",
    "plastic_dataset = CustomPlasticDataset(\n",
    "    root_dir=data_dir, \n",
    "    class_mapping=class_mapping, \n",
    "    transform=regular_transform,\n",
    "    tetra_transform=tetra_transform,\n",
    "    ps_transform=ps_transform,\n",
    "    pvc_transform=pvc_transform,\n",
    "    diverse_transform=diverse_transform \n",
    ")\n",
    "\n",
    "# After creating the dataset instance, count the classes\n",
    "initial_class_counts = Counter(plastic_dataset.labels)\n",
    "print('Initial ClassCounts after augmentation:', initial_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Without class 'Other'\n",
    "\n",
    "\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# from collections import Counter\n",
    "\n",
    "# # Updated class mapping to combine classes into unique classes (including 'Augmented')\n",
    "# # Removed the 'Other' class (class 6)\n",
    "# class_mapping = {\n",
    "#     #BigBag2\n",
    "#     'BigBag2_1_PET': 0,  # PET\n",
    "#     'BigBag2_2_PP': 1,   # PP\n",
    "#     'BigBag2_3_PE': 2,   # PE\n",
    "#     'BigBag2_4_Tetra': 3, # Tetra\n",
    "#     'BigBag2_5_PVC': 5,  # PVC\n",
    "#     'BigBag2_6_PS': 4,   # PS\n",
    "#     'BigBag2_4_Tetra_Augmented': 3,  # Augmented Tetra\n",
    "#     'BigBag2_6_PS_Augmented': 4,  # Augmented PS\n",
    "    \n",
    "#     #BigBag4\n",
    "#     'BigBag4_1_PET': 0,  # PET\n",
    "#     'BigBag4_2_PP': 1,   # PP\n",
    "#     'BigBag4_3_PE': 2,   # PE\n",
    "#     'BigBag4_4_Tetra': 3, # Tetra\n",
    "#     'BigBag4_6_PS': 4,   # PS\n",
    "#     'BigBag4_5_PVC': 5,  # PVC\n",
    "    \n",
    "#     #BigBag1\n",
    "#     'BigBag1_1_PET': 0,  # PET\n",
    "#     'BigBag1_2_PP': 1,   # PP\n",
    "#     'BigBag1_3_PE': 2,   # PE\n",
    "#     'BigBag1_4_Tetra': 3, # Tetra\n",
    "#     'BigBag1_6_PS': 4,   # PS\n",
    "    \n",
    "#     #BigBag3\n",
    "#     'BigBag3_PET': 0,  # PET\n",
    "#     'BigBag3_2_PP': 1,   # PP\n",
    "#     'BigBag3_PE': 2,   # PE\n",
    "#     'BigBag3_TETRA': 3, # Tetra\n",
    "#     #'BigBag3_PVC': 5, # PVC\n",
    "#     'BigBag3_6_PS': 4,   # PS\n",
    "    \n",
    "#     'DWRL7_extension_2_PVC': 5,  # PVC\n",
    "#     'BigBag2_5_PVC_Augmented': 5,  # Augmented PVC\n",
    "# }\n",
    "\n",
    "# class CustomPlasticDataset(Dataset):\n",
    "#     def __init__(self, root_dir, class_mapping, transform, tetra_transform=None, ps_transform=None, pvc_transform=None, diverse_transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.class_mapping = class_mapping\n",
    "#         self.transform = transform\n",
    "#         self.tetra_transform = tetra_transform\n",
    "#         self.ps_transform = ps_transform\n",
    "#         self.pvc_transform = pvc_transform\n",
    "#         self.diverse_transform = diverse_transform\n",
    "#         self.image_paths = []\n",
    "#         self.labels = []\n",
    "        \n",
    "#         # Gather image paths and labels\n",
    "#         for class_folder in class_mapping.keys():\n",
    "#             # Load original images\n",
    "#             image_dir = os.path.join(root_dir, class_folder, 'images_cutout')\n",
    "#             if os.path.exists(image_dir):\n",
    "#                 image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "#                 print(f\"Loaded {len(image_files)} original images for {class_folder}\")\n",
    "#                 self.image_paths.extend([os.path.join(image_dir, img) for img in image_files])\n",
    "#                 self.labels.extend([class_mapping[class_folder]] * len(image_files))\n",
    "\n",
    "#             # Load augmented images if they exist\n",
    "#             augmented_dir = os.path.join(root_dir, class_folder)  # Path to the augmented class\n",
    "#             if os.path.exists(augmented_dir) and 'Augmented' in class_folder:\n",
    "#                 augmented_files = [f for f in os.listdir(augmented_dir) if f.endswith(('.jpg', '.png'))]\n",
    "#                 print(f\"Loaded {len(augmented_files)} augmented images for {class_folder}\")\n",
    "#                 self.image_paths.extend([os.path.join(augmented_dir, img) for img in augmented_files])\n",
    "#                 self.labels.extend([class_mapping[class_folder]] * len(augmented_files))  # Map to the same class label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.image_paths[idx]\n",
    "#         image = Image.open(img_path)\n",
    "#         label = self.labels[idx]\n",
    "        \n",
    "#         # Apply specific transformations based on class\n",
    "#         if label == 3:  # Tetra\n",
    "#             image = self.tetra_transform(image)\n",
    "#         elif label == 4:  # PS\n",
    "#             image = self.ps_transform(image)\n",
    "#         elif label == 5:  # PVC\n",
    "#             image = self.pvc_transform(image)\n",
    "#         else:\n",
    "#             # Use diverse_transform with a 50% chance for other classes\n",
    "#             if self.diverse_transform is not None and random.random() > 0.5:\n",
    "#                 image = self.diverse_transform(image)\n",
    "#             else:\n",
    "#                 image = self.transform(image)\n",
    "        \n",
    "#         return image, label\n",
    "\n",
    "\n",
    "# # Augmentation for TETRA (Moderate)\n",
    "# tetra_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "#     transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Augmentation for PS (Light)\n",
    "# ps_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomCrop(224, padding=4),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Regular transform for other classes\n",
    "# regular_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "# ])\n",
    "    \n",
    "# # Augmentation for PVC (Light)\n",
    "# pvc_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# # Define more diverse transformations for augmentation\n",
    "# diverse_transform = transforms.Compose([\n",
    "#     transforms.RandomHorizontalFlip(p=0.5),  # Random horizontal flip with 50% probability\n",
    "#     transforms.RandomRotation(degrees=30),  # Random rotation by up to 30 degrees\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n",
    "#     transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # Random crop with resizing\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# def augment_and_save_images(original_dataset, class_label, transform, target_dir, num_augmented_images):\n",
    "#     os.makedirs(target_dir, exist_ok=True)  # Create the target directory if it doesn't exist\n",
    "#     class_images = [original_dataset.image_paths[idx] for idx, label in enumerate(original_dataset.labels) if label == class_label]\n",
    "\n",
    "#     for img_path in tqdm(class_images, desc=f'Augmenting Class {class_label}'):\n",
    "#         img = Image.open(img_path)\n",
    "#         for i in range(num_augmented_images):\n",
    "#             augmented_img = transform(img)  # Apply the transformation\n",
    "#             augmented_img = transforms.ToPILImage()(augmented_img)  # Convert back to PIL Image\n",
    "            \n",
    "#             # Create a unique filename using the original filename and the index\n",
    "#             base_filename = os.path.basename(img_path).split('.')[0]  # Get the original filename without extension\n",
    "#             augmented_img.save(os.path.join(target_dir, f'augmented_{base_filename}_{i}.png'))  # Save with a unique name\n",
    "\n",
    "# # Function to count images in each class\n",
    "# def count_images_in_classes(base_dir, class_mapping):\n",
    "#     class_counts = {class_name: 0 for class_name in class_mapping.keys()}\n",
    "\n",
    "#     for class_name in class_mapping.keys():\n",
    "#         # Check for original images\n",
    "#         image_dir = os.path.join(base_dir, class_name, 'images_cutout')  # Original images path\n",
    "#         if os.path.exists(image_dir):\n",
    "#             image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "#             class_counts[class_name] += len(image_files)\n",
    "\n",
    "#         # Check for augmented images in their respective folders\n",
    "#         if 'Augmented' in class_name:\n",
    "#             augmented_dir = os.path.join(base_dir, class_name)  # Path to the augmented class\n",
    "#             if os.path.exists(augmented_dir):\n",
    "#                 augmented_files = [f for f in os.listdir(augmented_dir) if f.endswith(('.jpg', '.png'))]\n",
    "#                 class_counts[class_name] += len(augmented_files)  # Count augmented images\n",
    "\n",
    "#     return class_counts\n",
    "\n",
    "# # Directory where all class folders are stored\n",
    "# data_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/DWRL7/data'\n",
    "\n",
    "# # Create dataset instance\n",
    "# plastic_dataset = CustomPlasticDataset(\n",
    "#     root_dir=data_dir, \n",
    "#     class_mapping=class_mapping, \n",
    "#     transform=regular_transform,\n",
    "#     tetra_transform=tetra_transform,\n",
    "#     ps_transform=ps_transform,\n",
    "#     pvc_transform=pvc_transform,\n",
    "#     diverse_transform=diverse_transform \n",
    "# )\n",
    "\n",
    "# # After creating the dataset instance, count the classes\n",
    "# initial_class_counts = Counter(plastic_dataset.labels)\n",
    "# print('Initial ClassCounts after augmentation:', initial_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size (with original + augmented): 14721\n",
      "Validation set size (with original + augmented): 3681\n",
      "Test set size (original only): 2040\n",
      "DataLoader setup complete for 4 clients.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset, random_split, DataLoader, WeightedRandomSampler\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Step 1: Separate Original and Augmented Images by class\n",
    "original_indices_by_class = defaultdict(list)\n",
    "augmented_indices_by_class = defaultdict(list)\n",
    "\n",
    "for i, (path, label) in enumerate(zip(plastic_dataset.image_paths, plastic_dataset.labels)):\n",
    "    if 'Augmented' not in path:  # Original images\n",
    "        original_indices_by_class[label].append(i)\n",
    "    else:  # Augmented images\n",
    "        augmented_indices_by_class[label].append(i)\n",
    "\n",
    "# Step 2: Calculate test size based on total images (original + augmented)\n",
    "test_indices = []\n",
    "train_val_indices_by_class = defaultdict(list)\n",
    "\n",
    "for label, original_indices in original_indices_by_class.items():\n",
    "    total_images_in_class = len(original_indices) + len(augmented_indices_by_class[label])  # Total images in class\n",
    "    test_size = int(0.1 * total_images_in_class)  # 10% of total images (original + augmented)\n",
    "    \n",
    "    # Ensure there are enough original images to select from for the test set\n",
    "    if test_size > len(original_indices):\n",
    "        raise ValueError(f\"Not enough original images in class {label} for test set.\")\n",
    "    \n",
    "    random.shuffle(original_indices)  # Shuffle for randomness\n",
    "    test_indices.extend(original_indices[:test_size])  # First 10% goes to test set\n",
    "    train_val_indices_by_class[label].extend(original_indices[test_size:])  # Remaining original images\n",
    "\n",
    "# Step 3: Combine remaining original and augmented images for training/validation split\n",
    "train_val_indices = []\n",
    "for label, original_remaining in train_val_indices_by_class.items():\n",
    "    train_val_indices.extend(original_remaining)  # Add remaining original images\n",
    "    train_val_indices.extend(augmented_indices_by_class[label])  # Add augmented images for this class\n",
    "\n",
    "# Step 4: Split the remaining images into training and validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(train_val_indices))  # 80% for training\n",
    "val_size = len(train_val_indices) - train_size  # 20% for validation\n",
    "\n",
    "# Randomly split the combined dataset into train and validation sets\n",
    "train_indices, val_indices = random_split(train_val_indices, [train_size, val_size])\n",
    "\n",
    "# Step 5: Create Subsets for train, validation, and test sets\n",
    "train_dataset = Subset(plastic_dataset, train_indices)\n",
    "val_dataset = Subset(plastic_dataset, val_indices)\n",
    "test_dataset = Subset(plastic_dataset, test_indices)  # Test set with only original images\n",
    "\n",
    "# Step 6: Split train_dataset into 4 clients\n",
    "client_split_size = len(train_dataset) // 4\n",
    "remaining = len(train_dataset) - client_split_size * 4\n",
    "split_sizes = [client_split_size] * 4\n",
    "split_sizes[-1] += remaining  # Adjust last split for any remainder\n",
    "\n",
    "client_datasets = random_split(train_dataset, split_sizes)\n",
    "\n",
    "# Step 7: Create DataLoaders with WeightedRandomSampler for each client\n",
    "batch_size = 32\n",
    "client_loaders = []\n",
    "\n",
    "for client_data in client_datasets:\n",
    "    # Calculate sample weights for each client\n",
    "    client_labels = [train_dataset.dataset.labels[idx] for idx in client_data.indices]\n",
    "    client_class_counts = Counter(client_labels)\n",
    "    client_class_weights = {cls: 1.0 / client_class_counts[cls] for cls in client_class_counts}\n",
    "    \n",
    "    # Create sample weights for each image\n",
    "    sample_weights = [client_class_weights[label] for label in client_labels]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    # Create DataLoader with the sampler\n",
    "    client_loader = DataLoader(client_data, batch_size=batch_size, sampler=sampler)\n",
    "    client_loaders.append(client_loader)\n",
    "\n",
    "# Step 8: Create DataLoaders for validation and test sets without sampling\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset sizes for verification\n",
    "print(f'Training set size (with original + augmented): {len(train_dataset)}')\n",
    "print(f'Validation set size (with original + augmented): {len(val_dataset)}')\n",
    "print(f'Test set size (original only): {len(test_dataset)}')\n",
    "print(\"DataLoader setup complete for 4 clients.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 dataset size: 3680\n",
      "Client 2 dataset size: 3680\n",
      "Client 3 dataset size: 3680\n",
      "Client 4 dataset size: 3681\n"
     ]
    }
   ],
   "source": [
    "# Print sizes of each client dataset\n",
    "for i, client_data in enumerate(client_datasets):\n",
    "    print(f\"Client {i+1} dataset size: {len(client_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ClassCounts: Counter({6: 3102, 1: 2810, 2: 2102, 5: 1862, 0: 1818, 4: 1730, 3: 1297})\n",
      "Class Weights: tensor([0.0006, 0.0004, 0.0005, 0.0008, 0.0006, 0.0005, 0.0003],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "# Count the frequency of each class in the entire training dataset\n",
    "train_labels = [plastic_dataset.labels[i] for i in train_dataset.indices]\n",
    "train_class_counts = Counter(train_labels)\n",
    "\n",
    "# Print class counts for debugging\n",
    "print('Training ClassCounts:', train_class_counts)\n",
    "\n",
    "# Calculate weights for each class: inverse of frequency for the classes (0 to 6)\n",
    "class_weights = {cls: 1.0 / count for cls, count in train_class_counts.items() if cls < 7}\n",
    "\n",
    "# Create the class_weight_tensor based on class weights (for classes 0-6)\n",
    "class_weight_tensor = torch.tensor(\n",
    "    [class_weights.get(i, 0.0) for i in range(7)]\n",
    ").to(device)\n",
    "\n",
    "# Print class weights for debugging\n",
    "print(\"Class Weights:\", class_weight_tensor)\n",
    "\n",
    "# Use CrossEntropyLoss with class weights in each clientâ€™s training\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weight_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Class Weights: tensor([0.5501, 0.3559, 0.4757, 0.7710, 0.5780, 0.5371, 0.3224],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "scaled_class_weight_tensor = class_weight_tensor * 1000\n",
    "print(\"Scaled Class Weights:\", scaled_class_weight_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Class Wights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Class Weights: tensor([0.1531, 0.0990, 0.1359, 0.2139, 0.1608, 0.1486, 0.0887],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# # Calculate the total sum of class weights\n",
    "# total_weight = sum(class_weight_tensor.cpu().numpy())\n",
    "\n",
    "# # Normalize the weights\n",
    "# normalized_class_weight_tensor = class_weight_tensor / total_weight\n",
    "\n",
    "# # Convert back to tensor if necessary\n",
    "# normalized_class_weight_tensor = normalized_class_weight_tensor.to(device)\n",
    "\n",
    "# # Print the normalized weights for debugging\n",
    "# print(\"Normalized Class Weights:\", normalized_class_weight_tensor)\n",
    "\n",
    "# # Use CrossEntropyLoss with class weights\n",
    "# criterion = nn.CrossEntropyLoss(weight=normalized_class_weight_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FL with FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training for DWRL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated learning round 1/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 1/40, Val Loss: 0.6332, Val Accuracy: 72.40%\n",
      "Starting federated learning round 2/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 2/40, Val Loss: 0.5697, Val Accuracy: 74.87%\n",
      "Starting federated learning round 3/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 3/40, Val Loss: 0.4952, Val Accuracy: 77.15%\n",
      "Starting federated learning round 4/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 4/40, Val Loss: 0.4822, Val Accuracy: 78.67%\n",
      "Starting federated learning round 5/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 5/40, Val Loss: 0.4576, Val Accuracy: 79.84%\n",
      "Starting federated learning round 6/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 6/40, Val Loss: 0.4486, Val Accuracy: 80.14%\n",
      "Starting federated learning round 7/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 7/40, Val Loss: 0.4224, Val Accuracy: 80.71%\n",
      "Starting federated learning round 8/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 8/40, Val Loss: 0.4271, Val Accuracy: 80.90%\n",
      "Starting federated learning round 9/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 9/40, Val Loss: 0.4180, Val Accuracy: 81.88%\n",
      "Starting federated learning round 10/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 10/40, Val Loss: 0.4117, Val Accuracy: 81.55%\n",
      "Starting federated learning round 11/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 11/40, Val Loss: 0.4103, Val Accuracy: 81.61%\n",
      "Starting federated learning round 12/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 12/40, Val Loss: 0.4006, Val Accuracy: 82.37%\n",
      "Starting federated learning round 13/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 13/40, Val Loss: 0.3970, Val Accuracy: 82.29%\n",
      "Starting federated learning round 14/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 14/40, Val Loss: 0.4005, Val Accuracy: 82.80%\n",
      "Starting federated learning round 15/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 15/40, Val Loss: 0.3952, Val Accuracy: 82.75%\n",
      "Starting federated learning round 16/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 16/40, Val Loss: 0.3829, Val Accuracy: 83.37%\n",
      "Starting federated learning round 17/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 17/40, Val Loss: 0.3903, Val Accuracy: 84.27%\n",
      "Starting federated learning round 18/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 18/40, Val Loss: 0.3782, Val Accuracy: 84.03%\n",
      "Starting federated learning round 19/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 19/40, Val Loss: 0.3919, Val Accuracy: 83.27%\n",
      "Starting federated learning round 20/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 20/40, Val Loss: 0.3702, Val Accuracy: 83.46%\n",
      "Starting federated learning round 21/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 21/40, Val Loss: 0.3859, Val Accuracy: 83.97%\n",
      "Starting federated learning round 22/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 22/40, Val Loss: 0.3851, Val Accuracy: 84.13%\n",
      "Starting federated learning round 23/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 23/40, Val Loss: 0.3867, Val Accuracy: 83.75%\n",
      "Starting federated learning round 24/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 24/40, Val Loss: 0.3788, Val Accuracy: 84.60%\n",
      "Starting federated learning round 25/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Validating model...\n",
      "Federated Round 25/40, Val Loss: 0.3866, Val Accuracy: 84.16%\n",
      "Early stopping\n",
      "Testing model...\n",
      "Test Loss: 0.4321, Test Accuracy: 83.68%\n",
      "Model saved to /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/final_federated_model_DWRL_4_clients.pth\n",
      "Federated training complete.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models\n",
    "\n",
    "# # Define the model preparation function for 7 classes (DWRL)\n",
    "# def prepare_model(num_classes=7):\n",
    "#     \"\"\"Load a pre-trained ResNet18 model and modify it for DWRL.\"\"\"\n",
    "#     model = models.resnet18(pretrained=True)\n",
    "#     model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "#     return model\n",
    "\n",
    "# # Early Stopping Class (unchanged from previous code)\n",
    "# class EarlyStopping:\n",
    "#     def __init__(self, patience=5, min_delta=0):\n",
    "#         self.patience = patience\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.best_loss = None\n",
    "#         self.early_stop = False\n",
    "\n",
    "#     def __call__(self, val_loss):\n",
    "#         if self.best_loss is None:\n",
    "#             self.best_loss = val_loss\n",
    "#         elif val_loss > self.best_loss - self.min_delta:\n",
    "#             self.counter += 1\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#         else:\n",
    "#             self.best_loss = val_loss\n",
    "#             self.counter = 0\n",
    "\n",
    "# # Federated Training Functions\n",
    "# def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     for _ in range(epochs):\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#     return model.state_dict()\n",
    "\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict\n",
    "\n",
    "# # Federated Training Loop for DWRL with 4 Clients\n",
    "# def train_federated_model(client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate=0.0001, patience=5, min_delta=0):\n",
    "#     model = prepare_model(num_classes=7).to(device)\n",
    "#     criterion = nn.CrossEntropyLoss(weight=normalized_class_weight_tensor)  # Apply class weights for imbalance handling\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for round in range(num_epochs):\n",
    "#         print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "#         state_dicts = []\n",
    "        \n",
    "#         # Train each client's model\n",
    "#         for i, client_loader in enumerate(client_loaders):\n",
    "#             print(f\"Training model for client {i+1}...\")\n",
    "#             client_model = prepare_model(num_classes=7).to(device)\n",
    "#             client_model.load_state_dict(model.state_dict())\n",
    "#             optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "#             client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "#             state_dicts.append(client_state_dict)\n",
    "\n",
    "#         # Federated Averaging\n",
    "#         avg_state_dict = federated_averaging(state_dicts)\n",
    "#         model.load_state_dict(avg_state_dict)\n",
    "#         model.to(device)\n",
    "        \n",
    "#         # Validation Phase\n",
    "#         print(\"Validating model...\")\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_running_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / len(val_loader)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "#         print(f'Federated Round {round+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#     # Test Phase\n",
    "#     print(\"Testing model...\")\n",
    "#     model.eval()\n",
    "#     test_running_loss = 0.0\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_loss = test_running_loss / len(test_loader)\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "#     print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "#     # Save the final model\n",
    "#     model_save_path = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/final_federated_model_DWRL_4_clients.pth'\n",
    "#     torch.save(model.state_dict(), model_save_path)\n",
    "#     print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "#     return model, val_loss, val_accuracy, test_loss, test_accuracy\n",
    "\n",
    "# # Parameters\n",
    "# num_clients = 4\n",
    "# num_epochs = 40\n",
    "# learning_rate = 0.0001\n",
    "# patience = 5\n",
    "# min_delta = 0.01\n",
    "\n",
    "# # Start federated training\n",
    "# print(\"Starting federated training for DWRL...\")\n",
    "# model, val_loss, val_accuracy, test_loss, test_accuracy = train_federated_model(\n",
    "#     client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate, patience, min_delta)\n",
    "# print(\"Federated training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FL with FedLAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Losses: [1.2, 0.8, 1.5, 1.0]\n",
      "Calculated Weights: [0.24444444444444446, 0.2740740740740741, 0.22222222222222224, 0.25925925925925924]\n"
     ]
    }
   ],
   "source": [
    "# Sample client losses for testing (replace these with realistic values if needed)\n",
    "client_losses = [1.2, 0.8, 1.5, 1.0]  # Simulated validation losses for 4 clients\n",
    "\n",
    "# Normalize client losses to use as weights (lower loss = higher weight)\n",
    "total_loss = sum(client_losses)\n",
    "weights = [(1 - loss / total_loss) for loss in client_losses]\n",
    "\n",
    "# Normalize weights to ensure they sum to 1\n",
    "weights = [w / sum(weights) for w in weights]\n",
    "\n",
    "# Print results for debugging\n",
    "print(f\"Client Losses: {client_losses}\")\n",
    "print(f\"Calculated Weights: {weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting federated training for DWRL with FedLAW...\n",
      "Starting federated learning round 1/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 1 Client Weights: [0.2450785343709934, 0.2551459911047975, 0.25552735733174214, 0.244248117192467]\n",
      "Validating model...\n",
      "Federated Round 1/40, Val Loss: 0.6588, Val Accuracy: 70.99%\n",
      "Starting federated learning round 2/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 2 Client Weights: [0.25128937026691145, 0.25101796629929657, 0.24709984520386205, 0.25059281822992996]\n",
      "Validating model...\n",
      "Federated Round 2/40, Val Loss: 0.5580, Val Accuracy: 74.98%\n",
      "Starting federated learning round 3/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 3 Client Weights: [0.25595510152336204, 0.250070379311115, 0.25400913163654854, 0.23996538752897448]\n",
      "Validating model...\n",
      "Federated Round 3/40, Val Loss: 0.5114, Val Accuracy: 77.21%\n",
      "Starting federated learning round 4/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 4 Client Weights: [0.251106047535196, 0.24815636870886482, 0.2554819131559613, 0.24525567059997788]\n",
      "Validating model...\n",
      "Federated Round 4/40, Val Loss: 0.4819, Val Accuracy: 77.97%\n",
      "Starting federated learning round 5/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 5 Client Weights: [0.24830010114563447, 0.25461940083328566, 0.25384508741071804, 0.2432354106103619]\n",
      "Validating model...\n",
      "Federated Round 5/40, Val Loss: 0.4694, Val Accuracy: 79.05%\n",
      "Starting federated learning round 6/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 6 Client Weights: [0.24663284918018125, 0.24872720520759647, 0.252033811469581, 0.2526061341426412]\n",
      "Validating model...\n",
      "Federated Round 6/40, Val Loss: 0.4565, Val Accuracy: 79.90%\n",
      "Starting federated learning round 7/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 7 Client Weights: [0.2560662260658358, 0.25213728953482234, 0.2465780123609226, 0.24521847203841932]\n",
      "Validating model...\n",
      "Federated Round 7/40, Val Loss: 0.4290, Val Accuracy: 80.96%\n",
      "Starting federated learning round 8/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 8 Client Weights: [0.2563257059303864, 0.2553540385734577, 0.24758721839293787, 0.240733037103218]\n",
      "Validating model...\n",
      "Federated Round 8/40, Val Loss: 0.4041, Val Accuracy: 82.21%\n",
      "Starting federated learning round 9/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 9 Client Weights: [0.25288673952157387, 0.25731855445920954, 0.24409541009851887, 0.2456992959206978]\n",
      "Validating model...\n",
      "Federated Round 9/40, Val Loss: 0.4248, Val Accuracy: 80.77%\n",
      "Starting federated learning round 10/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 10 Client Weights: [0.24110136102973778, 0.2526026762928703, 0.24994345773286739, 0.2563525049445246]\n",
      "Validating model...\n",
      "Federated Round 10/40, Val Loss: 0.4104, Val Accuracy: 81.23%\n",
      "Starting federated learning round 11/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 11 Client Weights: [0.24261003551139423, 0.2515957768970056, 0.25392952914988937, 0.2518646584417108]\n",
      "Validating model...\n",
      "Federated Round 11/40, Val Loss: 0.4061, Val Accuracy: 81.64%\n",
      "Starting federated learning round 12/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 12 Client Weights: [0.23698806104824674, 0.25956677780723997, 0.24841800311858667, 0.2550271580259266]\n",
      "Validating model...\n",
      "Federated Round 12/40, Val Loss: 0.3877, Val Accuracy: 82.31%\n",
      "Starting federated learning round 13/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 13 Client Weights: [0.24825125219390443, 0.25643968700375636, 0.24683822227518473, 0.24847083852715454]\n",
      "Validating model...\n",
      "Federated Round 13/40, Val Loss: 0.3921, Val Accuracy: 83.32%\n",
      "Starting federated learning round 14/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 14 Client Weights: [0.25232363668692687, 0.24503714710480565, 0.2518559437577597, 0.2507832724505079]\n",
      "Validating model...\n",
      "Federated Round 14/40, Val Loss: 0.3892, Val Accuracy: 82.67%\n",
      "Starting federated learning round 15/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 15 Client Weights: [0.24985580937608995, 0.25161540276213423, 0.24673675890033897, 0.2517920289614369]\n",
      "Validating model...\n",
      "Federated Round 15/40, Val Loss: 0.3863, Val Accuracy: 82.89%\n",
      "Starting federated learning round 16/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 16 Client Weights: [0.25079811457053286, 0.24904907927755227, 0.2524431864145575, 0.24770961973735747]\n",
      "Validating model...\n",
      "Federated Round 16/40, Val Loss: 0.3880, Val Accuracy: 83.10%\n",
      "Starting federated learning round 17/40...\n",
      "Training model for client 1...\n",
      "Training model for client 2...\n",
      "Training model for client 3...\n",
      "Training model for client 4...\n",
      "Round 17 Client Weights: [0.24993617458328257, 0.25632357056024885, 0.25457646989598426, 0.2391637849604843]\n",
      "Validating model...\n",
      "Federated Round 17/40, Val Loss: 0.3993, Val Accuracy: 82.89%\n",
      "Early stopping\n",
      "Testing model...\n",
      "Test Loss: 0.4696, Test Accuracy: 80.64%\n",
      "Model saved to /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/final_federated_model_DWRL_FedLAW.pth\n",
      "Federated training complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define the model preparation function for 7 classes (DWRL)\n",
    "def prepare_model(num_classes=7):\n",
    "    \"\"\"Load a pre-trained ResNet18 model and modify it for DWRL.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Federated Training Function with adaptive weighting for FedLAW\n",
    "def train_client(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.state_dict()\n",
    "\n",
    "def federated_adaptive_weighting(state_dicts, weights):\n",
    "    # Adaptive weighted averaging\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(weights[i] * state_dicts[i][key] for i in range(len(state_dicts)))\n",
    "    return avg_state_dict\n",
    "\n",
    "# Federated Training Loop with FedLAW for DWRL with 4 Clients\n",
    "def train_federated_model_with_FedLAW(client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate=0.0001, patience=5, min_delta=0):\n",
    "    model = prepare_model(num_classes=7).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=scaled_class_weight_tensor)  # Apply class weights for imbalance handling\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for round in range(num_epochs):\n",
    "        print(f\"Starting federated learning round {round+1}/{num_epochs}...\")\n",
    "        state_dicts = []\n",
    "        client_losses = []\n",
    "        \n",
    "        # Train each client's model\n",
    "        for i, client_loader in enumerate(client_loaders):\n",
    "            print(f\"Training model for client {i+1}...\")\n",
    "            client_model = prepare_model(num_classes=7).to(device)\n",
    "            client_model.load_state_dict(model.state_dict())\n",
    "            optimizer = optim.Adam(client_model.parameters(), lr=learning_rate)\n",
    "            client_state_dict = train_client(client_model, client_loader, criterion, optimizer)\n",
    "            state_dicts.append(client_state_dict)\n",
    "\n",
    "            # Calculate validation loss to determine adaptive weight\n",
    "            client_model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = client_model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            client_losses.append(val_loss)\n",
    "\n",
    "        # Normalize client losses to use as weights (lower loss = higher weight)\n",
    "        # total_loss = sum(client_losses)\n",
    "        # weights = [(1 - loss / total_loss) for loss in client_losses]\n",
    "        \n",
    "       # Normalize client losses to use as weights (lower loss = higher weight)\n",
    "        total_loss = sum(client_losses)\n",
    "        weights = [(1 - loss / total_loss) for loss in client_losses]\n",
    "\n",
    "        # Normalize weights to ensure they sum to 1\n",
    "        weights = [w / sum(weights) for w in weights]\n",
    "        print(f\"Round {round+1} Client Weights: {weights}\")\n",
    "\n",
    "        \n",
    "        # Perform Federated Adaptive Weighted Averaging (FedLAW)\n",
    "        avg_state_dict = federated_adaptive_weighting(state_dicts, weights)\n",
    "        model.load_state_dict(avg_state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Validation Phase\n",
    "        print(\"Validating model...\")\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f'Federated Round {round+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Test Phase\n",
    "    print(\"Testing model...\")\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_loss = test_running_loss / len(test_loader)\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    # Save the final model\n",
    "    model_save_path = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/models/final_federated_model_DWRL_FedLAW.pth'\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    return model, val_loss, val_accuracy, test_loss, test_accuracy\n",
    "\n",
    "# Parameters\n",
    "num_clients = 4\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "patience = 5\n",
    "min_delta = 0.01\n",
    "\n",
    "# Start federated training with FedLAW\n",
    "print(\"Starting federated training for DWRL with FedLAW...\")\n",
    "model, val_loss, val_accuracy, test_loss, test_accuracy = train_federated_model_with_FedLAW(\n",
    "    client_loaders, val_loader, test_loader, num_clients, num_epochs, learning_rate, patience, min_delta)\n",
    "print(\"Federated training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
