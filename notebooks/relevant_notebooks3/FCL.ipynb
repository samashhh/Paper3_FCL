{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting up the Environment for FL with 8 Clients\n",
    "##### Load CIFAR-100 Dataset: We apply standard transformations and load the CIFAR-100 dataset for training and testing.\n",
    "##### Validation Split: We split the dataset into 90% training and 10% validation.\n",
    "##### Output: After running the code, you should see the size of the training set (45,000 images) and validation set (5,000 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Dataset Size: 45000 images\n",
      "Validation Dataset Size: 5000 images\n",
      "Test Dataset Size: 10000 images\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setting up FL with 8 Clients\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformations for CIFAR-100\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset (train and test)\n",
    "dataset_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Total number of images after reserving 10% for validation (90% for training)\n",
    "num_train_images = int(0.9 * len(dataset_train))  # 45,000 images for training\n",
    "num_val_images = len(dataset_train) - num_train_images  # 5,000 images for validation\n",
    "\n",
    "# Randomly split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(dataset_train, [num_train_images, num_val_images])\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Output to check the size of the training, validation, and test sets\n",
    "print(f\"Training Dataset Size: {len(train_loader.dataset)} images\")\n",
    "print(f\"Validation Dataset Size: {len(val_loader.dataset)} images\")\n",
    "print(f\"Test Dataset Size: {len(test_loader.dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split Data for 8 Clients\n",
    "#### Splitting for 8 Clients: We use random_split to split the training dataset into 8 subsets, with each subset containing 5625 images.\n",
    "#### Output: After running the code, you should see the size of each client’s dataset (all should have 5625 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Dataset Size: 5625 images\n",
      "Client 2 Dataset Size: 5625 images\n",
      "Client 3 Dataset Size: 5625 images\n",
      "Client 4 Dataset Size: 5625 images\n",
      "Client 5 Dataset Size: 5625 images\n",
      "Client 6 Dataset Size: 5625 images\n",
      "Client 7 Dataset Size: 5625 images\n",
      "Client 8 Dataset Size: 5625 images\n"
     ]
    }
   ],
   "source": [
    "# # Step 2: Split the training dataset into 8 clients\n",
    "\n",
    "# # Each client will get 5625 images\n",
    "# client_splits = torch.utils.data.random_split(train_dataset, [5625] * 8)\n",
    "\n",
    "# # Verify that each client has 5625 images\n",
    "# for i, client_dataset in enumerate(client_splits):\n",
    "#     print(f\"Client {i+1} Dataset Size: {len(client_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split Data for 4 Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Dataset Size: 11250 images\n",
      "Client 2 Dataset Size: 11250 images\n",
      "Client 3 Dataset Size: 11250 images\n",
      "Client 4 Dataset Size: 11250 images\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the training dataset into 4 clients\n",
    "\n",
    "# Each client will get 11250 images\n",
    "client_splits = torch.utils.data.random_split(train_dataset, [11250] * 4)\n",
    "\n",
    "# Verify that each client has 11250 images\n",
    "for i, client_dataset in enumerate(client_splits):\n",
    "    print(f\"Client {i+1} Dataset Size: {len(client_dataset)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare DataLoaders for Each Client\n",
    "#### Creating DataLoaders: We create a DataLoader for each client to handle training, and another DataLoader for validation.\n",
    "#### Batch Verification: After running the code, you should see the shape of a batch of images and labels from Client 1’s dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 First Batch - Images Shape: torch.Size([256, 3, 224, 224]), Labels Shape: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create DataLoaders for each client\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "# Create a DataLoader for each client\n",
    "client_loaders = [DataLoader(client_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for client_dataset in client_splits]\n",
    "\n",
    "# Also create a DataLoader for the validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Output to verify one batch from Client 1\n",
    "images, labels = next(iter(client_loaders[0]))\n",
    "print(f\"Client 1 First Batch - Images Shape: {images.shape}, Labels Shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Applying Incremental Splitting with Replay\n",
    "#### First Batch: We start by splitting 2625 images from each client’s dataset as the first batch.\n",
    "#### 6 Incremental Batches: For each of the next 6 batches, we use 500 new images plus 500 replay images (from previously seen data).\n",
    "#### Replay Strategy: Each incremental batch includes 500 images from previously seen batches to help the model retain earlier knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 - Batch 1: 2625 images\n",
      "Client 1 - Batch 2: 1000 images\n",
      "Client 1 - Batch 3: 1000 images\n",
      "Client 1 - Batch 4: 1000 images\n",
      "Client 1 - Batch 5: 1000 images\n",
      "Client 1 - Batch 6: 1000 images\n",
      "Client 1 - Batch 7: 1000 images\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from torch.utils.data import random_split, ConcatDataset\n",
    "\n",
    "# # Split each client's 5625 images into 1 initial batch and 6 incremental batches\n",
    "# initial_split_size = 2625  # First batch size\n",
    "# num_incremental_batches = 6  # Number of incremental batches\n",
    "# new_data_per_batch = 500  # New images per incremental batch\n",
    "# replay_size = 500  # Replay size (previously seen data to include)\n",
    "\n",
    "# # Iterate over all clients to create incremental splits\n",
    "# client_splits_cl = []\n",
    "\n",
    "# for client_idx, client_dataset in enumerate(client_splits):\n",
    "#     # Initial split: 2625 images for the first batch, remaining for incremental batches\n",
    "#     remaining_split_size = len(client_dataset) - initial_split_size  # Should be 3000\n",
    "#     first_split, remaining_dataset = random_split(client_dataset, [initial_split_size, remaining_split_size])\n",
    "\n",
    "#     # Split the remaining dataset into 6 incremental batches of 500 new images each\n",
    "#     incremental_splits = random_split(remaining_dataset, [new_data_per_batch] * num_incremental_batches)\n",
    "    \n",
    "#     # Store the initial split as the first batch\n",
    "#     client_splits_for_cl = [first_split]\n",
    "    \n",
    "#     # Create incremental batches with replay technique\n",
    "#     for i in range(num_incremental_batches):\n",
    "#         # Get the previous seen data from all earlier batches\n",
    "#         previous_data = ConcatDataset(client_splits_for_cl)\n",
    "        \n",
    "#         # Select 500 random previously seen images (for replay)\n",
    "#         replay_data, _ = random_split(previous_data, [replay_size, len(previous_data) - replay_size])\n",
    "        \n",
    "#         # Create the new batch with 500 new images and 500 replay images\n",
    "#         new_batch = ConcatDataset([incremental_splits[i], replay_data])\n",
    "        \n",
    "#         # Add this new batch to the client's splits\n",
    "#         client_splits_for_cl.append(new_batch)\n",
    "    \n",
    "#     client_splits_cl.append(client_splits_for_cl)\n",
    "\n",
    "# # Verify the incremental splits for Client 1\n",
    "# for i, split in enumerate(client_splits_cl[0]):\n",
    "#     print(f\"Client 1 - Batch {i+1}: {len(split)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 - Batch 1: 5250 images\n",
      "Client 1 - Batch 2: 2000 images\n",
      "Client 1 - Batch 3: 2000 images\n",
      "Client 1 - Batch 4: 2000 images\n",
      "Client 1 - Batch 5: 2000 images\n",
      "Client 1 - Batch 6: 2000 images\n",
      "Client 1 - Batch 7: 2000 images\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "\n",
    "# Split each client's 11,250 images into 1 initial batch and 6 incremental batches\n",
    "initial_split_size = 5250  # First batch size \n",
    "num_incremental_batches = 6  # Number of incremental batches\n",
    "new_data_per_batch = 1000  # New images per incremental batch (adjusted to fit the total size)\n",
    "replay_size = 1000  # Replay size (previously seen data to include, adjusted to be 1000)\n",
    "\n",
    "# Iterate over all clients to create incremental splits\n",
    "client_splits_cl = []\n",
    "\n",
    "for client_idx, client_dataset in enumerate(client_splits):\n",
    "    # Initial split: 5125 images for the first batch, remaining for incremental batches\n",
    "    remaining_split_size = len(client_dataset) - initial_split_size  # Should be 6125\n",
    "    first_split, remaining_dataset = random_split(client_dataset, [initial_split_size, remaining_split_size])\n",
    "\n",
    "    # Split the remaining dataset into 6 incremental batches of 1000 new images each\n",
    "    incremental_splits = random_split(remaining_dataset, [new_data_per_batch] * num_incremental_batches)\n",
    "    \n",
    "    # Store the initial split as the first batch\n",
    "    client_splits_for_cl = [first_split]\n",
    "    \n",
    "    # Create incremental batches with replay technique\n",
    "    for i in range(num_incremental_batches):\n",
    "        # Get the previous seen data from all earlier batches\n",
    "        previous_data = ConcatDataset(client_splits_for_cl)\n",
    "        \n",
    "        # Select 1000 random previously seen images (for replay)\n",
    "        replay_data, _ = random_split(previous_data, [replay_size, len(previous_data) - replay_size])\n",
    "        \n",
    "        # Create the new batch with 1000 new images and 1000 replay images\n",
    "        new_batch = ConcatDataset([incremental_splits[i], replay_data])\n",
    "        \n",
    "        # Add this new batch to the client's splits\n",
    "        client_splits_for_cl.append(new_batch)\n",
    "    \n",
    "    client_splits_cl.append(client_splits_for_cl)\n",
    "\n",
    "# Verify the incremental splits for Client 1\n",
    "for i, split in enumerate(client_splits_cl[0]):\n",
    "    print(f\"Client 1 - Batch {i+1}: {len(split)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the Model with Continual Learning (CL)\n",
    "#### Model Preparation: For each client, we initialize a ResNet18 model with 100 classes (as in CIFAR-100).\n",
    "#### Training Function: The model is trained on each incremental batch in the Continual Learning (CL) setup. After training on one batch, the next batch is introduced.\n",
    "#### Validation: After each training phase, we evaluate the model using the validation set.\n",
    "#### Early Stopping: The training process includes early stopping to prevent overfitting if the validation loss doesn’t improve after a certain number of epochs.\n",
    "#### Output: After running the code, we should see the training and validation progress for each client and each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Continual Learning for Client 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Batch 1/7\n",
      "Epoch 1/5, Train Loss: 4.4896, Train Accuracy: 4.61%, Val Loss: 4.1334, Val Accuracy: 9.90%\n",
      "Epoch 2/5, Train Loss: 3.7782, Train Accuracy: 17.90%, Val Loss: 3.5704, Val Accuracy: 20.00%\n",
      "Epoch 3/5, Train Loss: 3.2899, Train Accuracy: 27.98%, Val Loss: 3.2131, Val Accuracy: 26.96%\n",
      "Epoch 4/5, Train Loss: 2.9340, Train Accuracy: 35.83%, Val Loss: 2.9628, Val Accuracy: 32.02%\n",
      "Epoch 5/5, Train Loss: 2.6534, Train Accuracy: 41.41%, Val Loss: 2.7683, Val Accuracy: 34.18%\n",
      "Training on Batch 2/7\n",
      "Epoch 1/5, Train Loss: 2.6293, Train Accuracy: 39.90%, Val Loss: 2.6840, Val Accuracy: 36.14%\n",
      "Epoch 2/5, Train Loss: 2.4704, Train Accuracy: 44.50%, Val Loss: 2.6461, Val Accuracy: 37.56%\n",
      "Epoch 3/5, Train Loss: 2.3059, Train Accuracy: 49.10%, Val Loss: 2.6259, Val Accuracy: 36.84%\n",
      "Epoch 4/5, Train Loss: 2.2154, Train Accuracy: 51.15%, Val Loss: 2.5799, Val Accuracy: 37.80%\n",
      "Epoch 5/5, Train Loss: 2.0843, Train Accuracy: 54.00%, Val Loss: 2.5581, Val Accuracy: 38.76%\n",
      "Training on Batch 3/7\n",
      "Epoch 1/5, Train Loss: 2.3035, Train Accuracy: 47.50%, Val Loss: 2.4914, Val Accuracy: 40.24%\n",
      "Epoch 2/5, Train Loss: 2.1870, Train Accuracy: 49.50%, Val Loss: 2.4555, Val Accuracy: 40.52%\n",
      "Epoch 3/5, Train Loss: 2.0298, Train Accuracy: 54.60%, Val Loss: 2.4082, Val Accuracy: 40.98%\n",
      "Epoch 4/5, Train Loss: 1.9044, Train Accuracy: 56.60%, Val Loss: 2.3994, Val Accuracy: 42.24%\n",
      "Epoch 5/5, Train Loss: 1.7997, Train Accuracy: 60.80%, Val Loss: 2.3732, Val Accuracy: 42.00%\n",
      "Training on Batch 4/7\n",
      "Epoch 1/5, Train Loss: 2.1304, Train Accuracy: 48.80%, Val Loss: 2.3292, Val Accuracy: 41.86%\n",
      "Epoch 2/5, Train Loss: 2.0185, Train Accuracy: 51.50%, Val Loss: 2.2868, Val Accuracy: 43.08%\n",
      "Epoch 3/5, Train Loss: 1.8516, Train Accuracy: 56.80%, Val Loss: 2.2501, Val Accuracy: 44.42%\n",
      "Epoch 4/5, Train Loss: 1.7391, Train Accuracy: 60.20%, Val Loss: 2.2409, Val Accuracy: 43.38%\n",
      "Epoch 5/5, Train Loss: 1.6222, Train Accuracy: 63.95%, Val Loss: 2.2305, Val Accuracy: 44.40%\n",
      "Training on Batch 5/7\n",
      "Epoch 1/5, Train Loss: 2.0169, Train Accuracy: 52.15%, Val Loss: 2.1765, Val Accuracy: 45.54%\n",
      "Epoch 2/5, Train Loss: 1.8683, Train Accuracy: 55.60%, Val Loss: 2.1699, Val Accuracy: 44.86%\n",
      "Epoch 3/5, Train Loss: 1.7450, Train Accuracy: 58.90%, Val Loss: 2.1633, Val Accuracy: 45.34%\n",
      "Epoch 4/5, Train Loss: 1.6610, Train Accuracy: 62.00%, Val Loss: 2.1581, Val Accuracy: 44.88%\n",
      "Epoch 5/5, Train Loss: 1.5495, Train Accuracy: 65.80%, Val Loss: 2.1124, Val Accuracy: 46.30%\n",
      "Training on Batch 6/7\n",
      "Epoch 1/5, Train Loss: 1.9143, Train Accuracy: 53.05%, Val Loss: 2.0977, Val Accuracy: 46.76%\n",
      "Epoch 2/5, Train Loss: 1.7229, Train Accuracy: 59.20%, Val Loss: 2.0700, Val Accuracy: 46.96%\n",
      "Epoch 3/5, Train Loss: 1.6157, Train Accuracy: 62.90%, Val Loss: 2.0645, Val Accuracy: 46.74%\n",
      "Epoch 4/5, Train Loss: 1.5180, Train Accuracy: 64.35%, Val Loss: 2.0456, Val Accuracy: 47.44%\n",
      "Epoch 5/5, Train Loss: 1.4290, Train Accuracy: 67.05%, Val Loss: 2.0385, Val Accuracy: 48.28%\n",
      "Training on Batch 7/7\n",
      "Epoch 1/5, Train Loss: 1.8434, Train Accuracy: 53.70%, Val Loss: 2.0329, Val Accuracy: 48.22%\n",
      "Epoch 2/5, Train Loss: 1.7158, Train Accuracy: 58.55%, Val Loss: 2.0084, Val Accuracy: 48.90%\n",
      "Epoch 3/5, Train Loss: 1.5585, Train Accuracy: 63.55%, Val Loss: 1.9670, Val Accuracy: 48.86%\n",
      "Epoch 4/5, Train Loss: 1.4226, Train Accuracy: 66.70%, Val Loss: 1.9909, Val Accuracy: 48.44%\n",
      "Epoch 5/5, Train Loss: 1.3665, Train Accuracy: 68.10%, Val Loss: 1.9755, Val Accuracy: 49.62%\n",
      "Testing model for Client 1 after Continual Learning...\n",
      "Test Accuracy: 49.77%\n",
      "Model for Client 1 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_1_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 2...\n",
      "Training on Batch 1/7\n",
      "Epoch 1/5, Train Loss: 4.4454, Train Accuracy: 4.86%, Val Loss: 4.1158, Val Accuracy: 10.80%\n",
      "Epoch 2/5, Train Loss: 3.7454, Train Accuracy: 19.05%, Val Loss: 3.5359, Val Accuracy: 20.16%\n",
      "Epoch 3/5, Train Loss: 3.2571, Train Accuracy: 28.50%, Val Loss: 3.2134, Val Accuracy: 25.88%\n",
      "Epoch 4/5, Train Loss: 2.9192, Train Accuracy: 36.53%, Val Loss: 2.9162, Val Accuracy: 31.20%\n",
      "Epoch 5/5, Train Loss: 2.6571, Train Accuracy: 40.63%, Val Loss: 2.7588, Val Accuracy: 35.58%\n",
      "Training on Batch 2/7\n",
      "Epoch 1/5, Train Loss: 2.6260, Train Accuracy: 41.05%, Val Loss: 2.6995, Val Accuracy: 36.38%\n",
      "Epoch 2/5, Train Loss: 2.4736, Train Accuracy: 44.60%, Val Loss: 2.6460, Val Accuracy: 36.22%\n",
      "Epoch 3/5, Train Loss: 2.3352, Train Accuracy: 48.55%, Val Loss: 2.6274, Val Accuracy: 36.92%\n",
      "Epoch 4/5, Train Loss: 2.1874, Train Accuracy: 52.30%, Val Loss: 2.5953, Val Accuracy: 37.72%\n",
      "Epoch 5/5, Train Loss: 2.0588, Train Accuracy: 54.55%, Val Loss: 2.5006, Val Accuracy: 38.72%\n",
      "Training on Batch 3/7\n",
      "Epoch 1/5, Train Loss: 2.3805, Train Accuracy: 43.95%, Val Loss: 2.4623, Val Accuracy: 40.50%\n",
      "Epoch 2/5, Train Loss: 2.2098, Train Accuracy: 50.20%, Val Loss: 2.4176, Val Accuracy: 41.40%\n",
      "Epoch 3/5, Train Loss: 2.0657, Train Accuracy: 52.90%, Val Loss: 2.4115, Val Accuracy: 41.34%\n",
      "Epoch 4/5, Train Loss: 1.9308, Train Accuracy: 57.20%, Val Loss: 2.3827, Val Accuracy: 42.38%\n",
      "Epoch 5/5, Train Loss: 1.8487, Train Accuracy: 60.00%, Val Loss: 2.3700, Val Accuracy: 42.30%\n",
      "Training on Batch 4/7\n",
      "Epoch 1/5, Train Loss: 2.1490, Train Accuracy: 47.65%, Val Loss: 2.3323, Val Accuracy: 43.30%\n",
      "Epoch 2/5, Train Loss: 1.9767, Train Accuracy: 54.00%, Val Loss: 2.2713, Val Accuracy: 43.34%\n",
      "Epoch 3/5, Train Loss: 1.8767, Train Accuracy: 57.25%, Val Loss: 2.2530, Val Accuracy: 44.40%\n",
      "Epoch 4/5, Train Loss: 1.7888, Train Accuracy: 59.55%, Val Loss: 2.2363, Val Accuracy: 44.90%\n",
      "Epoch 5/5, Train Loss: 1.6532, Train Accuracy: 62.70%, Val Loss: 2.2215, Val Accuracy: 45.04%\n",
      "Training on Batch 5/7\n",
      "Epoch 1/5, Train Loss: 1.9909, Train Accuracy: 51.85%, Val Loss: 2.1236, Val Accuracy: 47.70%\n",
      "Epoch 2/5, Train Loss: 1.8485, Train Accuracy: 57.70%, Val Loss: 2.1342, Val Accuracy: 46.68%\n",
      "Epoch 3/5, Train Loss: 1.7454, Train Accuracy: 59.15%, Val Loss: 2.1250, Val Accuracy: 46.78%\n",
      "Epoch 4/5, Train Loss: 1.6441, Train Accuracy: 62.10%, Val Loss: 2.1075, Val Accuracy: 47.12%\n",
      "Epoch 5/5, Train Loss: 1.5083, Train Accuracy: 66.25%, Val Loss: 2.1335, Val Accuracy: 47.52%\n",
      "Training on Batch 6/7\n",
      "Epoch 1/5, Train Loss: 1.9167, Train Accuracy: 52.00%, Val Loss: 2.0899, Val Accuracy: 46.76%\n",
      "Epoch 2/5, Train Loss: 1.7500, Train Accuracy: 57.90%, Val Loss: 2.0824, Val Accuracy: 47.46%\n",
      "Epoch 3/5, Train Loss: 1.6732, Train Accuracy: 59.90%, Val Loss: 2.0609, Val Accuracy: 47.78%\n",
      "Epoch 4/5, Train Loss: 1.4975, Train Accuracy: 67.00%, Val Loss: 2.0643, Val Accuracy: 47.26%\n",
      "Epoch 5/5, Train Loss: 1.4357, Train Accuracy: 68.40%, Val Loss: 2.0519, Val Accuracy: 48.28%\n",
      "Training on Batch 7/7\n",
      "Epoch 1/5, Train Loss: 1.8212, Train Accuracy: 55.25%, Val Loss: 1.9930, Val Accuracy: 49.00%\n",
      "Epoch 2/5, Train Loss: 1.6862, Train Accuracy: 59.20%, Val Loss: 1.9917, Val Accuracy: 48.46%\n",
      "Epoch 3/5, Train Loss: 1.5689, Train Accuracy: 62.45%, Val Loss: 1.9838, Val Accuracy: 48.58%\n",
      "Epoch 4/5, Train Loss: 1.4687, Train Accuracy: 65.75%, Val Loss: 1.9668, Val Accuracy: 49.20%\n",
      "Epoch 5/5, Train Loss: 1.3709, Train Accuracy: 69.30%, Val Loss: 1.9858, Val Accuracy: 49.66%\n",
      "Testing model for Client 2 after Continual Learning...\n",
      "Test Accuracy: 50.15%\n",
      "Model for Client 2 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_2_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 3...\n",
      "Training on Batch 1/7\n",
      "Epoch 1/5, Train Loss: 4.4625, Train Accuracy: 4.88%, Val Loss: 4.1276, Val Accuracy: 10.40%\n",
      "Epoch 2/5, Train Loss: 3.7553, Train Accuracy: 19.14%, Val Loss: 3.5576, Val Accuracy: 20.60%\n",
      "Epoch 3/5, Train Loss: 3.2813, Train Accuracy: 29.26%, Val Loss: 3.1928, Val Accuracy: 28.00%\n",
      "Epoch 4/5, Train Loss: 2.9233, Train Accuracy: 35.87%, Val Loss: 2.9527, Val Accuracy: 31.24%\n",
      "Epoch 5/5, Train Loss: 2.6425, Train Accuracy: 42.13%, Val Loss: 2.7550, Val Accuracy: 35.42%\n",
      "Training on Batch 2/7\n",
      "Epoch 1/5, Train Loss: 2.6062, Train Accuracy: 41.55%, Val Loss: 2.7145, Val Accuracy: 36.84%\n",
      "Epoch 2/5, Train Loss: 2.4473, Train Accuracy: 45.95%, Val Loss: 2.6829, Val Accuracy: 36.50%\n",
      "Epoch 3/5, Train Loss: 2.2944, Train Accuracy: 49.00%, Val Loss: 2.6310, Val Accuracy: 37.22%\n",
      "Epoch 4/5, Train Loss: 2.1557, Train Accuracy: 53.05%, Val Loss: 2.5996, Val Accuracy: 38.02%\n",
      "Epoch 5/5, Train Loss: 2.0656, Train Accuracy: 55.55%, Val Loss: 2.5531, Val Accuracy: 39.28%\n",
      "Training on Batch 3/7\n",
      "Epoch 1/5, Train Loss: 2.3395, Train Accuracy: 46.40%, Val Loss: 2.4683, Val Accuracy: 40.30%\n",
      "Epoch 2/5, Train Loss: 2.1579, Train Accuracy: 51.70%, Val Loss: 2.4512, Val Accuracy: 40.18%\n",
      "Epoch 3/5, Train Loss: 2.0216, Train Accuracy: 55.35%, Val Loss: 2.4235, Val Accuracy: 40.86%\n",
      "Epoch 4/5, Train Loss: 1.9264, Train Accuracy: 56.30%, Val Loss: 2.3938, Val Accuracy: 41.50%\n",
      "Epoch 5/5, Train Loss: 1.7948, Train Accuracy: 61.20%, Val Loss: 2.3623, Val Accuracy: 42.32%\n",
      "Training on Batch 4/7\n",
      "Epoch 1/5, Train Loss: 2.0840, Train Accuracy: 51.80%, Val Loss: 2.3246, Val Accuracy: 42.26%\n",
      "Epoch 2/5, Train Loss: 1.9759, Train Accuracy: 54.00%, Val Loss: 2.2617, Val Accuracy: 44.48%\n",
      "Epoch 3/5, Train Loss: 1.8554, Train Accuracy: 57.45%, Val Loss: 2.2537, Val Accuracy: 45.18%\n",
      "Epoch 4/5, Train Loss: 1.7398, Train Accuracy: 61.05%, Val Loss: 2.2675, Val Accuracy: 43.96%\n",
      "Epoch 5/5, Train Loss: 1.6541, Train Accuracy: 63.50%, Val Loss: 2.2440, Val Accuracy: 44.22%\n",
      "Training on Batch 5/7\n",
      "Epoch 1/5, Train Loss: 2.0041, Train Accuracy: 52.00%, Val Loss: 2.2109, Val Accuracy: 45.36%\n",
      "Epoch 2/5, Train Loss: 1.8558, Train Accuracy: 56.05%, Val Loss: 2.2039, Val Accuracy: 45.52%\n",
      "Epoch 3/5, Train Loss: 1.7449, Train Accuracy: 58.35%, Val Loss: 2.1953, Val Accuracy: 45.06%\n",
      "Epoch 4/5, Train Loss: 1.6515, Train Accuracy: 61.20%, Val Loss: 2.1598, Val Accuracy: 45.14%\n",
      "Epoch 5/5, Train Loss: 1.5719, Train Accuracy: 63.70%, Val Loss: 2.1589, Val Accuracy: 45.84%\n",
      "Training on Batch 6/7\n",
      "Epoch 1/5, Train Loss: 1.8913, Train Accuracy: 52.85%, Val Loss: 2.1022, Val Accuracy: 46.60%\n",
      "Epoch 2/5, Train Loss: 1.7738, Train Accuracy: 56.10%, Val Loss: 2.0862, Val Accuracy: 46.90%\n",
      "Epoch 3/5, Train Loss: 1.6211, Train Accuracy: 60.85%, Val Loss: 2.1071, Val Accuracy: 45.82%\n",
      "Epoch 4/5, Train Loss: 1.5499, Train Accuracy: 62.45%, Val Loss: 2.1028, Val Accuracy: 46.70%\n",
      "Epoch 5/5, Train Loss: 1.4321, Train Accuracy: 66.30%, Val Loss: 2.0939, Val Accuracy: 46.82%\n",
      "Training on Batch 7/7\n",
      "Epoch 1/5, Train Loss: 1.8113, Train Accuracy: 53.50%, Val Loss: 2.0110, Val Accuracy: 48.82%\n",
      "Epoch 2/5, Train Loss: 1.6435, Train Accuracy: 59.80%, Val Loss: 1.9693, Val Accuracy: 49.16%\n",
      "Epoch 3/5, Train Loss: 1.5338, Train Accuracy: 63.45%, Val Loss: 1.9891, Val Accuracy: 48.28%\n",
      "Epoch 4/5, Train Loss: 1.4009, Train Accuracy: 68.30%, Val Loss: 2.0079, Val Accuracy: 48.56%\n",
      "Epoch 5/5, Train Loss: 1.3200, Train Accuracy: 70.25%, Val Loss: 2.0087, Val Accuracy: 48.34%\n",
      "Testing model for Client 3 after Continual Learning...\n",
      "Test Accuracy: 49.36%\n",
      "Model for Client 3 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_3_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 4...\n",
      "Training on Batch 1/7\n",
      "Epoch 1/5, Train Loss: 4.4464, Train Accuracy: 5.47%, Val Loss: 4.1152, Val Accuracy: 11.16%\n",
      "Epoch 2/5, Train Loss: 3.7353, Train Accuracy: 19.12%, Val Loss: 3.5435, Val Accuracy: 21.62%\n",
      "Epoch 3/5, Train Loss: 3.2666, Train Accuracy: 29.58%, Val Loss: 3.2129, Val Accuracy: 26.68%\n",
      "Epoch 4/5, Train Loss: 2.9110, Train Accuracy: 36.30%, Val Loss: 2.9476, Val Accuracy: 30.96%\n",
      "Epoch 5/5, Train Loss: 2.6404, Train Accuracy: 42.21%, Val Loss: 2.7165, Val Accuracy: 35.04%\n",
      "Training on Batch 2/7\n",
      "Epoch 1/5, Train Loss: 2.5743, Train Accuracy: 40.00%, Val Loss: 2.7043, Val Accuracy: 36.00%\n",
      "Epoch 2/5, Train Loss: 2.4025, Train Accuracy: 45.95%, Val Loss: 2.6647, Val Accuracy: 35.98%\n",
      "Epoch 3/5, Train Loss: 2.3316, Train Accuracy: 48.20%, Val Loss: 2.6207, Val Accuracy: 37.04%\n",
      "Epoch 4/5, Train Loss: 2.1300, Train Accuracy: 53.20%, Val Loss: 2.5610, Val Accuracy: 37.74%\n",
      "Epoch 5/5, Train Loss: 2.0830, Train Accuracy: 54.20%, Val Loss: 2.5325, Val Accuracy: 39.00%\n",
      "Training on Batch 3/7\n",
      "Epoch 1/5, Train Loss: 2.3535, Train Accuracy: 43.65%, Val Loss: 2.4796, Val Accuracy: 39.40%\n",
      "Epoch 2/5, Train Loss: 2.2211, Train Accuracy: 49.70%, Val Loss: 2.4466, Val Accuracy: 40.58%\n",
      "Epoch 3/5, Train Loss: 2.0610, Train Accuracy: 53.70%, Val Loss: 2.4067, Val Accuracy: 40.88%\n",
      "Epoch 4/5, Train Loss: 1.9742, Train Accuracy: 56.05%, Val Loss: 2.3750, Val Accuracy: 41.36%\n",
      "Epoch 5/5, Train Loss: 1.8791, Train Accuracy: 58.25%, Val Loss: 2.3513, Val Accuracy: 42.22%\n",
      "Training on Batch 4/7\n",
      "Epoch 1/5, Train Loss: 2.1250, Train Accuracy: 47.65%, Val Loss: 2.3265, Val Accuracy: 43.14%\n",
      "Epoch 2/5, Train Loss: 2.0184, Train Accuracy: 52.00%, Val Loss: 2.2912, Val Accuracy: 42.96%\n",
      "Epoch 3/5, Train Loss: 1.8897, Train Accuracy: 55.80%, Val Loss: 2.2611, Val Accuracy: 43.54%\n",
      "Epoch 4/5, Train Loss: 1.7905, Train Accuracy: 57.40%, Val Loss: 2.2337, Val Accuracy: 44.42%\n",
      "Epoch 5/5, Train Loss: 1.6791, Train Accuracy: 62.00%, Val Loss: 2.2480, Val Accuracy: 44.40%\n",
      "Training on Batch 5/7\n",
      "Epoch 1/5, Train Loss: 1.9460, Train Accuracy: 53.40%, Val Loss: 2.1835, Val Accuracy: 46.72%\n",
      "Epoch 2/5, Train Loss: 1.8156, Train Accuracy: 58.15%, Val Loss: 2.1873, Val Accuracy: 46.14%\n",
      "Epoch 3/5, Train Loss: 1.7374, Train Accuracy: 58.95%, Val Loss: 2.1432, Val Accuracy: 45.40%\n",
      "Epoch 4/5, Train Loss: 1.6059, Train Accuracy: 63.70%, Val Loss: 2.1678, Val Accuracy: 45.90%\n",
      "Epoch 5/5, Train Loss: 1.5525, Train Accuracy: 63.70%, Val Loss: 2.1265, Val Accuracy: 46.84%\n",
      "Training on Batch 6/7\n",
      "Epoch 1/5, Train Loss: 1.9310, Train Accuracy: 52.80%, Val Loss: 2.0719, Val Accuracy: 47.58%\n",
      "Epoch 2/5, Train Loss: 1.7631, Train Accuracy: 57.95%, Val Loss: 2.0779, Val Accuracy: 47.54%\n",
      "Epoch 3/5, Train Loss: 1.6526, Train Accuracy: 60.95%, Val Loss: 2.0582, Val Accuracy: 48.30%\n",
      "Epoch 4/5, Train Loss: 1.5236, Train Accuracy: 65.10%, Val Loss: 2.0834, Val Accuracy: 48.12%\n",
      "Epoch 5/5, Train Loss: 1.4450, Train Accuracy: 66.95%, Val Loss: 2.0806, Val Accuracy: 47.74%\n",
      "Training on Batch 7/7\n",
      "Epoch 1/5, Train Loss: 1.7909, Train Accuracy: 54.50%, Val Loss: 2.0143, Val Accuracy: 49.28%\n",
      "Epoch 2/5, Train Loss: 1.6987, Train Accuracy: 56.55%, Val Loss: 2.0016, Val Accuracy: 47.84%\n",
      "Epoch 3/5, Train Loss: 1.5670, Train Accuracy: 63.15%, Val Loss: 2.0094, Val Accuracy: 47.64%\n",
      "Epoch 4/5, Train Loss: 1.4687, Train Accuracy: 64.85%, Val Loss: 2.0144, Val Accuracy: 48.62%\n",
      "Epoch 5/5, Train Loss: 1.4054, Train Accuracy: 68.00%, Val Loss: 2.0093, Val Accuracy: 48.28%\n",
      "Testing model for Client 4 after Continual Learning...\n",
      "Test Accuracy: 48.41%\n",
      "Model for Client 4 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_4_model.pth\n",
      "Continual Learning, testing, and saving models for all clients completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Step 5: Train each client with Continual Learning (CL), test after training, and save models\n",
    "\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Model Preparation Function (ResNet18 for CIFAR-100)\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "# EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Training function for Continual Learning\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, patience=5, min_delta=0):\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_running_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Testing function to evaluate the model on the test set\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Save function\n",
    "def save_model(model, client_idx, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f'Model for Client {client_idx} saved at {model_path}')\n",
    "\n",
    "# Prepare the training and validation loaders for each client\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "num_epochs = 5  # Set the number of epochs for each CL round\n",
    "\n",
    "# Define the directory to save the models\n",
    "save_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/'\n",
    "\n",
    "# Iterate over all clients and perform incremental training (CL)\n",
    "for client_idx, client_batches in enumerate(client_splits_cl):\n",
    "    print(f\"\\nStarting Continual Learning for Client {client_idx + 1}...\")\n",
    "\n",
    "    # Initialize the model once for each client, and then update it for each batch\n",
    "    model = prepare_model().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    # Train on each batch incrementally, continuing from the last updated model\n",
    "    for batch_idx, batch in enumerate(client_batches):\n",
    "        print(f\"Training on Batch {batch_idx + 1}/{len(client_batches)}\")\n",
    "\n",
    "        # Create DataLoader for the current batch\n",
    "        train_loader = DataLoader(batch, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "        # Use the same validation set for all batches\n",
    "        model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "    # After training on all batches, test the model on the test set\n",
    "    print(f\"Testing model for Client {client_idx + 1} after Continual Learning...\")\n",
    "    test_model(model, test_loader)\n",
    "\n",
    "    # Save the model for each client\n",
    "    save_model(model, client_idx + 1, save_dir)\n",
    "\n",
    "print(\"Continual Learning, testing, and saving models for all clients completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and adjusted model for Client 1 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_1_model.pth\n",
      "Loaded and adjusted model for Client 2 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_2_model.pth\n",
      "Loaded and adjusted model for Client 3 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_3_model.pth\n",
      "Loaded and adjusted model for Client 4 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_4_model.pth\n",
      "\n",
      "--- Federated Learning Round 1 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 2 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 3 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 4 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 5 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 6 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 7 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 8 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 9 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "\n",
      "--- Federated Learning Round 10 ---\n",
      "Updated Client 1 model with global model state.\n",
      "Updated Client 2 model with global model state.\n",
      "Updated Client 3 model with global model state.\n",
      "Updated Client 4 model with global model state.\n",
      "Test Accuracy of the global model: 43.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Path to saved client models after CL\n",
    "saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/'\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare model and dynamically adjust the final layer based on saved state_dict dimensions\n",
    "def prepare_model(state_dict, num_clients=4):\n",
    "    \"\"\"Initialize ResNet18 model and modify the output layer dynamically based on state_dict.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    \n",
    "    # Dynamically set final layer based on saved model's state_dict\n",
    "    model.fc = nn.Linear(num_ftrs, state_dict['fc.weight'].shape[0])\n",
    "    model.load_state_dict(state_dict)  # Load adjusted state_dict\n",
    "    return model.to(device)\n",
    "\n",
    "# Load CL models for 4 clients\n",
    "def load_cl_models(num_clients=4):\n",
    "    cl_models = []\n",
    "    for client_idx in range(1, num_clients + 1):\n",
    "        model_path = os.path.join(saved_models_dir, f'client_{client_idx}_model.pth')\n",
    "        state_dict = torch.load(model_path)\n",
    "        model = prepare_model(state_dict)\n",
    "        cl_models.append(model)\n",
    "        print(f'Loaded and adjusted model for Client {client_idx} from {model_path}')\n",
    "    return cl_models\n",
    "\n",
    "# Federated Averaging function\n",
    "def federated_averaging(state_dicts):\n",
    "    avg_state_dict = OrderedDict()\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "    return avg_state_dict\n",
    "\n",
    "# Fine-tune each client's model using FedAvg\n",
    "def apply_federated_learning(cl_models, num_epochs=10):\n",
    "    global_model = cl_models[0]  # Start with Client 1's model structure\n",
    "    for round in range(num_epochs):\n",
    "        print(f'\\n--- Federated Learning Round {round + 1} ---')\n",
    "        state_dicts = [model.state_dict() for model in cl_models]\n",
    "\n",
    "        # Perform FedAvg\n",
    "        avg_state_dict = federated_averaging(state_dicts)\n",
    "        global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "        # Update each client with the global model\n",
    "        for client_idx, client_model in enumerate(cl_models):\n",
    "            client_model.load_state_dict(global_model.state_dict())\n",
    "            print(f'Updated Client {client_idx + 1} model with global model state.')\n",
    "\n",
    "    return global_model\n",
    "\n",
    "\n",
    "\n",
    "# Load models saved after CL\n",
    "cl_models = load_cl_models(num_clients=4)\n",
    "\n",
    "# Apply FL with FedAvg on the saved CL models\n",
    "global_model = apply_federated_learning(cl_models, num_epochs=10)\n",
    "\n",
    "# Test the global model (assuming `test_loader` is defined)\n",
    "test_accuracy = test_global_model(global_model, test_loader)\n",
    "print(f'Test Accuracy of the global model: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# # Function to create DataLoaders for each client\n",
    "# def create_client_loaders(dataset, num_clients=4, batch_size=256, val_split=0.1, num_workers=4):\n",
    "#     \"\"\"Split dataset into `num_clients` parts and create train/validation loaders.\"\"\"\n",
    "#     client_loaders = []\n",
    "#     val_loaders = []\n",
    "    \n",
    "#     # Split the dataset randomly into `num_clients` equal parts\n",
    "#     client_datasets = random_split(dataset, [len(dataset) // num_clients] * num_clients)\n",
    "    \n",
    "#     for client_dataset in client_datasets:\n",
    "#         # Further split each client's dataset into train and validation sets\n",
    "#         train_size = int((1 - val_split) * len(client_dataset))\n",
    "#         val_size = len(client_dataset) - train_size\n",
    "#         train_dataset, val_dataset = random_split(client_dataset, [train_size, val_size])\n",
    "        \n",
    "#         # Create DataLoaders for the client with pin_memory and optimizations\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "#                                   num_workers=num_workers, pin_memory=True)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "#                                 num_workers=num_workers, pin_memory=True)\n",
    "        \n",
    "#         client_loaders.append(train_loader)\n",
    "#         val_loaders.append(val_loader)\n",
    "    \n",
    "#     return client_loaders, val_loaders\n",
    "\n",
    "# # Assuming `dataset_train` is your training dataset\n",
    "# train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=4, batch_size=256, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6a: BASELINE(FL After Full Continual Learning\" (FL-FCL))- Apply Federated Learning (FL) After Continual Learning (CL)\n",
    "#### Federated Averaging: We collect the models from each client after CL, average their weights, and load the averaged weights into the global model.\n",
    "#### Test the Global Model: After aggregation, the global model is tested on the test set, and the test accuracy is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model for Client 1 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_1_model.pth\n",
      "Loaded model for Client 2 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_2_model.pth\n",
      "Loaded model for Client 3 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_3_model.pth\n",
      "Loaded model for Client 4 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_4_model.pth\n",
      "\n",
      "--- Federated Learning Round 1 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 32.78%\n",
      "Epoch 2/10, Train Accuracy: 43.17%\n",
      "Epoch 3/10, Train Accuracy: 48.58%\n",
      "Epoch 4/10, Train Accuracy: 51.16%\n",
      "Epoch 5/10, Train Accuracy: 55.93%\n",
      "Epoch 6/10, Train Accuracy: 56.71%\n",
      "Epoch 7/10, Train Accuracy: 59.83%\n",
      "Epoch 8/10, Train Accuracy: 61.53%\n",
      "Epoch 9/10, Train Accuracy: 63.68%\n",
      "Epoch 10/10, Train Accuracy: 65.08%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 46.14%\n",
      "Epoch 2/10, Train Accuracy: 54.73%\n",
      "Epoch 3/10, Train Accuracy: 57.98%\n",
      "Epoch 4/10, Train Accuracy: 61.25%\n",
      "Epoch 5/10, Train Accuracy: 63.27%\n",
      "Epoch 6/10, Train Accuracy: 64.71%\n",
      "Epoch 7/10, Train Accuracy: 65.39%\n",
      "Epoch 8/10, Train Accuracy: 67.65%\n",
      "Epoch 9/10, Train Accuracy: 69.20%\n",
      "Epoch 10/10, Train Accuracy: 70.72%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 50.84%\n",
      "Epoch 2/10, Train Accuracy: 59.10%\n",
      "Epoch 3/10, Train Accuracy: 62.38%\n",
      "Epoch 4/10, Train Accuracy: 64.37%\n",
      "Epoch 5/10, Train Accuracy: 66.87%\n",
      "Epoch 6/10, Train Accuracy: 67.72%\n",
      "Epoch 7/10, Train Accuracy: 69.17%\n",
      "Epoch 8/10, Train Accuracy: 71.38%\n",
      "Epoch 9/10, Train Accuracy: 72.03%\n",
      "Epoch 10/10, Train Accuracy: 73.07%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 53.32%\n",
      "Epoch 2/10, Train Accuracy: 60.56%\n",
      "Epoch 3/10, Train Accuracy: 64.50%\n",
      "Epoch 4/10, Train Accuracy: 66.82%\n",
      "Epoch 5/10, Train Accuracy: 68.50%\n",
      "Epoch 6/10, Train Accuracy: 71.00%\n",
      "Epoch 7/10, Train Accuracy: 72.06%\n",
      "Epoch 8/10, Train Accuracy: 73.44%\n",
      "Epoch 9/10, Train Accuracy: 73.72%\n",
      "Epoch 10/10, Train Accuracy: 74.00%\n",
      "Test Accuracy after Round 1: 55.09%\n",
      "\n",
      "--- Federated Learning Round 2 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 57.71%\n",
      "Epoch 2/10, Train Accuracy: 65.64%\n",
      "Epoch 3/10, Train Accuracy: 68.76%\n",
      "Epoch 4/10, Train Accuracy: 71.62%\n",
      "Epoch 5/10, Train Accuracy: 72.83%\n",
      "Epoch 6/10, Train Accuracy: 74.22%\n",
      "Epoch 7/10, Train Accuracy: 75.28%\n",
      "Epoch 8/10, Train Accuracy: 75.91%\n",
      "Epoch 9/10, Train Accuracy: 77.61%\n",
      "Epoch 10/10, Train Accuracy: 78.01%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 60.39%\n",
      "Epoch 2/10, Train Accuracy: 67.67%\n",
      "Epoch 3/10, Train Accuracy: 70.52%\n",
      "Epoch 4/10, Train Accuracy: 73.57%\n",
      "Epoch 5/10, Train Accuracy: 75.43%\n",
      "Epoch 6/10, Train Accuracy: 77.07%\n",
      "Epoch 7/10, Train Accuracy: 76.86%\n",
      "Epoch 8/10, Train Accuracy: 77.79%\n",
      "Epoch 9/10, Train Accuracy: 78.38%\n",
      "Epoch 10/10, Train Accuracy: 80.05%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 60.42%\n",
      "Epoch 2/10, Train Accuracy: 68.08%\n",
      "Epoch 3/10, Train Accuracy: 72.79%\n",
      "Epoch 4/10, Train Accuracy: 75.21%\n",
      "Epoch 5/10, Train Accuracy: 75.87%\n",
      "Epoch 6/10, Train Accuracy: 77.75%\n",
      "Epoch 7/10, Train Accuracy: 78.26%\n",
      "Epoch 8/10, Train Accuracy: 78.68%\n",
      "Epoch 9/10, Train Accuracy: 79.27%\n",
      "Epoch 10/10, Train Accuracy: 80.32%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 62.08%\n",
      "Epoch 2/10, Train Accuracy: 70.35%\n",
      "Epoch 3/10, Train Accuracy: 72.92%\n",
      "Epoch 4/10, Train Accuracy: 74.81%\n",
      "Epoch 5/10, Train Accuracy: 76.39%\n",
      "Epoch 6/10, Train Accuracy: 78.36%\n",
      "Epoch 7/10, Train Accuracy: 79.46%\n",
      "Epoch 8/10, Train Accuracy: 79.90%\n",
      "Epoch 9/10, Train Accuracy: 80.04%\n",
      "Epoch 10/10, Train Accuracy: 81.28%\n",
      "Test Accuracy after Round 2: 58.00%\n",
      "\n",
      "--- Federated Learning Round 3 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 64.93%\n",
      "Epoch 2/10, Train Accuracy: 72.14%\n",
      "Epoch 3/10, Train Accuracy: 75.78%\n",
      "Epoch 4/10, Train Accuracy: 77.72%\n",
      "Epoch 5/10, Train Accuracy: 78.64%\n",
      "Epoch 6/10, Train Accuracy: 79.04%\n",
      "Epoch 7/10, Train Accuracy: 80.75%\n",
      "Epoch 8/10, Train Accuracy: 81.24%\n",
      "Epoch 9/10, Train Accuracy: 82.08%\n",
      "Epoch 10/10, Train Accuracy: 81.98%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 66.54%\n",
      "Epoch 2/10, Train Accuracy: 74.30%\n",
      "Epoch 3/10, Train Accuracy: 76.49%\n",
      "Epoch 4/10, Train Accuracy: 78.97%\n",
      "Epoch 5/10, Train Accuracy: 79.31%\n",
      "Epoch 6/10, Train Accuracy: 80.74%\n",
      "Epoch 7/10, Train Accuracy: 81.60%\n",
      "Epoch 8/10, Train Accuracy: 81.65%\n",
      "Epoch 9/10, Train Accuracy: 82.56%\n",
      "Epoch 10/10, Train Accuracy: 83.34%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 67.31%\n",
      "Epoch 2/10, Train Accuracy: 74.09%\n",
      "Epoch 3/10, Train Accuracy: 77.05%\n",
      "Epoch 4/10, Train Accuracy: 78.99%\n",
      "Epoch 5/10, Train Accuracy: 80.90%\n",
      "Epoch 6/10, Train Accuracy: 81.03%\n",
      "Epoch 7/10, Train Accuracy: 81.68%\n",
      "Epoch 8/10, Train Accuracy: 82.58%\n",
      "Epoch 9/10, Train Accuracy: 83.30%\n",
      "Epoch 10/10, Train Accuracy: 83.42%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 67.56%\n",
      "Epoch 2/10, Train Accuracy: 74.84%\n",
      "Epoch 3/10, Train Accuracy: 77.94%\n",
      "Epoch 4/10, Train Accuracy: 79.40%\n",
      "Epoch 5/10, Train Accuracy: 80.68%\n",
      "Epoch 6/10, Train Accuracy: 81.42%\n",
      "Epoch 7/10, Train Accuracy: 82.59%\n",
      "Epoch 8/10, Train Accuracy: 82.62%\n",
      "Epoch 9/10, Train Accuracy: 83.48%\n",
      "Epoch 10/10, Train Accuracy: 83.84%\n",
      "Test Accuracy after Round 3: 59.09%\n",
      "\n",
      "--- Federated Learning Round 4 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 69.22%\n",
      "Epoch 2/10, Train Accuracy: 76.76%\n",
      "Epoch 3/10, Train Accuracy: 79.47%\n",
      "Epoch 4/10, Train Accuracy: 82.02%\n",
      "Epoch 5/10, Train Accuracy: 82.54%\n",
      "Epoch 6/10, Train Accuracy: 82.71%\n",
      "Epoch 7/10, Train Accuracy: 82.85%\n",
      "Epoch 8/10, Train Accuracy: 83.64%\n",
      "Epoch 9/10, Train Accuracy: 84.43%\n",
      "Epoch 10/10, Train Accuracy: 84.60%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 70.97%\n",
      "Epoch 2/10, Train Accuracy: 77.88%\n",
      "Epoch 3/10, Train Accuracy: 80.43%\n",
      "Epoch 4/10, Train Accuracy: 81.65%\n",
      "Epoch 5/10, Train Accuracy: 82.24%\n",
      "Epoch 6/10, Train Accuracy: 83.24%\n",
      "Epoch 7/10, Train Accuracy: 83.37%\n",
      "Epoch 8/10, Train Accuracy: 84.08%\n",
      "Epoch 9/10, Train Accuracy: 84.17%\n",
      "Epoch 10/10, Train Accuracy: 84.55%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 70.98%\n",
      "Epoch 2/10, Train Accuracy: 78.10%\n",
      "Epoch 3/10, Train Accuracy: 80.37%\n",
      "Epoch 4/10, Train Accuracy: 81.63%\n",
      "Epoch 5/10, Train Accuracy: 82.30%\n",
      "Epoch 6/10, Train Accuracy: 83.27%\n",
      "Epoch 7/10, Train Accuracy: 84.04%\n",
      "Epoch 8/10, Train Accuracy: 84.09%\n",
      "Epoch 9/10, Train Accuracy: 84.72%\n",
      "Epoch 10/10, Train Accuracy: 84.73%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 71.93%\n",
      "Epoch 2/10, Train Accuracy: 77.88%\n",
      "Epoch 3/10, Train Accuracy: 81.10%\n",
      "Epoch 4/10, Train Accuracy: 82.42%\n",
      "Epoch 5/10, Train Accuracy: 83.73%\n",
      "Epoch 6/10, Train Accuracy: 83.61%\n",
      "Epoch 7/10, Train Accuracy: 84.41%\n",
      "Epoch 8/10, Train Accuracy: 84.87%\n",
      "Epoch 9/10, Train Accuracy: 85.24%\n",
      "Epoch 10/10, Train Accuracy: 85.22%\n",
      "Test Accuracy after Round 4: 59.88%\n",
      "\n",
      "--- Federated Learning Round 5 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 73.24%\n",
      "Epoch 2/10, Train Accuracy: 79.40%\n",
      "Epoch 3/10, Train Accuracy: 82.39%\n",
      "Epoch 4/10, Train Accuracy: 83.14%\n",
      "Epoch 5/10, Train Accuracy: 83.27%\n",
      "Epoch 6/10, Train Accuracy: 84.62%\n",
      "Epoch 7/10, Train Accuracy: 84.92%\n",
      "Epoch 8/10, Train Accuracy: 85.00%\n",
      "Epoch 9/10, Train Accuracy: 84.70%\n",
      "Epoch 10/10, Train Accuracy: 85.74%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 73.73%\n",
      "Epoch 2/10, Train Accuracy: 79.64%\n",
      "Epoch 3/10, Train Accuracy: 82.11%\n",
      "Epoch 4/10, Train Accuracy: 83.38%\n",
      "Epoch 5/10, Train Accuracy: 83.92%\n",
      "Epoch 6/10, Train Accuracy: 84.59%\n",
      "Epoch 7/10, Train Accuracy: 85.32%\n",
      "Epoch 8/10, Train Accuracy: 85.62%\n",
      "Epoch 9/10, Train Accuracy: 85.40%\n",
      "Epoch 10/10, Train Accuracy: 85.60%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 74.84%\n",
      "Epoch 2/10, Train Accuracy: 80.57%\n",
      "Epoch 3/10, Train Accuracy: 83.17%\n",
      "Epoch 4/10, Train Accuracy: 83.16%\n",
      "Epoch 5/10, Train Accuracy: 83.72%\n",
      "Epoch 6/10, Train Accuracy: 85.19%\n",
      "Epoch 7/10, Train Accuracy: 85.72%\n",
      "Epoch 8/10, Train Accuracy: 85.40%\n",
      "Epoch 9/10, Train Accuracy: 85.96%\n",
      "Epoch 10/10, Train Accuracy: 85.96%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 75.38%\n",
      "Epoch 2/10, Train Accuracy: 80.97%\n",
      "Epoch 3/10, Train Accuracy: 83.37%\n",
      "Epoch 4/10, Train Accuracy: 83.68%\n",
      "Epoch 5/10, Train Accuracy: 84.62%\n",
      "Epoch 6/10, Train Accuracy: 84.62%\n",
      "Epoch 7/10, Train Accuracy: 85.49%\n",
      "Epoch 8/10, Train Accuracy: 85.92%\n",
      "Epoch 9/10, Train Accuracy: 86.03%\n",
      "Epoch 10/10, Train Accuracy: 86.68%\n",
      "Test Accuracy after Round 5: 61.27%\n",
      "\n",
      "--- Federated Learning Round 6 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 76.20%\n",
      "Epoch 2/10, Train Accuracy: 81.88%\n",
      "Epoch 3/10, Train Accuracy: 83.24%\n",
      "Epoch 4/10, Train Accuracy: 84.40%\n",
      "Epoch 5/10, Train Accuracy: 85.21%\n",
      "Epoch 6/10, Train Accuracy: 85.72%\n",
      "Epoch 7/10, Train Accuracy: 86.19%\n",
      "Epoch 8/10, Train Accuracy: 86.13%\n",
      "Epoch 9/10, Train Accuracy: 86.73%\n",
      "Epoch 10/10, Train Accuracy: 86.74%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 76.82%\n",
      "Epoch 2/10, Train Accuracy: 82.02%\n",
      "Epoch 3/10, Train Accuracy: 84.02%\n",
      "Epoch 4/10, Train Accuracy: 84.52%\n",
      "Epoch 5/10, Train Accuracy: 84.79%\n",
      "Epoch 6/10, Train Accuracy: 86.32%\n",
      "Epoch 7/10, Train Accuracy: 85.94%\n",
      "Epoch 8/10, Train Accuracy: 86.07%\n",
      "Epoch 9/10, Train Accuracy: 86.28%\n",
      "Epoch 10/10, Train Accuracy: 86.16%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 77.51%\n",
      "Epoch 2/10, Train Accuracy: 81.45%\n",
      "Epoch 3/10, Train Accuracy: 83.91%\n",
      "Epoch 4/10, Train Accuracy: 85.07%\n",
      "Epoch 5/10, Train Accuracy: 85.53%\n",
      "Epoch 6/10, Train Accuracy: 86.36%\n",
      "Epoch 7/10, Train Accuracy: 86.23%\n",
      "Epoch 8/10, Train Accuracy: 87.10%\n",
      "Epoch 9/10, Train Accuracy: 86.92%\n",
      "Epoch 10/10, Train Accuracy: 86.75%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 77.45%\n",
      "Epoch 2/10, Train Accuracy: 82.50%\n",
      "Epoch 3/10, Train Accuracy: 84.36%\n",
      "Epoch 4/10, Train Accuracy: 84.71%\n",
      "Epoch 5/10, Train Accuracy: 85.56%\n",
      "Epoch 6/10, Train Accuracy: 85.96%\n",
      "Epoch 7/10, Train Accuracy: 86.04%\n",
      "Epoch 8/10, Train Accuracy: 86.80%\n",
      "Epoch 9/10, Train Accuracy: 87.38%\n",
      "Epoch 10/10, Train Accuracy: 86.95%\n",
      "Test Accuracy after Round 6: 61.63%\n",
      "\n",
      "--- Federated Learning Round 7 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 78.68%\n",
      "Epoch 2/10, Train Accuracy: 82.23%\n",
      "Epoch 3/10, Train Accuracy: 84.50%\n",
      "Epoch 4/10, Train Accuracy: 85.32%\n",
      "Epoch 5/10, Train Accuracy: 86.27%\n",
      "Epoch 6/10, Train Accuracy: 86.60%\n",
      "Epoch 7/10, Train Accuracy: 86.68%\n",
      "Epoch 8/10, Train Accuracy: 86.66%\n",
      "Epoch 9/10, Train Accuracy: 87.50%\n",
      "Epoch 10/10, Train Accuracy: 87.84%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 78.14%\n",
      "Epoch 2/10, Train Accuracy: 83.96%\n",
      "Epoch 3/10, Train Accuracy: 84.41%\n",
      "Epoch 4/10, Train Accuracy: 85.59%\n",
      "Epoch 5/10, Train Accuracy: 86.44%\n",
      "Epoch 6/10, Train Accuracy: 86.68%\n",
      "Epoch 7/10, Train Accuracy: 87.31%\n",
      "Epoch 8/10, Train Accuracy: 87.02%\n",
      "Epoch 9/10, Train Accuracy: 87.15%\n",
      "Epoch 10/10, Train Accuracy: 87.75%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 78.60%\n",
      "Epoch 2/10, Train Accuracy: 83.55%\n",
      "Epoch 3/10, Train Accuracy: 85.80%\n",
      "Epoch 4/10, Train Accuracy: 86.20%\n",
      "Epoch 5/10, Train Accuracy: 85.70%\n",
      "Epoch 6/10, Train Accuracy: 86.16%\n",
      "Epoch 7/10, Train Accuracy: 86.52%\n",
      "Epoch 8/10, Train Accuracy: 87.40%\n",
      "Epoch 9/10, Train Accuracy: 87.29%\n",
      "Epoch 10/10, Train Accuracy: 87.89%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 79.21%\n",
      "Epoch 2/10, Train Accuracy: 83.64%\n",
      "Epoch 3/10, Train Accuracy: 85.76%\n",
      "Epoch 4/10, Train Accuracy: 85.65%\n",
      "Epoch 5/10, Train Accuracy: 86.46%\n",
      "Epoch 6/10, Train Accuracy: 86.61%\n",
      "Epoch 7/10, Train Accuracy: 87.44%\n",
      "Epoch 8/10, Train Accuracy: 88.00%\n",
      "Epoch 9/10, Train Accuracy: 87.56%\n",
      "Epoch 10/10, Train Accuracy: 87.50%\n",
      "Test Accuracy after Round 7: 61.65%\n",
      "\n",
      "--- Federated Learning Round 8 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 79.70%\n",
      "Epoch 2/10, Train Accuracy: 84.62%\n",
      "Epoch 3/10, Train Accuracy: 85.10%\n",
      "Epoch 4/10, Train Accuracy: 85.74%\n",
      "Epoch 5/10, Train Accuracy: 86.52%\n",
      "Epoch 6/10, Train Accuracy: 86.94%\n",
      "Epoch 7/10, Train Accuracy: 87.34%\n",
      "Epoch 8/10, Train Accuracy: 87.61%\n",
      "Epoch 9/10, Train Accuracy: 88.01%\n",
      "Epoch 10/10, Train Accuracy: 87.88%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 80.52%\n",
      "Epoch 2/10, Train Accuracy: 84.70%\n",
      "Epoch 3/10, Train Accuracy: 85.95%\n",
      "Epoch 4/10, Train Accuracy: 86.37%\n",
      "Epoch 5/10, Train Accuracy: 87.00%\n",
      "Epoch 6/10, Train Accuracy: 87.38%\n",
      "Epoch 7/10, Train Accuracy: 87.89%\n",
      "Epoch 8/10, Train Accuracy: 87.70%\n",
      "Epoch 9/10, Train Accuracy: 88.12%\n",
      "Epoch 10/10, Train Accuracy: 88.61%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 80.51%\n",
      "Epoch 2/10, Train Accuracy: 85.16%\n",
      "Epoch 3/10, Train Accuracy: 85.98%\n",
      "Epoch 4/10, Train Accuracy: 86.13%\n",
      "Epoch 5/10, Train Accuracy: 87.25%\n",
      "Epoch 6/10, Train Accuracy: 87.21%\n",
      "Epoch 7/10, Train Accuracy: 87.85%\n",
      "Epoch 8/10, Train Accuracy: 88.09%\n",
      "Epoch 9/10, Train Accuracy: 88.15%\n",
      "Epoch 10/10, Train Accuracy: 87.96%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 80.25%\n",
      "Epoch 2/10, Train Accuracy: 84.54%\n",
      "Epoch 3/10, Train Accuracy: 85.61%\n",
      "Epoch 4/10, Train Accuracy: 86.14%\n",
      "Epoch 5/10, Train Accuracy: 87.15%\n",
      "Epoch 6/10, Train Accuracy: 87.10%\n",
      "Epoch 7/10, Train Accuracy: 87.56%\n",
      "Epoch 8/10, Train Accuracy: 88.08%\n",
      "Epoch 9/10, Train Accuracy: 87.96%\n",
      "Epoch 10/10, Train Accuracy: 88.12%\n",
      "Test Accuracy after Round 8: 62.22%\n",
      "\n",
      "--- Federated Learning Round 9 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 81.29%\n",
      "Epoch 2/10, Train Accuracy: 85.14%\n",
      "Epoch 3/10, Train Accuracy: 86.35%\n",
      "Epoch 4/10, Train Accuracy: 87.21%\n",
      "Epoch 5/10, Train Accuracy: 86.61%\n",
      "Epoch 6/10, Train Accuracy: 87.67%\n",
      "Epoch 7/10, Train Accuracy: 87.97%\n",
      "Epoch 8/10, Train Accuracy: 88.52%\n",
      "Epoch 9/10, Train Accuracy: 87.70%\n",
      "Epoch 10/10, Train Accuracy: 88.67%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 81.00%\n",
      "Epoch 2/10, Train Accuracy: 85.12%\n",
      "Epoch 3/10, Train Accuracy: 86.70%\n",
      "Epoch 4/10, Train Accuracy: 87.50%\n",
      "Epoch 5/10, Train Accuracy: 86.99%\n",
      "Epoch 6/10, Train Accuracy: 87.73%\n",
      "Epoch 7/10, Train Accuracy: 87.80%\n",
      "Epoch 8/10, Train Accuracy: 88.20%\n",
      "Epoch 9/10, Train Accuracy: 88.33%\n",
      "Epoch 10/10, Train Accuracy: 88.70%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 81.01%\n",
      "Epoch 2/10, Train Accuracy: 85.17%\n",
      "Epoch 3/10, Train Accuracy: 86.48%\n",
      "Epoch 4/10, Train Accuracy: 86.62%\n",
      "Epoch 5/10, Train Accuracy: 88.04%\n",
      "Epoch 6/10, Train Accuracy: 87.75%\n",
      "Epoch 7/10, Train Accuracy: 88.01%\n",
      "Epoch 8/10, Train Accuracy: 87.50%\n",
      "Epoch 9/10, Train Accuracy: 88.99%\n",
      "Epoch 10/10, Train Accuracy: 88.36%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 81.46%\n",
      "Epoch 2/10, Train Accuracy: 84.69%\n",
      "Epoch 3/10, Train Accuracy: 86.60%\n",
      "Epoch 4/10, Train Accuracy: 87.16%\n",
      "Epoch 5/10, Train Accuracy: 87.67%\n",
      "Epoch 6/10, Train Accuracy: 87.71%\n",
      "Epoch 7/10, Train Accuracy: 87.94%\n",
      "Epoch 8/10, Train Accuracy: 88.42%\n",
      "Epoch 9/10, Train Accuracy: 88.20%\n",
      "Epoch 10/10, Train Accuracy: 88.79%\n",
      "Test Accuracy after Round 9: 62.60%\n",
      "\n",
      "--- Federated Learning Round 10 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/10, Train Accuracy: 82.25%\n",
      "Epoch 2/10, Train Accuracy: 85.48%\n",
      "Epoch 3/10, Train Accuracy: 86.86%\n",
      "Epoch 4/10, Train Accuracy: 87.58%\n",
      "Epoch 5/10, Train Accuracy: 88.60%\n",
      "Epoch 6/10, Train Accuracy: 87.66%\n",
      "Epoch 7/10, Train Accuracy: 88.52%\n",
      "Epoch 8/10, Train Accuracy: 88.84%\n",
      "Epoch 9/10, Train Accuracy: 89.24%\n",
      "Epoch 10/10, Train Accuracy: 89.19%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/10, Train Accuracy: 82.35%\n",
      "Epoch 2/10, Train Accuracy: 85.91%\n",
      "Epoch 3/10, Train Accuracy: 86.76%\n",
      "Epoch 4/10, Train Accuracy: 87.64%\n",
      "Epoch 5/10, Train Accuracy: 87.65%\n",
      "Epoch 6/10, Train Accuracy: 88.11%\n",
      "Epoch 7/10, Train Accuracy: 88.53%\n",
      "Epoch 8/10, Train Accuracy: 88.95%\n",
      "Epoch 9/10, Train Accuracy: 89.13%\n",
      "Epoch 10/10, Train Accuracy: 89.10%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/10, Train Accuracy: 82.76%\n",
      "Epoch 2/10, Train Accuracy: 85.85%\n",
      "Epoch 3/10, Train Accuracy: 86.78%\n",
      "Epoch 4/10, Train Accuracy: 87.74%\n",
      "Epoch 5/10, Train Accuracy: 87.89%\n",
      "Epoch 6/10, Train Accuracy: 88.37%\n",
      "Epoch 7/10, Train Accuracy: 88.76%\n",
      "Epoch 8/10, Train Accuracy: 88.76%\n",
      "Epoch 9/10, Train Accuracy: 88.76%\n",
      "Epoch 10/10, Train Accuracy: 89.17%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/10, Train Accuracy: 82.54%\n",
      "Epoch 2/10, Train Accuracy: 85.95%\n",
      "Epoch 3/10, Train Accuracy: 87.52%\n",
      "Epoch 4/10, Train Accuracy: 87.07%\n",
      "Epoch 5/10, Train Accuracy: 88.14%\n",
      "Epoch 6/10, Train Accuracy: 88.64%\n",
      "Epoch 7/10, Train Accuracy: 89.08%\n",
      "Epoch 8/10, Train Accuracy: 88.68%\n",
      "Epoch 9/10, Train Accuracy: 88.60%\n",
      "Epoch 10/10, Train Accuracy: 89.30%\n",
      "Test Accuracy after Round 10: 62.52%\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models\n",
    "# from torch.cuda.amp import autocast, GradScaler  # For mixed-precision training\n",
    "\n",
    "# # Path where client models are saved\n",
    "# saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/'\n",
    "\n",
    "# # Model Preparation (ResNet18 for CIFAR-100)\n",
    "# def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "#     \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "#     model = models.resnet18(pretrained=True)\n",
    "#     num_ftrs = model.fc.in_features\n",
    "#     if use_dropout:\n",
    "#         model.fc = nn.Sequential(\n",
    "#             nn.Dropout(p=dropout_prob),\n",
    "#             nn.Linear(num_ftrs, num_classes)\n",
    "#         )\n",
    "#     else:\n",
    "#         model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "#     return model\n",
    "\n",
    "# # Function to load a model from a file\n",
    "# def load_client_model(client_idx, save_dir):\n",
    "#     model = prepare_model().to(device)\n",
    "#     model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     print(f'Loaded model for Client {client_idx} from {model_path}')\n",
    "#     return model\n",
    "\n",
    "# # Function to average client models (FL aggregation)\n",
    "# def federated_averaging(state_dicts):\n",
    "#     avg_state_dict = {}\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "#     return avg_state_dict\n",
    "\n",
    "# # Training function for each client after receiving the global model\n",
    "# def fine_tune_client(global_model, train_loader, val_loader, num_epochs=510, use_mixed_precision=True):\n",
    "#     model = global_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Optional: Mixed precision training\n",
    "#     scaler = GradScaler() if use_mixed_precision else None\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if use_mixed_precision:\n",
    "#                 with autocast():\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#             else:\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "            \n",
    "#             if use_mixed_precision:\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         train_accuracy = 100 * correct / total\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "#     return model.state_dict()\n",
    "\n",
    "# # Step 6: Federated Learning (FL) after Continual Learning (CL)\n",
    "# def apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader, num_clients=4, num_epochs=10):\n",
    "#     global_model = prepare_model().to(device)\n",
    "\n",
    "#     # Collect state dicts from all clients after CL\n",
    "#     state_dicts = [model.state_dict() for model in cl_models]\n",
    "\n",
    "#     # Perform federated averaging\n",
    "#     avg_state_dict = federated_averaging(state_dicts)\n",
    "#     global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "#     for round in range(num_epochs):\n",
    "#         print(f'\\n--- Federated Learning Round {round + 1} ---')\n",
    "#         client_state_dicts = []\n",
    "\n",
    "#         for client_idx in range(num_clients):\n",
    "#             print(f'\\nTraining client {client_idx + 1} with the global model')\n",
    "#             client_state_dict = fine_tune_client(global_model, train_loaders[client_idx], val_loaders[client_idx], num_epochs=10)\n",
    "#             client_state_dicts.append(client_state_dict)\n",
    "\n",
    "#         # Federated averaging after each round\n",
    "#         avg_state_dict = federated_averaging(client_state_dicts)\n",
    "#         global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "#         test_accuracy = test_global_model(global_model, test_loader)\n",
    "#         print(f'Test Accuracy after Round {round + 1}: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return global_model\n",
    "\n",
    "# # Function to test the global model\n",
    "# def test_global_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "#     return test_accuracy\n",
    "\n",
    "# # Load models saved after CL\n",
    "# cl_models = [load_client_model(client_idx, saved_models_dir) for client_idx in range(1, 5)]\n",
    "\n",
    "# # Prepare DataLoader splits for each client with pin_memory=True and fewer workers\n",
    "# train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=4, batch_size=256)\n",
    "\n",
    "# # Apply FL and test the global model\n",
    "# global_model = apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6b: (FL After Each Continual Learning Round (FL-CL)) - Apply Federated Learning (FL) After Each CL Round\n",
    "- **Federated Averaging**: After each client completes a round of CL (training on a batch), we perform FL by averaging the client models.\n",
    "- **Global Model Updates**: The aggregated global model is redistributed to all clients before continuing with the next batch.\n",
    "- **Testing**: The global model is tested after each FL round to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model for Client 1 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_1_model.pth\n",
      "Loaded model for Client 2 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_2_model.pth\n",
      "Loaded model for Client 3 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_3_model.pth\n",
      "Loaded model for Client 4 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/client_4_model.pth\n",
      "\n",
      "--- Training and Federated Learning after Batch 1 ---\n",
      "\n",
      "Training client 1 on Batch 1\n",
      "Epoch 1/10, Train Accuracy: 10.21%\n",
      "Epoch 2/10, Train Accuracy: 19.81%\n",
      "Epoch 3/10, Train Accuracy: 27.90%\n",
      "Epoch 4/10, Train Accuracy: 30.06%\n",
      "Epoch 5/10, Train Accuracy: 34.17%\n",
      "Epoch 6/10, Train Accuracy: 37.47%\n",
      "Epoch 7/10, Train Accuracy: 40.29%\n",
      "Epoch 8/10, Train Accuracy: 44.40%\n",
      "Epoch 9/10, Train Accuracy: 45.30%\n",
      "Epoch 10/10, Train Accuracy: 44.88%\n",
      "\n",
      "Training client 2 on Batch 1\n",
      "Epoch 1/10, Train Accuracy: 32.17%\n",
      "Epoch 2/10, Train Accuracy: 40.17%\n",
      "Epoch 3/10, Train Accuracy: 44.23%\n",
      "Epoch 4/10, Train Accuracy: 47.14%\n",
      "Epoch 5/10, Train Accuracy: 46.27%\n",
      "Epoch 6/10, Train Accuracy: 50.44%\n",
      "Epoch 7/10, Train Accuracy: 53.18%\n",
      "Epoch 8/10, Train Accuracy: 53.81%\n",
      "Epoch 9/10, Train Accuracy: 53.66%\n",
      "Epoch 10/10, Train Accuracy: 56.15%\n",
      "\n",
      "Training client 3 on Batch 1\n",
      "Epoch 1/10, Train Accuracy: 37.64%\n",
      "Epoch 2/10, Train Accuracy: 44.91%\n",
      "Epoch 3/10, Train Accuracy: 47.28%\n",
      "Epoch 4/10, Train Accuracy: 50.50%\n",
      "Epoch 5/10, Train Accuracy: 52.78%\n",
      "Epoch 6/10, Train Accuracy: 51.30%\n",
      "Epoch 7/10, Train Accuracy: 56.30%\n",
      "Epoch 8/10, Train Accuracy: 56.69%\n",
      "Epoch 9/10, Train Accuracy: 57.24%\n",
      "Epoch 10/10, Train Accuracy: 60.34%\n",
      "\n",
      "Training client 4 on Batch 1\n",
      "Epoch 1/10, Train Accuracy: 40.80%\n",
      "Epoch 2/10, Train Accuracy: 46.84%\n",
      "Epoch 3/10, Train Accuracy: 52.19%\n",
      "Epoch 4/10, Train Accuracy: 55.22%\n",
      "Epoch 5/10, Train Accuracy: 57.70%\n",
      "Epoch 6/10, Train Accuracy: 58.69%\n",
      "Epoch 7/10, Train Accuracy: 60.50%\n",
      "Epoch 8/10, Train Accuracy: 60.32%\n",
      "Epoch 9/10, Train Accuracy: 64.13%\n",
      "Epoch 10/10, Train Accuracy: 60.93%\n",
      "Test Accuracy after Batch 1: 41.29%\n",
      "\n",
      "--- Training and Federated Learning after Batch 2 ---\n",
      "\n",
      "Training client 1 on Batch 2\n",
      "Epoch 1/10, Train Accuracy: 41.80%\n",
      "Epoch 2/10, Train Accuracy: 52.60%\n",
      "Epoch 3/10, Train Accuracy: 57.20%\n",
      "Epoch 4/10, Train Accuracy: 65.50%\n",
      "Epoch 5/10, Train Accuracy: 68.60%\n",
      "Epoch 6/10, Train Accuracy: 70.25%\n",
      "Epoch 7/10, Train Accuracy: 73.55%\n",
      "Epoch 8/10, Train Accuracy: 75.15%\n",
      "Epoch 9/10, Train Accuracy: 78.15%\n",
      "Epoch 10/10, Train Accuracy: 78.40%\n",
      "\n",
      "Training client 2 on Batch 2\n",
      "Epoch 1/10, Train Accuracy: 43.15%\n",
      "Epoch 2/10, Train Accuracy: 54.85%\n",
      "Epoch 3/10, Train Accuracy: 59.90%\n",
      "Epoch 4/10, Train Accuracy: 64.45%\n",
      "Epoch 5/10, Train Accuracy: 68.40%\n",
      "Epoch 6/10, Train Accuracy: 74.20%\n",
      "Epoch 7/10, Train Accuracy: 75.95%\n",
      "Epoch 8/10, Train Accuracy: 79.15%\n",
      "Epoch 9/10, Train Accuracy: 78.05%\n",
      "Epoch 10/10, Train Accuracy: 79.20%\n",
      "\n",
      "Training client 3 on Batch 2\n",
      "Epoch 1/10, Train Accuracy: 43.05%\n",
      "Epoch 2/10, Train Accuracy: 55.95%\n",
      "Epoch 3/10, Train Accuracy: 63.10%\n",
      "Epoch 4/10, Train Accuracy: 68.60%\n",
      "Epoch 5/10, Train Accuracy: 70.55%\n",
      "Epoch 6/10, Train Accuracy: 73.55%\n",
      "Epoch 7/10, Train Accuracy: 77.10%\n",
      "Epoch 8/10, Train Accuracy: 77.30%\n",
      "Epoch 9/10, Train Accuracy: 79.15%\n",
      "Epoch 10/10, Train Accuracy: 80.35%\n",
      "\n",
      "Training client 4 on Batch 2\n",
      "Epoch 1/10, Train Accuracy: 47.70%\n",
      "Epoch 2/10, Train Accuracy: 58.80%\n",
      "Epoch 3/10, Train Accuracy: 65.65%\n",
      "Epoch 4/10, Train Accuracy: 71.55%\n",
      "Epoch 5/10, Train Accuracy: 72.75%\n",
      "Epoch 6/10, Train Accuracy: 76.30%\n",
      "Epoch 7/10, Train Accuracy: 76.00%\n",
      "Epoch 8/10, Train Accuracy: 79.20%\n",
      "Epoch 9/10, Train Accuracy: 80.10%\n",
      "Epoch 10/10, Train Accuracy: 81.65%\n",
      "Test Accuracy after Batch 2: 45.46%\n",
      "\n",
      "--- Training and Federated Learning after Batch 3 ---\n",
      "\n",
      "Training client 1 on Batch 3\n",
      "Epoch 1/10, Train Accuracy: 48.30%\n",
      "Epoch 2/10, Train Accuracy: 56.60%\n",
      "Epoch 3/10, Train Accuracy: 64.85%\n",
      "Epoch 4/10, Train Accuracy: 69.50%\n",
      "Epoch 5/10, Train Accuracy: 72.25%\n",
      "Epoch 6/10, Train Accuracy: 75.10%\n",
      "Epoch 7/10, Train Accuracy: 77.60%\n",
      "Epoch 8/10, Train Accuracy: 80.20%\n",
      "Epoch 9/10, Train Accuracy: 79.20%\n",
      "Epoch 10/10, Train Accuracy: 80.75%\n",
      "\n",
      "Training client 2 on Batch 3\n",
      "Epoch 1/10, Train Accuracy: 46.80%\n",
      "Epoch 2/10, Train Accuracy: 57.20%\n",
      "Epoch 3/10, Train Accuracy: 63.95%\n",
      "Epoch 4/10, Train Accuracy: 68.55%\n",
      "Epoch 5/10, Train Accuracy: 74.00%\n",
      "Epoch 6/10, Train Accuracy: 75.10%\n",
      "Epoch 7/10, Train Accuracy: 77.65%\n",
      "Epoch 8/10, Train Accuracy: 78.00%\n",
      "Epoch 9/10, Train Accuracy: 79.55%\n",
      "Epoch 10/10, Train Accuracy: 80.70%\n",
      "\n",
      "Training client 3 on Batch 3\n",
      "Epoch 1/10, Train Accuracy: 48.25%\n",
      "Epoch 2/10, Train Accuracy: 58.45%\n",
      "Epoch 3/10, Train Accuracy: 68.20%\n",
      "Epoch 4/10, Train Accuracy: 69.65%\n",
      "Epoch 5/10, Train Accuracy: 73.85%\n",
      "Epoch 6/10, Train Accuracy: 76.70%\n",
      "Epoch 7/10, Train Accuracy: 77.90%\n",
      "Epoch 8/10, Train Accuracy: 79.75%\n",
      "Epoch 9/10, Train Accuracy: 81.05%\n",
      "Epoch 10/10, Train Accuracy: 81.65%\n",
      "\n",
      "Training client 4 on Batch 3\n",
      "Epoch 1/10, Train Accuracy: 49.80%\n",
      "Epoch 2/10, Train Accuracy: 58.90%\n",
      "Epoch 3/10, Train Accuracy: 66.60%\n",
      "Epoch 4/10, Train Accuracy: 71.45%\n",
      "Epoch 5/10, Train Accuracy: 74.85%\n",
      "Epoch 6/10, Train Accuracy: 76.75%\n",
      "Epoch 7/10, Train Accuracy: 78.50%\n",
      "Epoch 8/10, Train Accuracy: 79.65%\n",
      "Epoch 9/10, Train Accuracy: 80.05%\n",
      "Epoch 10/10, Train Accuracy: 82.10%\n",
      "Test Accuracy after Batch 3: 46.60%\n",
      "\n",
      "--- Training and Federated Learning after Batch 4 ---\n",
      "\n",
      "Training client 1 on Batch 4\n",
      "Epoch 1/10, Train Accuracy: 49.40%\n",
      "Epoch 2/10, Train Accuracy: 59.75%\n",
      "Epoch 3/10, Train Accuracy: 65.20%\n",
      "Epoch 4/10, Train Accuracy: 71.70%\n",
      "Epoch 5/10, Train Accuracy: 74.65%\n",
      "Epoch 6/10, Train Accuracy: 76.90%\n",
      "Epoch 7/10, Train Accuracy: 77.35%\n",
      "Epoch 8/10, Train Accuracy: 79.50%\n",
      "Epoch 9/10, Train Accuracy: 82.10%\n",
      "Epoch 10/10, Train Accuracy: 81.30%\n",
      "\n",
      "Training client 2 on Batch 4\n",
      "Epoch 1/10, Train Accuracy: 48.15%\n",
      "Epoch 2/10, Train Accuracy: 58.65%\n",
      "Epoch 3/10, Train Accuracy: 65.30%\n",
      "Epoch 4/10, Train Accuracy: 70.95%\n",
      "Epoch 5/10, Train Accuracy: 74.30%\n",
      "Epoch 6/10, Train Accuracy: 76.60%\n",
      "Epoch 7/10, Train Accuracy: 78.65%\n",
      "Epoch 8/10, Train Accuracy: 80.10%\n",
      "Epoch 9/10, Train Accuracy: 81.05%\n",
      "Epoch 10/10, Train Accuracy: 82.00%\n",
      "\n",
      "Training client 3 on Batch 4\n",
      "Epoch 1/10, Train Accuracy: 49.95%\n",
      "Epoch 2/10, Train Accuracy: 61.70%\n",
      "Epoch 3/10, Train Accuracy: 68.75%\n",
      "Epoch 4/10, Train Accuracy: 71.35%\n",
      "Epoch 5/10, Train Accuracy: 75.55%\n",
      "Epoch 6/10, Train Accuracy: 75.80%\n",
      "Epoch 7/10, Train Accuracy: 78.35%\n",
      "Epoch 8/10, Train Accuracy: 80.75%\n",
      "Epoch 9/10, Train Accuracy: 81.50%\n",
      "Epoch 10/10, Train Accuracy: 83.25%\n",
      "\n",
      "Training client 4 on Batch 4\n",
      "Epoch 1/10, Train Accuracy: 49.30%\n",
      "Epoch 2/10, Train Accuracy: 58.55%\n",
      "Epoch 3/10, Train Accuracy: 68.70%\n",
      "Epoch 4/10, Train Accuracy: 72.30%\n",
      "Epoch 5/10, Train Accuracy: 75.25%\n",
      "Epoch 6/10, Train Accuracy: 77.60%\n",
      "Epoch 7/10, Train Accuracy: 79.80%\n",
      "Epoch 8/10, Train Accuracy: 80.95%\n",
      "Epoch 9/10, Train Accuracy: 81.85%\n",
      "Epoch 10/10, Train Accuracy: 82.75%\n",
      "Test Accuracy after Batch 4: 47.86%\n",
      "\n",
      "--- Training and Federated Learning after Batch 5 ---\n",
      "\n",
      "Training client 1 on Batch 5\n",
      "Epoch 1/10, Train Accuracy: 50.85%\n",
      "Epoch 2/10, Train Accuracy: 61.20%\n",
      "Epoch 3/10, Train Accuracy: 67.60%\n",
      "Epoch 4/10, Train Accuracy: 72.20%\n",
      "Epoch 5/10, Train Accuracy: 75.30%\n",
      "Epoch 6/10, Train Accuracy: 78.95%\n",
      "Epoch 7/10, Train Accuracy: 79.30%\n",
      "Epoch 8/10, Train Accuracy: 80.30%\n",
      "Epoch 9/10, Train Accuracy: 83.25%\n",
      "Epoch 10/10, Train Accuracy: 82.40%\n",
      "\n",
      "Training client 2 on Batch 5\n",
      "Epoch 1/10, Train Accuracy: 52.55%\n",
      "Epoch 2/10, Train Accuracy: 62.50%\n",
      "Epoch 3/10, Train Accuracy: 67.45%\n",
      "Epoch 4/10, Train Accuracy: 71.40%\n",
      "Epoch 5/10, Train Accuracy: 76.05%\n",
      "Epoch 6/10, Train Accuracy: 77.00%\n",
      "Epoch 7/10, Train Accuracy: 78.35%\n",
      "Epoch 8/10, Train Accuracy: 80.45%\n",
      "Epoch 9/10, Train Accuracy: 81.70%\n",
      "Epoch 10/10, Train Accuracy: 83.05%\n",
      "\n",
      "Training client 3 on Batch 5\n",
      "Epoch 1/10, Train Accuracy: 51.20%\n",
      "Epoch 2/10, Train Accuracy: 61.15%\n",
      "Epoch 3/10, Train Accuracy: 67.55%\n",
      "Epoch 4/10, Train Accuracy: 74.10%\n",
      "Epoch 5/10, Train Accuracy: 74.80%\n",
      "Epoch 6/10, Train Accuracy: 76.40%\n",
      "Epoch 7/10, Train Accuracy: 79.30%\n",
      "Epoch 8/10, Train Accuracy: 80.25%\n",
      "Epoch 9/10, Train Accuracy: 80.70%\n",
      "Epoch 10/10, Train Accuracy: 81.85%\n",
      "\n",
      "Training client 4 on Batch 5\n",
      "Epoch 1/10, Train Accuracy: 51.90%\n",
      "Epoch 2/10, Train Accuracy: 62.85%\n",
      "Epoch 3/10, Train Accuracy: 69.75%\n",
      "Epoch 4/10, Train Accuracy: 73.10%\n",
      "Epoch 5/10, Train Accuracy: 76.30%\n",
      "Epoch 6/10, Train Accuracy: 78.60%\n",
      "Epoch 7/10, Train Accuracy: 80.10%\n",
      "Epoch 8/10, Train Accuracy: 82.10%\n",
      "Epoch 9/10, Train Accuracy: 82.90%\n",
      "Epoch 10/10, Train Accuracy: 84.15%\n",
      "Test Accuracy after Batch 5: 48.20%\n",
      "\n",
      "--- Training and Federated Learning after Batch 6 ---\n",
      "\n",
      "Training client 1 on Batch 6\n",
      "Epoch 1/10, Train Accuracy: 49.90%\n",
      "Epoch 2/10, Train Accuracy: 61.80%\n",
      "Epoch 3/10, Train Accuracy: 69.50%\n",
      "Epoch 4/10, Train Accuracy: 74.05%\n",
      "Epoch 5/10, Train Accuracy: 74.15%\n",
      "Epoch 6/10, Train Accuracy: 76.40%\n",
      "Epoch 7/10, Train Accuracy: 79.65%\n",
      "Epoch 8/10, Train Accuracy: 81.40%\n",
      "Epoch 9/10, Train Accuracy: 81.70%\n",
      "Epoch 10/10, Train Accuracy: 83.45%\n",
      "\n",
      "Training client 2 on Batch 6\n",
      "Epoch 1/10, Train Accuracy: 50.55%\n",
      "Epoch 2/10, Train Accuracy: 61.60%\n",
      "Epoch 3/10, Train Accuracy: 68.10%\n",
      "Epoch 4/10, Train Accuracy: 73.15%\n",
      "Epoch 5/10, Train Accuracy: 75.70%\n",
      "Epoch 6/10, Train Accuracy: 77.10%\n",
      "Epoch 7/10, Train Accuracy: 81.00%\n",
      "Epoch 8/10, Train Accuracy: 80.95%\n",
      "Epoch 9/10, Train Accuracy: 82.40%\n",
      "Epoch 10/10, Train Accuracy: 84.20%\n",
      "\n",
      "Training client 3 on Batch 6\n",
      "Epoch 1/10, Train Accuracy: 51.20%\n",
      "Epoch 2/10, Train Accuracy: 61.65%\n",
      "Epoch 3/10, Train Accuracy: 68.05%\n",
      "Epoch 4/10, Train Accuracy: 72.65%\n",
      "Epoch 5/10, Train Accuracy: 75.90%\n",
      "Epoch 6/10, Train Accuracy: 78.05%\n",
      "Epoch 7/10, Train Accuracy: 78.55%\n",
      "Epoch 8/10, Train Accuracy: 81.50%\n",
      "Epoch 9/10, Train Accuracy: 82.45%\n",
      "Epoch 10/10, Train Accuracy: 82.60%\n",
      "\n",
      "Training client 4 on Batch 6\n",
      "Epoch 1/10, Train Accuracy: 52.65%\n",
      "Epoch 2/10, Train Accuracy: 60.45%\n",
      "Epoch 3/10, Train Accuracy: 68.50%\n",
      "Epoch 4/10, Train Accuracy: 74.15%\n",
      "Epoch 5/10, Train Accuracy: 76.80%\n",
      "Epoch 6/10, Train Accuracy: 79.50%\n",
      "Epoch 7/10, Train Accuracy: 82.00%\n",
      "Epoch 8/10, Train Accuracy: 82.95%\n",
      "Epoch 9/10, Train Accuracy: 82.35%\n",
      "Epoch 10/10, Train Accuracy: 82.30%\n",
      "Test Accuracy after Batch 6: 48.50%\n",
      "\n",
      "--- Training and Federated Learning after Batch 7 ---\n",
      "\n",
      "Training client 1 on Batch 7\n",
      "Epoch 1/10, Train Accuracy: 52.50%\n",
      "Epoch 2/10, Train Accuracy: 62.25%\n",
      "Epoch 3/10, Train Accuracy: 68.20%\n",
      "Epoch 4/10, Train Accuracy: 72.80%\n",
      "Epoch 5/10, Train Accuracy: 75.75%\n",
      "Epoch 6/10, Train Accuracy: 76.10%\n",
      "Epoch 7/10, Train Accuracy: 78.05%\n",
      "Epoch 8/10, Train Accuracy: 81.00%\n",
      "Epoch 9/10, Train Accuracy: 81.70%\n",
      "Epoch 10/10, Train Accuracy: 83.15%\n",
      "\n",
      "Training client 2 on Batch 7\n",
      "Epoch 1/10, Train Accuracy: 52.70%\n",
      "Epoch 2/10, Train Accuracy: 64.05%\n",
      "Epoch 3/10, Train Accuracy: 67.40%\n",
      "Epoch 4/10, Train Accuracy: 72.40%\n",
      "Epoch 5/10, Train Accuracy: 75.80%\n",
      "Epoch 6/10, Train Accuracy: 77.80%\n",
      "Epoch 7/10, Train Accuracy: 80.90%\n",
      "Epoch 8/10, Train Accuracy: 81.05%\n",
      "Epoch 9/10, Train Accuracy: 83.75%\n",
      "Epoch 10/10, Train Accuracy: 82.55%\n",
      "\n",
      "Training client 3 on Batch 7\n",
      "Epoch 1/10, Train Accuracy: 53.75%\n",
      "Epoch 2/10, Train Accuracy: 64.60%\n",
      "Epoch 3/10, Train Accuracy: 70.05%\n",
      "Epoch 4/10, Train Accuracy: 72.00%\n",
      "Epoch 5/10, Train Accuracy: 75.80%\n",
      "Epoch 6/10, Train Accuracy: 78.85%\n",
      "Epoch 7/10, Train Accuracy: 81.35%\n",
      "Epoch 8/10, Train Accuracy: 81.40%\n",
      "Epoch 9/10, Train Accuracy: 83.00%\n",
      "Epoch 10/10, Train Accuracy: 84.95%\n",
      "\n",
      "Training client 4 on Batch 7\n",
      "Epoch 1/10, Train Accuracy: 53.55%\n",
      "Epoch 2/10, Train Accuracy: 62.05%\n",
      "Epoch 3/10, Train Accuracy: 69.50%\n",
      "Epoch 4/10, Train Accuracy: 72.85%\n",
      "Epoch 5/10, Train Accuracy: 76.05%\n",
      "Epoch 6/10, Train Accuracy: 77.10%\n",
      "Epoch 7/10, Train Accuracy: 80.60%\n",
      "Epoch 8/10, Train Accuracy: 81.60%\n",
      "Epoch 9/10, Train Accuracy: 82.20%\n",
      "Epoch 10/10, Train Accuracy: 82.15%\n",
      "Test Accuracy after Batch 7: 50.49%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Path where client models are saved\n",
    "saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_4clients/'\n",
    "\n",
    "# Model Preparation (ResNet18 for CIFAR-100)\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "# Function to load a model from a file\n",
    "def load_client_model(client_idx, save_dir):\n",
    "    model = prepare_model().to(device)\n",
    "    model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f'Loaded model for Client {client_idx} from {model_path}')\n",
    "    return model\n",
    "\n",
    "# Function to average client models (FL aggregation)\n",
    "def federated_averaging(state_dicts):\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "    return avg_state_dict\n",
    "\n",
    "# Training function for each client after receiving the global model\n",
    "def fine_tune_client(global_model, train_loader, val_loader, num_epochs=10):  \n",
    "    model = global_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Fine-tune the model for the client\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    return model.state_dict()  # Return the fine-tuned state_dict\n",
    "\n",
    "\n",
    "# Step 6: Federated Learning (FL) after each batch of Continual Learning (CL)\n",
    "def apply_federated_learning_after_each_batch(client_splits_cl, val_loaders, test_loader, num_clients=4, num_epochs=10):\n",
    "    # Initialize a global model\n",
    "    global_model = prepare_model().to(device)\n",
    "\n",
    "    # Iterate over each batch of CL for all clients\n",
    "    for batch_idx in range(len(client_splits_cl[0])):  \n",
    "        print(f'\\n--- Training and Federated Learning after Batch {batch_idx + 1} ---')\n",
    "\n",
    "        client_state_dicts = []\n",
    "\n",
    "        for client_idx in range(num_clients):\n",
    "            print(f'\\nTraining client {client_idx + 1} on Batch {batch_idx + 1}')\n",
    "\n",
    "            # Create DataLoader for the current batch\n",
    "            train_loader = DataLoader(client_splits_cl[client_idx][batch_idx], batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "            # Fine-tune client model with the current global model\n",
    "            client_state_dict = fine_tune_client(global_model, train_loader, val_loaders[client_idx], num_epochs=num_epochs)\n",
    "            client_state_dicts.append(client_state_dict)\n",
    "\n",
    "        # Perform federated averaging after this batch for all clients\n",
    "        avg_state_dict = federated_averaging(client_state_dicts)\n",
    "        global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "        # Optionally test the global model after each batch\n",
    "        test_accuracy = test_global_model(global_model, test_loader)\n",
    "        print(f'Test Accuracy after Batch {batch_idx + 1}: {test_accuracy:.2f}%')\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Load models saved after Continual Learning (CL)\n",
    "cl_models = [load_client_model(client_idx, saved_models_dir) for client_idx in range(1, 5)]\n",
    "\n",
    "# Apply Federated Learning after each batch and test the global model\n",
    "global_model = apply_federated_learning_after_each_batch(client_splits_cl, val_loaders, test_loader, num_clients=4, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step6C - FedAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model for Client 1 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_1_model.pth\n",
      "Loaded model for Client 2 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_2_model.pth\n",
      "Loaded model for Client 3 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_3_model.pth\n",
      "Loaded model for Client 4 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_4_model.pth\n",
      "Loaded model for Client 5 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_5_model.pth\n",
      "Loaded model for Client 6 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_6_model.pth\n",
      "Loaded model for Client 7 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_7_model.pth\n",
      "Loaded model for Client 8 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/client_8_model.pth\n",
      "\n",
      "--- Federated Learning Round 1 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.08%\n",
      "Epoch 2/3, Train Accuracy: 1.08%\n",
      "Epoch 3/3, Train Accuracy: 1.08%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/3, Train Accuracy: 0.84%\n",
      "Epoch 2/3, Train Accuracy: 0.84%\n",
      "Epoch 3/3, Train Accuracy: 0.84%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.01%\n",
      "Epoch 2/3, Train Accuracy: 1.01%\n",
      "Epoch 3/3, Train Accuracy: 1.01%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/3, Train Accuracy: 0.92%\n",
      "Epoch 2/3, Train Accuracy: 0.92%\n",
      "Epoch 3/3, Train Accuracy: 0.92%\n",
      "\n",
      "Training client 5 with the global model\n",
      "Epoch 1/3, Train Accuracy: 0.85%\n",
      "Epoch 2/3, Train Accuracy: 0.85%\n",
      "Epoch 3/3, Train Accuracy: 0.85%\n",
      "\n",
      "Training client 6 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.05%\n",
      "Epoch 2/3, Train Accuracy: 1.05%\n",
      "Epoch 3/3, Train Accuracy: 1.05%\n",
      "\n",
      "Training client 7 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.10%\n",
      "Epoch 2/3, Train Accuracy: 1.10%\n",
      "Epoch 3/3, Train Accuracy: 1.10%\n",
      "\n",
      "Training client 8 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.23%\n",
      "Epoch 2/3, Train Accuracy: 1.23%\n",
      "Epoch 3/3, Train Accuracy: 1.23%\n",
      "Test Accuracy after Round 1: 1.00%\n",
      "\n",
      "--- Federated Learning Round 2 ---\n",
      "\n",
      "Training client 1 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.08%\n",
      "Epoch 2/3, Train Accuracy: 1.08%\n",
      "Epoch 3/3, Train Accuracy: 1.08%\n",
      "\n",
      "Training client 2 with the global model\n",
      "Epoch 1/3, Train Accuracy: 0.84%\n",
      "Epoch 2/3, Train Accuracy: 0.84%\n",
      "Epoch 3/3, Train Accuracy: 0.84%\n",
      "\n",
      "Training client 3 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.01%\n",
      "Epoch 2/3, Train Accuracy: 1.01%\n",
      "Epoch 3/3, Train Accuracy: 1.01%\n",
      "\n",
      "Training client 4 with the global model\n",
      "Epoch 1/3, Train Accuracy: 0.92%\n",
      "Epoch 2/3, Train Accuracy: 0.92%\n",
      "Epoch 3/3, Train Accuracy: 0.92%\n",
      "\n",
      "Training client 5 with the global model\n",
      "Epoch 1/3, Train Accuracy: 0.85%\n",
      "Epoch 2/3, Train Accuracy: 0.85%\n",
      "Epoch 3/3, Train Accuracy: 0.85%\n",
      "\n",
      "Training client 6 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.05%\n",
      "Epoch 2/3, Train Accuracy: 1.05%\n",
      "Epoch 3/3, Train Accuracy: 1.05%\n",
      "\n",
      "Training client 7 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.10%\n",
      "Epoch 2/3, Train Accuracy: 1.10%\n",
      "Epoch 3/3, Train Accuracy: 1.10%\n",
      "\n",
      "Training client 8 with the global model\n",
      "Epoch 1/3, Train Accuracy: 1.23%\n",
      "Epoch 2/3, Train Accuracy: 1.23%\n",
      "Epoch 3/3, Train Accuracy: 1.23%\n",
      "Test Accuracy after Round 2: 1.00%\n",
      "\n",
      "--- Federated Learning Round 3 ---\n",
      "\n",
      "Training client 1 with the global model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 166\u001b[0m\n\u001b[1;32m    163\u001b[0m train_loaders, val_loaders \u001b[38;5;241m=\u001b[39m create_client_loaders(dataset_train, num_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Apply FL and test the global model\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m global_model \u001b[38;5;241m=\u001b[39m \u001b[43mapply_federated_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 131\u001b[0m, in \u001b[0;36mapply_federated_learning\u001b[0;34m(cl_models, train_loaders, val_loaders, test_loader, num_clients, num_epochs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clients):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining client \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the global model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m     client_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     client_state_dicts\u001b[38;5;241m.\u001b[39mappend(client_state_dict)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Federated optimization after each round using FedAdam\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 81\u001b[0m, in \u001b[0;36mfine_tune_client\u001b[0;34m(global_model, train_loader, val_loader, num_epochs, use_mixed_precision)\u001b[0m\n\u001b[1;32m     78\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     79\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     82\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     84\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# # Global device setting\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Path where client models are saved\n",
    "# saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/'\n",
    "\n",
    "# # Model Preparation (ResNet18 for CIFAR-100)\n",
    "# def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "#     \"\"\"Load a pre-trained ResNet18 model and modify it for CIFAR-100 with optional dropout.\"\"\"\n",
    "#     model = models.resnet18(pretrained=True)\n",
    "#     num_ftrs = model.fc.in_features\n",
    "#     if use_dropout:\n",
    "#         model.fc = nn.Sequential(\n",
    "#             nn.Dropout(p=dropout_prob),\n",
    "#             nn.Linear(num_ftrs, num_classes)\n",
    "#         )\n",
    "#     else:\n",
    "#         model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "#     return model\n",
    "\n",
    "# # Function to load a model from a file\n",
    "# def load_client_model(client_idx, save_dir):\n",
    "#     model = prepare_model().to(device)\n",
    "#     model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "#     try:\n",
    "#         model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#         print(f'Loaded model for Client {client_idx} from {model_path}')\n",
    "#     except FileNotFoundError:\n",
    "#         print(f'Error: Model for Client {client_idx} not found at {model_path}')\n",
    "#     return model\n",
    "\n",
    "# # Initialize optimizer state variables for FedAdam\n",
    "# optimizer_state = None\n",
    "# optimizer_variance = None\n",
    "# beta_1 = 0.8  # Adjusted momentum term\n",
    "# beta_2 = 0.9  # Adjusted variance term\n",
    "# epsilon = 1e-8\n",
    "# eta = 0.01  # Increased learning rate for server-side optimization\n",
    "\n",
    "# # Function for adaptive federated optimization (FedAdam)\n",
    "# def adaptive_federated_optimization(state_dicts, global_model, optimizer_state, optimizer_variance):\n",
    "#     global beta_1, beta_2, epsilon, eta\n",
    "\n",
    "#     # Initialize optimizer state and variance if not already done\n",
    "#     if optimizer_state is None or optimizer_variance is None:\n",
    "#         optimizer_state = {key: torch.zeros_like(param, device=device) for key, param in global_model.state_dict().items()}\n",
    "#         optimizer_variance = {key: torch.zeros_like(param, device=device) for key, param in global_model.state_dict().items()}\n",
    "\n",
    "#     avg_state_dict = {}\n",
    "\n",
    "#     # Compute federated optimization using the FedAdam algorithm\n",
    "#     for key in state_dicts[0].keys():\n",
    "#         # Compute the average of the updates (delta_w)\n",
    "#         delta_w = sum([state_dict[key] - global_model.state_dict()[key] for state_dict in state_dicts]) / len(state_dicts)\n",
    "        \n",
    "#         # Update optimizer state (momentum) and variance\n",
    "#         optimizer_state[key] = beta_1 * optimizer_state[key] + (1 - beta_1) * delta_w\n",
    "#         optimizer_variance[key] = beta_2 * optimizer_variance[key] + (1 - beta_2) * (delta_w ** 2)\n",
    "        \n",
    "#         # Compute bias-corrected updates\n",
    "#         m_hat = optimizer_state[key] / (1 - beta_1)\n",
    "#         v_hat = optimizer_variance[key] / (1 - beta_2)\n",
    "        \n",
    "#         # Apply the FedAdam update rule with numerical stability\n",
    "#         avg_state_dict[key] = global_model.state_dict()[key] - eta * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
    "\n",
    "#     return avg_state_dict, optimizer_state, optimizer_variance\n",
    "\n",
    "# # Training function for each client after receiving the global model\n",
    "# def fine_tune_client(global_model, train_loader, val_loader, num_epochs=3, use_mixed_precision=True):\n",
    "#     model = global_model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "#     # Optional: Mixed precision training\n",
    "#     scaler = GradScaler() if use_mixed_precision else None\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if use_mixed_precision:\n",
    "#                 with autocast():\n",
    "#                     outputs = model(inputs)\n",
    "#                     loss = criterion(outputs, labels)\n",
    "#             else:\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "            \n",
    "#             if use_mixed_precision:\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 scaler.step(optimizer)\n",
    "#                 scaler.update()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         train_accuracy = 100 * correct / total\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "#     return model.state_dict()\n",
    "\n",
    "# # Step 6: Federated Learning (FL) after Continual Learning (CL)\n",
    "# def apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader, num_clients=8, num_epochs=3):\n",
    "#     global global_model, optimizer_state, optimizer_variance\n",
    "\n",
    "#     global_model = prepare_model().to(device)\n",
    "\n",
    "#     # Collect state dicts from all clients after CL\n",
    "#     state_dicts = [model.state_dict() for model in cl_models]\n",
    "\n",
    "#     # Perform adaptive federated optimization (FedAdam)\n",
    "#     avg_state_dict, optimizer_state, optimizer_variance = adaptive_federated_optimization(state_dicts, global_model, optimizer_state, optimizer_variance)\n",
    "#     global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "#     for round in range(num_epochs):\n",
    "#         print(f'\\n--- Federated Learning Round {round + 1} ---')\n",
    "#         client_state_dicts = []\n",
    "\n",
    "#         for client_idx in range(num_clients):\n",
    "#             print(f'\\nTraining client {client_idx + 1} with the global model')\n",
    "#             client_state_dict = fine_tune_client(global_model, train_loaders[client_idx], val_loaders[client_idx], num_epochs=3)\n",
    "#             client_state_dicts.append(client_state_dict)\n",
    "\n",
    "#         # Federated optimization after each round using FedAdam\n",
    "#         avg_state_dict, optimizer_state, optimizer_variance = adaptive_federated_optimization(client_state_dicts, global_model, optimizer_state, optimizer_variance)\n",
    "#         global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "#         test_accuracy = test_global_model(global_model, test_loader)\n",
    "#         print(f'Test Accuracy after Round {round + 1}: {test_accuracy:.2f}%')\n",
    "\n",
    "#     return global_model\n",
    "\n",
    "# # Function to test the global model\n",
    "# def test_global_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "#     return test_accuracy\n",
    "\n",
    "# # Load models saved after CL\n",
    "# cl_models = [load_client_model(client_idx, saved_models_dir) for client_idx in range(1, 9)]\n",
    "\n",
    "# # Prepare DataLoader splits for each client with pin_memory=True and fewer workers\n",
    "# train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=8, batch_size=256)\n",
    "\n",
    "# # Apply FL and test the global model\n",
    "# global_model = apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCL with FedAvg, FedAdam, and Adaptive FedOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Global device setting\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Path where client models are saved\n",
    "saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/'\n",
    "\n",
    "# Model Preparation (ResNet18 for CIFAR-100)\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "# Function to load a model from a file\n",
    "def load_client_model(client_idx, save_dir):\n",
    "    model = prepare_model().to(device)\n",
    "    model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f'Loaded model for Client {client_idx} from {model_path}')\n",
    "    return model\n",
    "\n",
    "# Function to average client models (FedAvg aggregation)\n",
    "def federated_averaging(state_dicts):\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "    return avg_state_dict\n",
    "\n",
    "# Function for adaptive federated optimization (FedAdam)\n",
    "def fed_adam(state_dicts, global_model, optimizer_state, optimizer_variance, beta_1=0.9, beta_2=0.99, eta=0.01, epsilon=1e-8):\n",
    "    if optimizer_state is None or optimizer_variance is None:\n",
    "        optimizer_state = {key: torch.zeros_like(param) for key, param in global_model.state_dict().items()}\n",
    "        optimizer_variance = {key: torch.zeros_like(param) for key, param in global_model.state_dict().items()}\n",
    "    \n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        delta_w = sum([state_dict[key] - global_model.state_dict()[key] for state_dict in state_dicts]) / len(state_dicts)\n",
    "        optimizer_state[key] = beta_1 * optimizer_state[key] + (1 - beta_1) * delta_w\n",
    "        optimizer_variance[key] = beta_2 * optimizer_variance[key] + (1 - beta_2) * (delta_w ** 2)\n",
    "\n",
    "        m_hat = optimizer_state[key] / (1 - beta_1)\n",
    "        v_hat = optimizer_variance[key] / (1 - beta_2)\n",
    "        avg_state_dict[key] = global_model.state_dict()[key] - eta * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    return avg_state_dict, optimizer_state, optimizer_variance\n",
    "\n",
    "# Function for Adaptive Federated Optimization (AdapFedOpt)\n",
    "def adaptive_fedopt(state_dicts, global_model, learning_rates, beta_1=0.9, beta_2=0.99, epsilon=1e-8):\n",
    "    if learning_rates is None:\n",
    "        learning_rates = {key: torch.ones_like(param) * 0.01 for key, param in global_model.state_dict().items()}  # Initial learning rates\n",
    "\n",
    "    avg_state_dict = {}\n",
    "    optimizer_state = {key: torch.zeros_like(param) for key, param in global_model.state_dict().items()}\n",
    "    optimizer_variance = {key: torch.zeros_like(param) for key, param in global_model.state_dict().items()}\n",
    "    \n",
    "    for key in state_dicts[0].keys():\n",
    "        delta_w = sum([state_dict[key] - global_model.state_dict()[key] for state_dict in state_dicts]) / len(state_dicts)\n",
    "        optimizer_state[key] = beta_1 * optimizer_state[key] + (1 - beta_1) * delta_w\n",
    "        optimizer_variance[key] = beta_2 * optimizer_variance[key] + (1 - beta_2) * (delta_w ** 2)\n",
    "\n",
    "        m_hat = optimizer_state[key] / (1 - beta_1)\n",
    "        v_hat = optimizer_variance[key] / (1 - beta_2)\n",
    "        \n",
    "        # Use adaptive learning rate for each parameter\n",
    "        avg_state_dict[key] = global_model.state_dict()[key] - learning_rates[key] * m_hat / (torch.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        # Update learning rates for next round (can be adapted further as needed)\n",
    "        learning_rates[key] *= 0.9  # Reduce the learning rate over time as an example\n",
    "\n",
    "    return avg_state_dict, learning_rates\n",
    "\n",
    "# Training function for each client\n",
    "def fine_tune_client(global_model, train_loader, val_loader, num_epochs=3, use_mixed_precision=True):\n",
    "    model = global_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    scaler = GradScaler() if use_mixed_precision else None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if use_mixed_precision:\n",
    "                with autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            if use_mixed_precision:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    return model.state_dict()\n",
    "\n",
    "# Federated Learning (FL) after Continual Learning (CL)\n",
    "def apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader, num_clients=8, num_epochs=3, method='fedavg'):\n",
    "    global global_model, optimizer_state, optimizer_variance, learning_rates\n",
    "\n",
    "    global_model = prepare_model().to(device)\n",
    "\n",
    "    # Collect state dicts from all clients after CL\n",
    "    state_dicts = [model.state_dict() for model in cl_models]\n",
    "\n",
    "    # Initialize learning rates for Adaptive FedOpt\n",
    "    learning_rates = None if method != 'adaptive_fedopt' else {key: torch.ones_like(param) * 0.01 for key, param in global_model.state_dict().items()}\n",
    "\n",
    "    # Perform federated aggregation using the chosen method\n",
    "    if method == 'fedavg':\n",
    "        avg_state_dict = federated_averaging(state_dicts)\n",
    "    elif method == 'fedadam':\n",
    "        avg_state_dict, optimizer_state, optimizer_variance = fed_adam(state_dicts, global_model, optimizer_state, optimizer_variance)\n",
    "    elif method == 'adaptive_fedopt':\n",
    "        avg_state_dict, learning_rates = adaptive_fedopt(state_dicts, global_model, learning_rates)\n",
    "\n",
    "    global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "    for round in range(num_epochs):\n",
    "        print(f'\\n--- Federated Learning Round {round + 1} ---')\n",
    "        client_state_dicts = []\n",
    "\n",
    "        for client_idx in range(num_clients):\n",
    "            print(f'\\nTraining client {client_idx + 1} with the global model')\n",
    "            client_state_dict = fine_tune_client(global_model, train_loaders[client_idx], val_loaders[client_idx], num_epochs=3)\n",
    "            client_state_dicts.append(client_state_dict)\n",
    "\n",
    "        # Perform aggregation using the chosen method\n",
    "        if method == 'fedavg':\n",
    "            avg_state_dict = federated_averaging(client_state_dicts)\n",
    "        elif method == 'fedadam':\n",
    "            avg_state_dict, optimizer_state, optimizer_variance = fed_adam(client_state_dicts, global_model, optimizer_state, optimizer_variance)\n",
    "        elif method == 'adaptive_fedopt':\n",
    "            avg_state_dict, learning_rates = adaptive_fedopt(client_state_dicts, global_model, learning_rates)\n",
    "\n",
    "        global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "        test_accuracy = test_global_model(global_model, test_loader)\n",
    "        print(f'Test Accuracy after Round {round + 1}: {test_accuracy:.2f}%')\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Function to test the global model\n",
    "def test_global_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    return test_accuracy\n",
    "\n",
    "# Load models saved after CL\n",
    "cl_models = [load_client_model(client_idx, saved_models_dir) for client_idx in range(1, 9)]\n",
    "\n",
    "# Prepare DataLoader splits for each client with pin_memory=True and fewer workers\n",
    "train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=8, batch_size=256)\n",
    "\n",
    "# Apply FL and test the global model using one of the methods: 'fedavg', 'fedadam'\n",
    "global_model = apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader, method='fedadam')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
