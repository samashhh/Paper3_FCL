{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Dataset Size: 45000 images\n",
      "Validation Dataset Size: 5000 images\n",
      "Test Dataset Size: 10000 images\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setting up FL with 8 Clients\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformations for CIFAR-100\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset (train and test)\n",
    "dataset_train = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "dataset_test = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Total number of images after reserving 10% for validation (90% for training)\n",
    "num_train_images = int(0.9 * len(dataset_train))  # 45,000 images for training\n",
    "num_val_images = len(dataset_train) - num_train_images  # 5,000 images for validation\n",
    "\n",
    "# Randomly split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(dataset_train, [num_train_images, num_val_images])\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Output to check the size of the training, validation, and test sets\n",
    "print(f\"Training Dataset Size: {len(train_loader.dataset)} images\")\n",
    "print(f\"Validation Dataset Size: {len(val_loader.dataset)} images\")\n",
    "print(f\"Test Dataset Size: {len(test_loader.dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 Dataset Size: 5625 images\n",
      "Client 2 Dataset Size: 5625 images\n",
      "Client 3 Dataset Size: 5625 images\n",
      "Client 4 Dataset Size: 5625 images\n",
      "Client 5 Dataset Size: 5625 images\n",
      "Client 6 Dataset Size: 5625 images\n",
      "Client 7 Dataset Size: 5625 images\n",
      "Client 8 Dataset Size: 5625 images\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split the training dataset into 8 clients\n",
    "\n",
    "# Each client will get 5625 images\n",
    "client_splits = torch.utils.data.random_split(train_dataset, [5625] * 8)\n",
    "\n",
    "# Verify that each client has 5625 images\n",
    "for i, client_dataset in enumerate(client_splits):\n",
    "    print(f\"Client {i+1} Dataset Size: {len(client_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 First Batch - Images Shape: torch.Size([256, 3, 224, 224]), Labels Shape: torch.Size([256])\n",
      "Client 1 Dataset Size: 5625 images\n",
      "Client 2 Dataset Size: 5625 images\n",
      "Client 3 Dataset Size: 5625 images\n",
      "Client 4 Dataset Size: 5625 images\n",
      "Client 5 Dataset Size: 5625 images\n",
      "Client 6 Dataset Size: 5625 images\n",
      "Client 7 Dataset Size: 5625 images\n",
      "Client 8 Dataset Size: 5625 images\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create DataLoaders for each client\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "# Create a DataLoader for each client\n",
    "client_loaders = [DataLoader(client_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers) for client_dataset in client_splits]\n",
    "\n",
    "# Also create a DataLoader for the validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Output to verify one batch from Client 1\n",
    "images, labels = next(iter(client_loaders[0]))\n",
    "print(f\"Client 1 First Batch - Images Shape: {images.shape}, Labels Shape: {labels.shape}\")\n",
    "\n",
    "# Each client will get 5625 images\n",
    "client_splits = torch.utils.data.random_split(train_dataset, [5625] * 8)\n",
    "\n",
    "# Verify that each client has 5625 images\n",
    "for i, client_dataset in enumerate(client_splits):\n",
    "    print(f\"Client {i+1} Dataset Size: {len(client_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the training dataset into 16 clients\n",
    "\n",
    "# # Each client gets 2813 or 2812 images, reduce clients with 2813 images to 2812\n",
    "# for i, client_dataset in enumerate(client_splits):\n",
    "#     # If client has 2813 images, reduce it to 2812\n",
    "#     if len(client_dataset) == 2813:\n",
    "#         client_splits[i] = Subset(client_dataset, range(2812))\n",
    "\n",
    "# # Verify that each client now has exactly 2812 images\n",
    "# for i, client_dataset in enumerate(client_splits):\n",
    "#     print(f\"Client {i+1} Dataset Size: {len(client_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 - Batch 1: 1312 images\n",
      "Client 1 - Batch 2: 1000 images\n",
      "Client 1 - Batch 3: 1000 images\n",
      "Client 1 - Batch 4: 1000 images\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # New settings for 16 clients with 2812 images each\n",
    "# initial_split_size = 1312  # First batch size\n",
    "# num_incremental_batches = 3  # Number of incremental batches\n",
    "# new_data_per_batch = 500  # New images per incremental batch\n",
    "# replay_data_per_batch = 500  # Replay images per incremental batch\n",
    "\n",
    "# # Iterate over all clients to create incremental splits\n",
    "# client_splits_cl = []\n",
    "\n",
    "# for client_idx, client_dataset in enumerate(client_splits):\n",
    "#     # Initial split: 1312 images for the first batch, remaining for incremental batches\n",
    "#     remaining_split_size = len(client_dataset) - initial_split_size  # Should be 1500\n",
    "#     first_split, remaining_dataset = random_split(client_dataset, [initial_split_size, remaining_split_size])\n",
    "\n",
    "#     # Split the remaining dataset into 3 incremental batches of 500 new images each\n",
    "#     incremental_splits = random_split(remaining_dataset, [new_data_per_batch] * num_incremental_batches)\n",
    "    \n",
    "#     # Prepare the combined splits (replay strategy)\n",
    "#     seen_indices_list = list(first_split.indices)\n",
    "#     client_splits_for_cl = [first_split]  # First split goes in directly\n",
    "\n",
    "#     # Create the 3 subsequent batches with 500 new + 500 replay images\n",
    "#     for i, current_split in enumerate(incremental_splits):\n",
    "#         current_split_indices = list(current_split.indices)\n",
    "        \n",
    "#         # Select 500 previously seen images (replay strategy)\n",
    "#         previous_seen_indices = list(set(seen_indices_list) - set(current_split_indices))\n",
    "#         additional_data_indices = np.random.choice(previous_seen_indices, replay_data_per_batch, replace=False)\n",
    "        \n",
    "#         # Combine current split (500 new) and additional data (500 replay)\n",
    "#         combined_indices = np.concatenate([current_split_indices, additional_data_indices])\n",
    "        \n",
    "#         # Add current batch indices to seen indices list\n",
    "#         seen_indices_list.extend(current_split_indices)\n",
    "        \n",
    "#         # Create a subset for the current batch with new + replay data\n",
    "#         combined_split = Subset(client_dataset, combined_indices)\n",
    "#         client_splits_for_cl.append(combined_split)\n",
    "    \n",
    "#     client_splits_cl.append(client_splits_for_cl)\n",
    "\n",
    "# # Verify the incremental splits for Client 1\n",
    "# for i, split in enumerate(client_splits_cl[0]):\n",
    "#     print(f\"Client 1 - Batch {i+1}: {len(split)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Continual Learning for Client 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.6841, Train Accuracy: 1.37%, Val Loss: 4.6223, Val Accuracy: 1.86%\n",
      "Epoch 2/5, Train Loss: 4.3622, Train Accuracy: 5.95%, Val Loss: 4.4780, Val Accuracy: 4.16%\n",
      "Epoch 3/5, Train Loss: 4.1142, Train Accuracy: 11.13%, Val Loss: 4.3373, Val Accuracy: 6.86%\n",
      "Epoch 4/5, Train Loss: 3.9236, Train Accuracy: 17.91%, Val Loss: 4.1707, Val Accuracy: 9.42%\n",
      "Epoch 5/5, Train Loss: 3.6981, Train Accuracy: 20.43%, Val Loss: 4.0286, Val Accuracy: 11.66%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6990, Train Accuracy: 21.60%, Val Loss: 3.9337, Val Accuracy: 13.26%\n",
      "Epoch 2/5, Train Loss: 3.5556, Train Accuracy: 24.70%, Val Loss: 3.8664, Val Accuracy: 14.02%\n",
      "Epoch 3/5, Train Loss: 3.4217, Train Accuracy: 28.40%, Val Loss: 3.8002, Val Accuracy: 14.96%\n",
      "Epoch 4/5, Train Loss: 3.2608, Train Accuracy: 32.90%, Val Loss: 3.7161, Val Accuracy: 15.52%\n",
      "Epoch 5/5, Train Loss: 3.1224, Train Accuracy: 34.80%, Val Loss: 3.6615, Val Accuracy: 16.12%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2903, Train Accuracy: 31.00%, Val Loss: 3.5963, Val Accuracy: 17.76%\n",
      "Epoch 2/5, Train Loss: 3.1712, Train Accuracy: 33.40%, Val Loss: 3.5062, Val Accuracy: 19.38%\n",
      "Epoch 3/5, Train Loss: 3.0331, Train Accuracy: 37.70%, Val Loss: 3.4511, Val Accuracy: 20.08%\n",
      "Epoch 4/5, Train Loss: 2.9109, Train Accuracy: 38.60%, Val Loss: 3.4088, Val Accuracy: 20.30%\n",
      "Epoch 5/5, Train Loss: 2.7852, Train Accuracy: 41.60%, Val Loss: 3.3461, Val Accuracy: 21.58%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9304, Train Accuracy: 36.00%, Val Loss: 3.3164, Val Accuracy: 21.58%\n",
      "Epoch 2/5, Train Loss: 2.7975, Train Accuracy: 39.80%, Val Loss: 3.2490, Val Accuracy: 22.78%\n",
      "Epoch 3/5, Train Loss: 2.7051, Train Accuracy: 42.40%, Val Loss: 3.1877, Val Accuracy: 24.90%\n",
      "Epoch 4/5, Train Loss: 2.6103, Train Accuracy: 45.70%, Val Loss: 3.1463, Val Accuracy: 25.66%\n",
      "Epoch 5/5, Train Loss: 2.4805, Train Accuracy: 49.20%, Val Loss: 3.1271, Val Accuracy: 25.64%\n",
      "Testing model for Client 1 after Continual Learning...\n",
      "Test Accuracy: 25.58%\n",
      "Model for Client 1 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_1_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 2...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7402, Train Accuracy: 2.06%, Val Loss: 4.6151, Val Accuracy: 2.20%\n",
      "Epoch 2/5, Train Loss: 4.3620, Train Accuracy: 5.56%, Val Loss: 4.4737, Val Accuracy: 3.60%\n",
      "Epoch 3/5, Train Loss: 4.1270, Train Accuracy: 10.37%, Val Loss: 4.3338, Val Accuracy: 6.90%\n",
      "Epoch 4/5, Train Loss: 3.9171, Train Accuracy: 16.08%, Val Loss: 4.1781, Val Accuracy: 9.60%\n",
      "Epoch 5/5, Train Loss: 3.6849, Train Accuracy: 20.35%, Val Loss: 4.0193, Val Accuracy: 13.58%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6656, Train Accuracy: 23.10%, Val Loss: 3.9395, Val Accuracy: 14.86%\n",
      "Epoch 2/5, Train Loss: 3.5430, Train Accuracy: 25.50%, Val Loss: 3.8715, Val Accuracy: 14.76%\n",
      "Epoch 3/5, Train Loss: 3.3637, Train Accuracy: 29.60%, Val Loss: 3.8002, Val Accuracy: 15.20%\n",
      "Epoch 4/5, Train Loss: 3.2566, Train Accuracy: 31.60%, Val Loss: 3.7134, Val Accuracy: 16.92%\n",
      "Epoch 5/5, Train Loss: 3.1172, Train Accuracy: 35.30%, Val Loss: 3.6600, Val Accuracy: 17.82%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2691, Train Accuracy: 29.40%, Val Loss: 3.5928, Val Accuracy: 18.94%\n",
      "Epoch 2/5, Train Loss: 3.1254, Train Accuracy: 33.10%, Val Loss: 3.5139, Val Accuracy: 20.42%\n",
      "Epoch 3/5, Train Loss: 3.0085, Train Accuracy: 36.80%, Val Loss: 3.4584, Val Accuracy: 21.54%\n",
      "Epoch 4/5, Train Loss: 2.8847, Train Accuracy: 40.40%, Val Loss: 3.3780, Val Accuracy: 23.42%\n",
      "Epoch 5/5, Train Loss: 2.7497, Train Accuracy: 44.70%, Val Loss: 3.3681, Val Accuracy: 23.50%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 3.0087, Train Accuracy: 34.80%, Val Loss: 3.3005, Val Accuracy: 23.92%\n",
      "Epoch 2/5, Train Loss: 2.8656, Train Accuracy: 38.10%, Val Loss: 3.2473, Val Accuracy: 24.64%\n",
      "Epoch 3/5, Train Loss: 2.7894, Train Accuracy: 39.80%, Val Loss: 3.1958, Val Accuracy: 26.76%\n",
      "Epoch 4/5, Train Loss: 2.6254, Train Accuracy: 46.70%, Val Loss: 3.1559, Val Accuracy: 26.36%\n",
      "Epoch 5/5, Train Loss: 2.5409, Train Accuracy: 48.50%, Val Loss: 3.1252, Val Accuracy: 27.42%\n",
      "Testing model for Client 2 after Continual Learning...\n",
      "Test Accuracy: 26.43%\n",
      "Model for Client 2 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_2_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 3...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7619, Train Accuracy: 1.30%, Val Loss: 4.6260, Val Accuracy: 2.24%\n",
      "Epoch 2/5, Train Loss: 4.4043, Train Accuracy: 6.17%, Val Loss: 4.4825, Val Accuracy: 4.12%\n",
      "Epoch 3/5, Train Loss: 4.0922, Train Accuracy: 11.66%, Val Loss: 4.3388, Val Accuracy: 7.66%\n",
      "Epoch 4/5, Train Loss: 3.9596, Train Accuracy: 16.69%, Val Loss: 4.1890, Val Accuracy: 9.76%\n",
      "Epoch 5/5, Train Loss: 3.7647, Train Accuracy: 20.43%, Val Loss: 4.0457, Val Accuracy: 11.92%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6853, Train Accuracy: 22.70%, Val Loss: 3.9650, Val Accuracy: 13.34%\n",
      "Epoch 2/5, Train Loss: 3.5694, Train Accuracy: 25.80%, Val Loss: 3.8916, Val Accuracy: 14.12%\n",
      "Epoch 3/5, Train Loss: 3.4391, Train Accuracy: 28.40%, Val Loss: 3.7890, Val Accuracy: 16.24%\n",
      "Epoch 4/5, Train Loss: 3.3084, Train Accuracy: 32.70%, Val Loss: 3.7134, Val Accuracy: 17.82%\n",
      "Epoch 5/5, Train Loss: 3.2175, Train Accuracy: 33.10%, Val Loss: 3.6612, Val Accuracy: 18.24%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.3021, Train Accuracy: 30.70%, Val Loss: 3.5998, Val Accuracy: 18.52%\n",
      "Epoch 2/5, Train Loss: 3.2184, Train Accuracy: 32.20%, Val Loss: 3.5449, Val Accuracy: 19.20%\n",
      "Epoch 3/5, Train Loss: 3.0783, Train Accuracy: 37.50%, Val Loss: 3.4662, Val Accuracy: 20.42%\n",
      "Epoch 4/5, Train Loss: 2.9476, Train Accuracy: 40.20%, Val Loss: 3.4110, Val Accuracy: 21.18%\n",
      "Epoch 5/5, Train Loss: 2.8266, Train Accuracy: 42.10%, Val Loss: 3.3653, Val Accuracy: 22.22%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9473, Train Accuracy: 38.20%, Val Loss: 3.3309, Val Accuracy: 22.78%\n",
      "Epoch 2/5, Train Loss: 2.8327, Train Accuracy: 41.90%, Val Loss: 3.2764, Val Accuracy: 24.46%\n",
      "Epoch 3/5, Train Loss: 2.7552, Train Accuracy: 43.20%, Val Loss: 3.2148, Val Accuracy: 25.20%\n",
      "Epoch 4/5, Train Loss: 2.5947, Train Accuracy: 47.80%, Val Loss: 3.1796, Val Accuracy: 25.36%\n",
      "Epoch 5/5, Train Loss: 2.4951, Train Accuracy: 50.60%, Val Loss: 3.1537, Val Accuracy: 26.14%\n",
      "Testing model for Client 3 after Continual Learning...\n",
      "Test Accuracy: 25.20%\n",
      "Model for Client 3 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_3_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 4...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7097, Train Accuracy: 0.69%, Val Loss: 4.6100, Val Accuracy: 1.66%\n",
      "Epoch 2/5, Train Loss: 4.3494, Train Accuracy: 6.02%, Val Loss: 4.4568, Val Accuracy: 4.40%\n",
      "Epoch 3/5, Train Loss: 4.1214, Train Accuracy: 10.82%, Val Loss: 4.2897, Val Accuracy: 8.06%\n",
      "Epoch 4/5, Train Loss: 3.9214, Train Accuracy: 15.40%, Val Loss: 4.1504, Val Accuracy: 9.96%\n",
      "Epoch 5/5, Train Loss: 3.7212, Train Accuracy: 21.49%, Val Loss: 3.9941, Val Accuracy: 12.72%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.7258, Train Accuracy: 20.90%, Val Loss: 3.9216, Val Accuracy: 13.42%\n",
      "Epoch 2/5, Train Loss: 3.6112, Train Accuracy: 24.30%, Val Loss: 3.8412, Val Accuracy: 15.02%\n",
      "Epoch 3/5, Train Loss: 3.4731, Train Accuracy: 27.40%, Val Loss: 3.7787, Val Accuracy: 16.18%\n",
      "Epoch 4/5, Train Loss: 3.3320, Train Accuracy: 32.00%, Val Loss: 3.7157, Val Accuracy: 17.32%\n",
      "Epoch 5/5, Train Loss: 3.2015, Train Accuracy: 34.50%, Val Loss: 3.6621, Val Accuracy: 18.44%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2972, Train Accuracy: 30.40%, Val Loss: 3.5811, Val Accuracy: 19.58%\n",
      "Epoch 2/5, Train Loss: 3.2128, Train Accuracy: 33.80%, Val Loss: 3.4949, Val Accuracy: 20.74%\n",
      "Epoch 3/5, Train Loss: 3.0445, Train Accuracy: 37.60%, Val Loss: 3.4365, Val Accuracy: 21.90%\n",
      "Epoch 4/5, Train Loss: 2.9522, Train Accuracy: 40.70%, Val Loss: 3.3912, Val Accuracy: 22.98%\n",
      "Epoch 5/5, Train Loss: 2.8142, Train Accuracy: 43.90%, Val Loss: 3.3686, Val Accuracy: 23.20%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9349, Train Accuracy: 37.70%, Val Loss: 3.3226, Val Accuracy: 23.10%\n",
      "Epoch 2/5, Train Loss: 2.8412, Train Accuracy: 39.20%, Val Loss: 3.2861, Val Accuracy: 24.24%\n",
      "Epoch 3/5, Train Loss: 2.7083, Train Accuracy: 43.60%, Val Loss: 3.2365, Val Accuracy: 24.84%\n",
      "Epoch 4/5, Train Loss: 2.6059, Train Accuracy: 46.90%, Val Loss: 3.1845, Val Accuracy: 26.24%\n",
      "Epoch 5/5, Train Loss: 2.5238, Train Accuracy: 46.30%, Val Loss: 3.1688, Val Accuracy: 26.30%\n",
      "Testing model for Client 4 after Continual Learning...\n",
      "Test Accuracy: 26.02%\n",
      "Model for Client 4 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_4_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 5...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7477, Train Accuracy: 0.69%, Val Loss: 4.6586, Val Accuracy: 1.74%\n",
      "Epoch 2/5, Train Loss: 4.3924, Train Accuracy: 4.34%, Val Loss: 4.4978, Val Accuracy: 3.12%\n",
      "Epoch 3/5, Train Loss: 4.1662, Train Accuracy: 8.54%, Val Loss: 4.3442, Val Accuracy: 5.42%\n",
      "Epoch 4/5, Train Loss: 3.9396, Train Accuracy: 17.07%, Val Loss: 4.2126, Val Accuracy: 8.62%\n",
      "Epoch 5/5, Train Loss: 3.6990, Train Accuracy: 21.88%, Val Loss: 4.0669, Val Accuracy: 10.84%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6852, Train Accuracy: 24.00%, Val Loss: 3.9873, Val Accuracy: 12.20%\n",
      "Epoch 2/5, Train Loss: 3.5446, Train Accuracy: 26.20%, Val Loss: 3.9141, Val Accuracy: 12.38%\n",
      "Epoch 3/5, Train Loss: 3.4355, Train Accuracy: 29.10%, Val Loss: 3.8346, Val Accuracy: 13.00%\n",
      "Epoch 4/5, Train Loss: 3.2982, Train Accuracy: 32.40%, Val Loss: 3.7646, Val Accuracy: 14.18%\n",
      "Epoch 5/5, Train Loss: 3.1719, Train Accuracy: 34.10%, Val Loss: 3.6899, Val Accuracy: 16.70%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2765, Train Accuracy: 29.50%, Val Loss: 3.6299, Val Accuracy: 17.20%\n",
      "Epoch 2/5, Train Loss: 3.1463, Train Accuracy: 35.20%, Val Loss: 3.5820, Val Accuracy: 18.24%\n",
      "Epoch 3/5, Train Loss: 3.0214, Train Accuracy: 38.40%, Val Loss: 3.4859, Val Accuracy: 20.74%\n",
      "Epoch 4/5, Train Loss: 2.8937, Train Accuracy: 42.70%, Val Loss: 3.4515, Val Accuracy: 20.66%\n",
      "Epoch 5/5, Train Loss: 2.7937, Train Accuracy: 43.90%, Val Loss: 3.3916, Val Accuracy: 21.80%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9885, Train Accuracy: 35.20%, Val Loss: 3.3344, Val Accuracy: 22.66%\n",
      "Epoch 2/5, Train Loss: 2.8676, Train Accuracy: 40.20%, Val Loss: 3.2738, Val Accuracy: 23.62%\n",
      "Epoch 3/5, Train Loss: 2.7507, Train Accuracy: 41.70%, Val Loss: 3.2270, Val Accuracy: 24.58%\n",
      "Epoch 4/5, Train Loss: 2.6233, Train Accuracy: 45.30%, Val Loss: 3.1663, Val Accuracy: 25.70%\n",
      "Epoch 5/5, Train Loss: 2.5283, Train Accuracy: 51.40%, Val Loss: 3.1451, Val Accuracy: 25.78%\n",
      "Testing model for Client 5 after Continual Learning...\n",
      "Test Accuracy: 26.82%\n",
      "Model for Client 5 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_5_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 6...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7177, Train Accuracy: 0.76%, Val Loss: 4.6636, Val Accuracy: 1.88%\n",
      "Epoch 2/5, Train Loss: 4.3710, Train Accuracy: 3.05%, Val Loss: 4.4974, Val Accuracy: 3.86%\n",
      "Epoch 3/5, Train Loss: 4.1000, Train Accuracy: 11.13%, Val Loss: 4.3469, Val Accuracy: 6.76%\n",
      "Epoch 4/5, Train Loss: 3.8923, Train Accuracy: 17.61%, Val Loss: 4.1906, Val Accuracy: 9.46%\n",
      "Epoch 5/5, Train Loss: 3.6769, Train Accuracy: 21.04%, Val Loss: 4.0579, Val Accuracy: 11.94%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6924, Train Accuracy: 20.90%, Val Loss: 3.9884, Val Accuracy: 12.26%\n",
      "Epoch 2/5, Train Loss: 3.5843, Train Accuracy: 23.50%, Val Loss: 3.9010, Val Accuracy: 13.76%\n",
      "Epoch 3/5, Train Loss: 3.3839, Train Accuracy: 29.50%, Val Loss: 3.8286, Val Accuracy: 15.60%\n",
      "Epoch 4/5, Train Loss: 3.2626, Train Accuracy: 31.60%, Val Loss: 3.7319, Val Accuracy: 17.30%\n",
      "Epoch 5/5, Train Loss: 3.1315, Train Accuracy: 37.30%, Val Loss: 3.6852, Val Accuracy: 18.16%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2640, Train Accuracy: 31.00%, Val Loss: 3.6131, Val Accuracy: 18.96%\n",
      "Epoch 2/5, Train Loss: 3.1804, Train Accuracy: 33.80%, Val Loss: 3.5537, Val Accuracy: 20.00%\n",
      "Epoch 3/5, Train Loss: 3.0420, Train Accuracy: 36.50%, Val Loss: 3.4886, Val Accuracy: 20.90%\n",
      "Epoch 4/5, Train Loss: 2.8973, Train Accuracy: 41.40%, Val Loss: 3.4684, Val Accuracy: 20.76%\n",
      "Epoch 5/5, Train Loss: 2.8160, Train Accuracy: 42.30%, Val Loss: 3.4191, Val Accuracy: 21.38%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9448, Train Accuracy: 36.90%, Val Loss: 3.3598, Val Accuracy: 22.94%\n",
      "Epoch 2/5, Train Loss: 2.8724, Train Accuracy: 37.40%, Val Loss: 3.2807, Val Accuracy: 24.72%\n",
      "Epoch 3/5, Train Loss: 2.7535, Train Accuracy: 40.90%, Val Loss: 3.2374, Val Accuracy: 25.28%\n",
      "Epoch 4/5, Train Loss: 2.6242, Train Accuracy: 46.60%, Val Loss: 3.2055, Val Accuracy: 25.32%\n",
      "Epoch 5/5, Train Loss: 2.5040, Train Accuracy: 47.90%, Val Loss: 3.1771, Val Accuracy: 26.28%\n",
      "Testing model for Client 6 after Continual Learning...\n",
      "Test Accuracy: 26.20%\n",
      "Model for Client 6 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_6_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 7...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7353, Train Accuracy: 1.07%, Val Loss: 4.6292, Val Accuracy: 1.70%\n",
      "Epoch 2/5, Train Loss: 4.3885, Train Accuracy: 5.11%, Val Loss: 4.4677, Val Accuracy: 4.58%\n",
      "Epoch 3/5, Train Loss: 4.1012, Train Accuracy: 12.50%, Val Loss: 4.2989, Val Accuracy: 8.52%\n",
      "Epoch 4/5, Train Loss: 3.9044, Train Accuracy: 17.91%, Val Loss: 4.1508, Val Accuracy: 10.80%\n",
      "Epoch 5/5, Train Loss: 3.7014, Train Accuracy: 21.19%, Val Loss: 4.0171, Val Accuracy: 12.56%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6699, Train Accuracy: 20.70%, Val Loss: 3.9349, Val Accuracy: 13.56%\n",
      "Epoch 2/5, Train Loss: 3.5371, Train Accuracy: 23.90%, Val Loss: 3.8675, Val Accuracy: 14.52%\n",
      "Epoch 3/5, Train Loss: 3.4192, Train Accuracy: 26.20%, Val Loss: 3.7854, Val Accuracy: 15.60%\n",
      "Epoch 4/5, Train Loss: 3.2806, Train Accuracy: 30.50%, Val Loss: 3.7094, Val Accuracy: 16.44%\n",
      "Epoch 5/5, Train Loss: 3.1431, Train Accuracy: 36.20%, Val Loss: 3.6469, Val Accuracy: 17.70%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2559, Train Accuracy: 30.00%, Val Loss: 3.5761, Val Accuracy: 18.74%\n",
      "Epoch 2/5, Train Loss: 3.1617, Train Accuracy: 31.70%, Val Loss: 3.5478, Val Accuracy: 18.80%\n",
      "Epoch 3/5, Train Loss: 3.0191, Train Accuracy: 34.30%, Val Loss: 3.4669, Val Accuracy: 20.92%\n",
      "Epoch 4/5, Train Loss: 2.8924, Train Accuracy: 39.10%, Val Loss: 3.4313, Val Accuracy: 20.94%\n",
      "Epoch 5/5, Train Loss: 2.7939, Train Accuracy: 41.30%, Val Loss: 3.3718, Val Accuracy: 22.62%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9544, Train Accuracy: 36.60%, Val Loss: 3.3110, Val Accuracy: 23.50%\n",
      "Epoch 2/5, Train Loss: 2.8194, Train Accuracy: 40.20%, Val Loss: 3.2802, Val Accuracy: 24.16%\n",
      "Epoch 3/5, Train Loss: 2.7327, Train Accuracy: 45.00%, Val Loss: 3.2367, Val Accuracy: 24.28%\n",
      "Epoch 4/5, Train Loss: 2.5715, Train Accuracy: 45.20%, Val Loss: 3.1940, Val Accuracy: 24.96%\n",
      "Epoch 5/5, Train Loss: 2.4612, Train Accuracy: 50.00%, Val Loss: 3.1582, Val Accuracy: 25.88%\n",
      "Testing model for Client 7 after Continual Learning...\n",
      "Test Accuracy: 25.75%\n",
      "Model for Client 7 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_7_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 8...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7650, Train Accuracy: 0.99%, Val Loss: 4.6302, Val Accuracy: 1.80%\n",
      "Epoch 2/5, Train Loss: 4.4028, Train Accuracy: 5.56%, Val Loss: 4.4810, Val Accuracy: 3.86%\n",
      "Epoch 3/5, Train Loss: 4.1819, Train Accuracy: 11.51%, Val Loss: 4.3386, Val Accuracy: 6.90%\n",
      "Epoch 4/5, Train Loss: 3.9279, Train Accuracy: 16.92%, Val Loss: 4.1880, Val Accuracy: 10.32%\n",
      "Epoch 5/5, Train Loss: 3.7872, Train Accuracy: 20.27%, Val Loss: 4.0609, Val Accuracy: 12.00%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.7146, Train Accuracy: 22.50%, Val Loss: 3.9688, Val Accuracy: 13.64%\n",
      "Epoch 2/5, Train Loss: 3.6065, Train Accuracy: 24.60%, Val Loss: 3.9016, Val Accuracy: 13.72%\n",
      "Epoch 3/5, Train Loss: 3.4645, Train Accuracy: 28.20%, Val Loss: 3.8109, Val Accuracy: 15.98%\n",
      "Epoch 4/5, Train Loss: 3.3502, Train Accuracy: 28.80%, Val Loss: 3.7583, Val Accuracy: 16.04%\n",
      "Epoch 5/5, Train Loss: 3.1759, Train Accuracy: 34.70%, Val Loss: 3.6894, Val Accuracy: 16.82%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2967, Train Accuracy: 29.90%, Val Loss: 3.6144, Val Accuracy: 18.64%\n",
      "Epoch 2/5, Train Loss: 3.1917, Train Accuracy: 33.10%, Val Loss: 3.5310, Val Accuracy: 20.28%\n",
      "Epoch 3/5, Train Loss: 3.0678, Train Accuracy: 35.90%, Val Loss: 3.4625, Val Accuracy: 20.62%\n",
      "Epoch 4/5, Train Loss: 2.9493, Train Accuracy: 40.10%, Val Loss: 3.4153, Val Accuracy: 21.26%\n",
      "Epoch 5/5, Train Loss: 2.8312, Train Accuracy: 43.10%, Val Loss: 3.3590, Val Accuracy: 21.98%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9796, Train Accuracy: 36.00%, Val Loss: 3.3197, Val Accuracy: 23.40%\n",
      "Epoch 2/5, Train Loss: 2.8515, Train Accuracy: 40.20%, Val Loss: 3.2684, Val Accuracy: 24.00%\n",
      "Epoch 3/5, Train Loss: 2.7881, Train Accuracy: 41.50%, Val Loss: 3.2182, Val Accuracy: 25.10%\n",
      "Epoch 4/5, Train Loss: 2.6525, Train Accuracy: 43.70%, Val Loss: 3.1783, Val Accuracy: 26.52%\n",
      "Epoch 5/5, Train Loss: 2.5236, Train Accuracy: 48.70%, Val Loss: 3.1592, Val Accuracy: 25.64%\n",
      "Testing model for Client 8 after Continual Learning...\n",
      "Test Accuracy: 25.46%\n",
      "Model for Client 8 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_8_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 9...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7091, Train Accuracy: 1.37%, Val Loss: 4.5886, Val Accuracy: 2.58%\n",
      "Epoch 2/5, Train Loss: 4.3433, Train Accuracy: 6.55%, Val Loss: 4.4445, Val Accuracy: 4.74%\n",
      "Epoch 3/5, Train Loss: 4.1663, Train Accuracy: 10.90%, Val Loss: 4.2832, Val Accuracy: 7.64%\n",
      "Epoch 4/5, Train Loss: 3.9105, Train Accuracy: 15.93%, Val Loss: 4.1372, Val Accuracy: 9.70%\n",
      "Epoch 5/5, Train Loss: 3.7034, Train Accuracy: 22.10%, Val Loss: 4.0146, Val Accuracy: 11.80%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6651, Train Accuracy: 21.70%, Val Loss: 3.9467, Val Accuracy: 12.04%\n",
      "Epoch 2/5, Train Loss: 3.5365, Train Accuracy: 23.90%, Val Loss: 3.8638, Val Accuracy: 13.36%\n",
      "Epoch 3/5, Train Loss: 3.4116, Train Accuracy: 27.80%, Val Loss: 3.7847, Val Accuracy: 15.02%\n",
      "Epoch 4/5, Train Loss: 3.2784, Train Accuracy: 31.40%, Val Loss: 3.7101, Val Accuracy: 15.88%\n",
      "Epoch 5/5, Train Loss: 3.1707, Train Accuracy: 35.10%, Val Loss: 3.6399, Val Accuracy: 17.06%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2891, Train Accuracy: 29.80%, Val Loss: 3.5812, Val Accuracy: 18.16%\n",
      "Epoch 2/5, Train Loss: 3.1768, Train Accuracy: 33.30%, Val Loss: 3.5235, Val Accuracy: 18.78%\n",
      "Epoch 3/5, Train Loss: 3.0679, Train Accuracy: 36.40%, Val Loss: 3.4733, Val Accuracy: 19.66%\n",
      "Epoch 4/5, Train Loss: 2.9590, Train Accuracy: 39.10%, Val Loss: 3.4264, Val Accuracy: 20.84%\n",
      "Epoch 5/5, Train Loss: 2.8737, Train Accuracy: 39.40%, Val Loss: 3.3918, Val Accuracy: 20.62%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9422, Train Accuracy: 37.60%, Val Loss: 3.3493, Val Accuracy: 22.72%\n",
      "Epoch 2/5, Train Loss: 2.8821, Train Accuracy: 38.10%, Val Loss: 3.2728, Val Accuracy: 24.10%\n",
      "Epoch 3/5, Train Loss: 2.7412, Train Accuracy: 42.50%, Val Loss: 3.2331, Val Accuracy: 24.74%\n",
      "Epoch 4/5, Train Loss: 2.6382, Train Accuracy: 46.10%, Val Loss: 3.1655, Val Accuracy: 26.06%\n",
      "Epoch 5/5, Train Loss: 2.5158, Train Accuracy: 48.70%, Val Loss: 3.1506, Val Accuracy: 26.04%\n",
      "Testing model for Client 9 after Continual Learning...\n",
      "Test Accuracy: 26.53%\n",
      "Model for Client 9 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_9_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 10...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.6684, Train Accuracy: 2.13%, Val Loss: 4.5471, Val Accuracy: 2.32%\n",
      "Epoch 2/5, Train Loss: 4.3358, Train Accuracy: 7.24%, Val Loss: 4.3922, Val Accuracy: 5.38%\n",
      "Epoch 3/5, Train Loss: 4.0612, Train Accuracy: 13.34%, Val Loss: 4.2552, Val Accuracy: 8.22%\n",
      "Epoch 4/5, Train Loss: 3.8987, Train Accuracy: 17.76%, Val Loss: 4.1123, Val Accuracy: 9.48%\n",
      "Epoch 5/5, Train Loss: 3.5994, Train Accuracy: 23.32%, Val Loss: 3.9820, Val Accuracy: 11.62%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6218, Train Accuracy: 21.60%, Val Loss: 3.8927, Val Accuracy: 13.24%\n",
      "Epoch 2/5, Train Loss: 3.5003, Train Accuracy: 25.00%, Val Loss: 3.8006, Val Accuracy: 14.54%\n",
      "Epoch 3/5, Train Loss: 3.3455, Train Accuracy: 30.40%, Val Loss: 3.7075, Val Accuracy: 16.14%\n",
      "Epoch 4/5, Train Loss: 3.2117, Train Accuracy: 32.30%, Val Loss: 3.6634, Val Accuracy: 16.60%\n",
      "Epoch 5/5, Train Loss: 3.0663, Train Accuracy: 36.50%, Val Loss: 3.6133, Val Accuracy: 17.84%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2354, Train Accuracy: 31.00%, Val Loss: 3.5488, Val Accuracy: 19.56%\n",
      "Epoch 2/5, Train Loss: 3.0919, Train Accuracy: 34.30%, Val Loss: 3.5035, Val Accuracy: 19.96%\n",
      "Epoch 3/5, Train Loss: 2.9600, Train Accuracy: 37.90%, Val Loss: 3.4546, Val Accuracy: 20.84%\n",
      "Epoch 4/5, Train Loss: 2.8296, Train Accuracy: 42.00%, Val Loss: 3.4035, Val Accuracy: 22.48%\n",
      "Epoch 5/5, Train Loss: 2.7137, Train Accuracy: 44.80%, Val Loss: 3.3465, Val Accuracy: 22.62%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.8557, Train Accuracy: 39.10%, Val Loss: 3.2838, Val Accuracy: 24.72%\n",
      "Epoch 2/5, Train Loss: 2.7710, Train Accuracy: 41.10%, Val Loss: 3.2049, Val Accuracy: 26.34%\n",
      "Epoch 3/5, Train Loss: 2.6223, Train Accuracy: 46.10%, Val Loss: 3.1882, Val Accuracy: 25.50%\n",
      "Epoch 4/5, Train Loss: 2.5374, Train Accuracy: 47.80%, Val Loss: 3.1430, Val Accuracy: 26.88%\n",
      "Epoch 5/5, Train Loss: 2.4100, Train Accuracy: 53.00%, Val Loss: 3.1102, Val Accuracy: 28.16%\n",
      "Testing model for Client 10 after Continual Learning...\n",
      "Test Accuracy: 27.40%\n",
      "Model for Client 10 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_10_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 11...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7452, Train Accuracy: 0.84%, Val Loss: 4.6751, Val Accuracy: 1.28%\n",
      "Epoch 2/5, Train Loss: 4.4647, Train Accuracy: 2.97%, Val Loss: 4.5002, Val Accuracy: 3.86%\n",
      "Epoch 3/5, Train Loss: 4.2345, Train Accuracy: 10.44%, Val Loss: 4.3706, Val Accuracy: 5.48%\n",
      "Epoch 4/5, Train Loss: 4.0332, Train Accuracy: 16.62%, Val Loss: 4.2306, Val Accuracy: 7.32%\n",
      "Epoch 5/5, Train Loss: 3.7893, Train Accuracy: 20.73%, Val Loss: 4.1022, Val Accuracy: 9.78%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.7498, Train Accuracy: 21.60%, Val Loss: 4.0234, Val Accuracy: 11.88%\n",
      "Epoch 2/5, Train Loss: 3.6732, Train Accuracy: 23.10%, Val Loss: 3.9375, Val Accuracy: 12.98%\n",
      "Epoch 3/5, Train Loss: 3.5257, Train Accuracy: 25.30%, Val Loss: 3.8738, Val Accuracy: 14.50%\n",
      "Epoch 4/5, Train Loss: 3.3856, Train Accuracy: 32.70%, Val Loss: 3.7732, Val Accuracy: 15.84%\n",
      "Epoch 5/5, Train Loss: 3.2588, Train Accuracy: 35.20%, Val Loss: 3.7083, Val Accuracy: 16.52%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.3355, Train Accuracy: 31.90%, Val Loss: 3.6542, Val Accuracy: 17.44%\n",
      "Epoch 2/5, Train Loss: 3.2345, Train Accuracy: 34.50%, Val Loss: 3.5859, Val Accuracy: 19.08%\n",
      "Epoch 3/5, Train Loss: 3.0871, Train Accuracy: 39.10%, Val Loss: 3.5059, Val Accuracy: 20.46%\n",
      "Epoch 4/5, Train Loss: 2.9925, Train Accuracy: 41.40%, Val Loss: 3.4473, Val Accuracy: 21.42%\n",
      "Epoch 5/5, Train Loss: 2.8446, Train Accuracy: 43.40%, Val Loss: 3.4118, Val Accuracy: 22.10%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 3.0061, Train Accuracy: 35.00%, Val Loss: 3.3586, Val Accuracy: 23.36%\n",
      "Epoch 2/5, Train Loss: 2.9276, Train Accuracy: 36.00%, Val Loss: 3.3256, Val Accuracy: 22.96%\n",
      "Epoch 3/5, Train Loss: 2.7994, Train Accuracy: 39.70%, Val Loss: 3.2410, Val Accuracy: 24.96%\n",
      "Epoch 4/5, Train Loss: 2.6724, Train Accuracy: 45.40%, Val Loss: 3.2078, Val Accuracy: 25.94%\n",
      "Epoch 5/5, Train Loss: 2.5877, Train Accuracy: 47.40%, Val Loss: 3.1648, Val Accuracy: 26.22%\n",
      "Testing model for Client 11 after Continual Learning...\n",
      "Test Accuracy: 26.76%\n",
      "Model for Client 11 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_11_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 12...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7201, Train Accuracy: 1.45%, Val Loss: 4.6113, Val Accuracy: 2.26%\n",
      "Epoch 2/5, Train Loss: 4.3656, Train Accuracy: 4.88%, Val Loss: 4.4568, Val Accuracy: 4.26%\n",
      "Epoch 3/5, Train Loss: 4.1540, Train Accuracy: 9.98%, Val Loss: 4.2870, Val Accuracy: 7.64%\n",
      "Epoch 4/5, Train Loss: 3.9355, Train Accuracy: 16.54%, Val Loss: 4.1330, Val Accuracy: 9.16%\n",
      "Epoch 5/5, Train Loss: 3.6845, Train Accuracy: 20.50%, Val Loss: 4.0191, Val Accuracy: 11.16%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.7097, Train Accuracy: 20.90%, Val Loss: 3.9392, Val Accuracy: 12.52%\n",
      "Epoch 2/5, Train Loss: 3.6001, Train Accuracy: 22.70%, Val Loss: 3.8655, Val Accuracy: 13.46%\n",
      "Epoch 3/5, Train Loss: 3.4572, Train Accuracy: 25.60%, Val Loss: 3.7844, Val Accuracy: 15.02%\n",
      "Epoch 4/5, Train Loss: 3.2977, Train Accuracy: 31.00%, Val Loss: 3.7311, Val Accuracy: 15.96%\n",
      "Epoch 5/5, Train Loss: 3.1950, Train Accuracy: 33.70%, Val Loss: 3.6627, Val Accuracy: 16.62%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.3045, Train Accuracy: 28.10%, Val Loss: 3.6035, Val Accuracy: 18.50%\n",
      "Epoch 2/5, Train Loss: 3.1559, Train Accuracy: 32.90%, Val Loss: 3.5546, Val Accuracy: 18.56%\n",
      "Epoch 3/5, Train Loss: 3.0557, Train Accuracy: 36.10%, Val Loss: 3.4813, Val Accuracy: 19.60%\n",
      "Epoch 4/5, Train Loss: 2.9226, Train Accuracy: 38.00%, Val Loss: 3.4562, Val Accuracy: 19.74%\n",
      "Epoch 5/5, Train Loss: 2.8367, Train Accuracy: 39.50%, Val Loss: 3.4108, Val Accuracy: 20.34%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9619, Train Accuracy: 34.60%, Val Loss: 3.3687, Val Accuracy: 21.46%\n",
      "Epoch 2/5, Train Loss: 2.8784, Train Accuracy: 37.60%, Val Loss: 3.3164, Val Accuracy: 22.64%\n",
      "Epoch 3/5, Train Loss: 2.7544, Train Accuracy: 41.50%, Val Loss: 3.2687, Val Accuracy: 24.34%\n",
      "Epoch 4/5, Train Loss: 2.6090, Train Accuracy: 45.40%, Val Loss: 3.2302, Val Accuracy: 24.48%\n",
      "Epoch 5/5, Train Loss: 2.5068, Train Accuracy: 49.40%, Val Loss: 3.2248, Val Accuracy: 24.60%\n",
      "Testing model for Client 12 after Continual Learning...\n",
      "Test Accuracy: 24.55%\n",
      "Model for Client 12 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_12_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 13...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7572, Train Accuracy: 1.14%, Val Loss: 4.6422, Val Accuracy: 2.00%\n",
      "Epoch 2/5, Train Loss: 4.4591, Train Accuracy: 5.34%, Val Loss: 4.4677, Val Accuracy: 4.32%\n",
      "Epoch 3/5, Train Loss: 4.1768, Train Accuracy: 10.75%, Val Loss: 4.3145, Val Accuracy: 7.62%\n",
      "Epoch 4/5, Train Loss: 3.9575, Train Accuracy: 16.92%, Val Loss: 4.1730, Val Accuracy: 10.30%\n",
      "Epoch 5/5, Train Loss: 3.7239, Train Accuracy: 22.33%, Val Loss: 4.0420, Val Accuracy: 11.68%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6855, Train Accuracy: 20.80%, Val Loss: 3.9715, Val Accuracy: 12.34%\n",
      "Epoch 2/5, Train Loss: 3.5779, Train Accuracy: 22.70%, Val Loss: 3.8863, Val Accuracy: 14.62%\n",
      "Epoch 3/5, Train Loss: 3.4208, Train Accuracy: 27.30%, Val Loss: 3.8169, Val Accuracy: 14.58%\n",
      "Epoch 4/5, Train Loss: 3.2902, Train Accuracy: 31.50%, Val Loss: 3.7544, Val Accuracy: 15.96%\n",
      "Epoch 5/5, Train Loss: 3.1885, Train Accuracy: 33.30%, Val Loss: 3.6968, Val Accuracy: 16.46%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2807, Train Accuracy: 29.40%, Val Loss: 3.6233, Val Accuracy: 18.72%\n",
      "Epoch 2/5, Train Loss: 3.1772, Train Accuracy: 31.60%, Val Loss: 3.5492, Val Accuracy: 19.46%\n",
      "Epoch 3/5, Train Loss: 3.0554, Train Accuracy: 34.60%, Val Loss: 3.4859, Val Accuracy: 19.34%\n",
      "Epoch 4/5, Train Loss: 2.9116, Train Accuracy: 38.90%, Val Loss: 3.4634, Val Accuracy: 19.82%\n",
      "Epoch 5/5, Train Loss: 2.7997, Train Accuracy: 42.10%, Val Loss: 3.4126, Val Accuracy: 20.84%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.8995, Train Accuracy: 35.90%, Val Loss: 3.3435, Val Accuracy: 22.16%\n",
      "Epoch 2/5, Train Loss: 2.8243, Train Accuracy: 38.00%, Val Loss: 3.2884, Val Accuracy: 23.40%\n",
      "Epoch 3/5, Train Loss: 2.7218, Train Accuracy: 42.10%, Val Loss: 3.2469, Val Accuracy: 23.60%\n",
      "Epoch 4/5, Train Loss: 2.5822, Train Accuracy: 45.20%, Val Loss: 3.2179, Val Accuracy: 25.00%\n",
      "Epoch 5/5, Train Loss: 2.4859, Train Accuracy: 48.90%, Val Loss: 3.2005, Val Accuracy: 25.28%\n",
      "Testing model for Client 13 after Continual Learning...\n",
      "Test Accuracy: 25.45%\n",
      "Model for Client 13 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_13_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 14...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.6817, Train Accuracy: 0.91%, Val Loss: 4.6054, Val Accuracy: 1.70%\n",
      "Epoch 2/5, Train Loss: 4.3449, Train Accuracy: 6.25%, Val Loss: 4.4528, Val Accuracy: 4.56%\n",
      "Epoch 3/5, Train Loss: 4.1005, Train Accuracy: 12.65%, Val Loss: 4.2979, Val Accuracy: 7.90%\n",
      "Epoch 4/5, Train Loss: 3.8693, Train Accuracy: 17.61%, Val Loss: 4.1509, Val Accuracy: 9.58%\n",
      "Epoch 5/5, Train Loss: 3.6776, Train Accuracy: 22.48%, Val Loss: 4.0175, Val Accuracy: 11.90%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6255, Train Accuracy: 21.20%, Val Loss: 3.9471, Val Accuracy: 12.68%\n",
      "Epoch 2/5, Train Loss: 3.5417, Train Accuracy: 24.60%, Val Loss: 3.8630, Val Accuracy: 13.60%\n",
      "Epoch 3/5, Train Loss: 3.3712, Train Accuracy: 29.60%, Val Loss: 3.7808, Val Accuracy: 14.98%\n",
      "Epoch 4/5, Train Loss: 3.2177, Train Accuracy: 34.00%, Val Loss: 3.7119, Val Accuracy: 17.26%\n",
      "Epoch 5/5, Train Loss: 3.0943, Train Accuracy: 36.90%, Val Loss: 3.6328, Val Accuracy: 17.22%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2734, Train Accuracy: 29.60%, Val Loss: 3.5588, Val Accuracy: 18.92%\n",
      "Epoch 2/5, Train Loss: 3.1728, Train Accuracy: 32.90%, Val Loss: 3.4966, Val Accuracy: 20.16%\n",
      "Epoch 3/5, Train Loss: 3.0119, Train Accuracy: 36.50%, Val Loss: 3.4299, Val Accuracy: 21.56%\n",
      "Epoch 4/5, Train Loss: 2.9351, Train Accuracy: 39.20%, Val Loss: 3.3732, Val Accuracy: 22.60%\n",
      "Epoch 5/5, Train Loss: 2.8334, Train Accuracy: 41.60%, Val Loss: 3.3459, Val Accuracy: 22.62%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9125, Train Accuracy: 38.30%, Val Loss: 3.2870, Val Accuracy: 23.84%\n",
      "Epoch 2/5, Train Loss: 2.7819, Train Accuracy: 41.70%, Val Loss: 3.2379, Val Accuracy: 24.10%\n",
      "Epoch 3/5, Train Loss: 2.7230, Train Accuracy: 43.80%, Val Loss: 3.1843, Val Accuracy: 26.42%\n",
      "Epoch 4/5, Train Loss: 2.6013, Train Accuracy: 46.70%, Val Loss: 3.1502, Val Accuracy: 27.12%\n",
      "Epoch 5/5, Train Loss: 2.4687, Train Accuracy: 49.30%, Val Loss: 3.1224, Val Accuracy: 27.20%\n",
      "Testing model for Client 14 after Continual Learning...\n",
      "Test Accuracy: 27.17%\n",
      "Model for Client 14 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_14_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 15...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7079, Train Accuracy: 1.14%, Val Loss: 4.5860, Val Accuracy: 1.68%\n",
      "Epoch 2/5, Train Loss: 4.3483, Train Accuracy: 6.71%, Val Loss: 4.4587, Val Accuracy: 4.26%\n",
      "Epoch 3/5, Train Loss: 4.1360, Train Accuracy: 12.12%, Val Loss: 4.3107, Val Accuracy: 6.92%\n",
      "Epoch 4/5, Train Loss: 3.9285, Train Accuracy: 16.62%, Val Loss: 4.1646, Val Accuracy: 9.42%\n",
      "Epoch 5/5, Train Loss: 3.7309, Train Accuracy: 19.36%, Val Loss: 4.0238, Val Accuracy: 11.24%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6863, Train Accuracy: 21.60%, Val Loss: 3.9581, Val Accuracy: 11.50%\n",
      "Epoch 2/5, Train Loss: 3.5892, Train Accuracy: 23.50%, Val Loss: 3.8632, Val Accuracy: 13.58%\n",
      "Epoch 3/5, Train Loss: 3.4586, Train Accuracy: 26.30%, Val Loss: 3.7973, Val Accuracy: 14.36%\n",
      "Epoch 4/5, Train Loss: 3.2887, Train Accuracy: 30.50%, Val Loss: 3.7355, Val Accuracy: 14.82%\n",
      "Epoch 5/5, Train Loss: 3.1843, Train Accuracy: 32.70%, Val Loss: 3.6738, Val Accuracy: 16.46%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.3054, Train Accuracy: 26.70%, Val Loss: 3.5934, Val Accuracy: 17.60%\n",
      "Epoch 2/5, Train Loss: 3.1571, Train Accuracy: 30.40%, Val Loss: 3.5597, Val Accuracy: 17.36%\n",
      "Epoch 3/5, Train Loss: 3.0666, Train Accuracy: 33.30%, Val Loss: 3.5075, Val Accuracy: 18.30%\n",
      "Epoch 4/5, Train Loss: 2.9528, Train Accuracy: 35.50%, Val Loss: 3.4440, Val Accuracy: 19.46%\n",
      "Epoch 5/5, Train Loss: 2.8102, Train Accuracy: 42.20%, Val Loss: 3.3907, Val Accuracy: 21.12%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 3.0143, Train Accuracy: 35.50%, Val Loss: 3.3357, Val Accuracy: 22.18%\n",
      "Epoch 2/5, Train Loss: 2.9015, Train Accuracy: 37.60%, Val Loss: 3.2867, Val Accuracy: 23.42%\n",
      "Epoch 3/5, Train Loss: 2.7472, Train Accuracy: 40.70%, Val Loss: 3.2297, Val Accuracy: 23.72%\n",
      "Epoch 4/5, Train Loss: 2.6149, Train Accuracy: 45.90%, Val Loss: 3.1959, Val Accuracy: 24.44%\n",
      "Epoch 5/5, Train Loss: 2.5183, Train Accuracy: 48.50%, Val Loss: 3.1555, Val Accuracy: 25.98%\n",
      "Testing model for Client 15 after Continual Learning...\n",
      "Test Accuracy: 26.03%\n",
      "Model for Client 15 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_15_model.pth\n",
      "\n",
      "Starting Continual Learning for Client 16...\n",
      "Training on Batch 1/4\n",
      "Epoch 1/5, Train Loss: 4.7825, Train Accuracy: 1.07%, Val Loss: 4.6226, Val Accuracy: 1.40%\n",
      "Epoch 2/5, Train Loss: 4.4422, Train Accuracy: 4.42%, Val Loss: 4.4646, Val Accuracy: 3.54%\n",
      "Epoch 3/5, Train Loss: 4.1810, Train Accuracy: 9.53%, Val Loss: 4.3094, Val Accuracy: 7.60%\n",
      "Epoch 4/5, Train Loss: 3.9471, Train Accuracy: 17.91%, Val Loss: 4.1598, Val Accuracy: 9.88%\n",
      "Epoch 5/5, Train Loss: 3.7741, Train Accuracy: 21.42%, Val Loss: 4.0150, Val Accuracy: 11.92%\n",
      "Training on Batch 2/4\n",
      "Epoch 1/5, Train Loss: 3.6898, Train Accuracy: 22.10%, Val Loss: 3.9221, Val Accuracy: 13.08%\n",
      "Epoch 2/5, Train Loss: 3.5985, Train Accuracy: 23.90%, Val Loss: 3.8591, Val Accuracy: 14.22%\n",
      "Epoch 3/5, Train Loss: 3.4555, Train Accuracy: 26.10%, Val Loss: 3.7947, Val Accuracy: 15.18%\n",
      "Epoch 4/5, Train Loss: 3.3305, Train Accuracy: 29.50%, Val Loss: 3.7239, Val Accuracy: 16.18%\n",
      "Epoch 5/5, Train Loss: 3.1664, Train Accuracy: 36.00%, Val Loss: 3.6784, Val Accuracy: 16.78%\n",
      "Training on Batch 3/4\n",
      "Epoch 1/5, Train Loss: 3.2727, Train Accuracy: 29.60%, Val Loss: 3.6069, Val Accuracy: 17.84%\n",
      "Epoch 2/5, Train Loss: 3.1590, Train Accuracy: 32.80%, Val Loss: 3.5345, Val Accuracy: 19.08%\n",
      "Epoch 3/5, Train Loss: 3.0464, Train Accuracy: 36.30%, Val Loss: 3.4701, Val Accuracy: 19.86%\n",
      "Epoch 4/5, Train Loss: 2.8858, Train Accuracy: 41.40%, Val Loss: 3.4217, Val Accuracy: 21.10%\n",
      "Epoch 5/5, Train Loss: 2.8188, Train Accuracy: 41.70%, Val Loss: 3.3554, Val Accuracy: 21.22%\n",
      "Training on Batch 4/4\n",
      "Epoch 1/5, Train Loss: 2.9637, Train Accuracy: 35.70%, Val Loss: 3.2921, Val Accuracy: 23.26%\n",
      "Epoch 2/5, Train Loss: 2.8794, Train Accuracy: 38.30%, Val Loss: 3.2339, Val Accuracy: 25.22%\n",
      "Epoch 3/5, Train Loss: 2.7517, Train Accuracy: 42.90%, Val Loss: 3.2066, Val Accuracy: 25.84%\n",
      "Epoch 4/5, Train Loss: 2.6204, Train Accuracy: 46.90%, Val Loss: 3.1744, Val Accuracy: 26.58%\n",
      "Epoch 5/5, Train Loss: 2.5123, Train Accuracy: 48.90%, Val Loss: 3.1527, Val Accuracy: 26.24%\n",
      "Testing model for Client 16 after Continual Learning...\n",
      "Test Accuracy: 26.04%\n",
      "Model for Client 16 saved at /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_16_model.pth\n",
      "Continual Learning, testing, and saving models for all clients completed.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Step 5: Train each client with Continual Learning (CL), test after training, and save models\n",
    "\n",
    "# from torchvision import models\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Model Preparation Function (ResNet18 for CIFAR-100)\n",
    "# def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "#     \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "#     model = models.resnet18(pretrained=True)\n",
    "#     num_ftrs = model.fc.in_features\n",
    "#     if use_dropout:\n",
    "#         model.fc = nn.Sequential(\n",
    "#             nn.Dropout(p=dropout_prob),\n",
    "#             nn.Linear(num_ftrs, num_classes)\n",
    "#         )\n",
    "#     else:\n",
    "#         model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "#     return model\n",
    "\n",
    "# # EarlyStopping class\n",
    "# class EarlyStopping:\n",
    "#     def __init__(self, patience=5, min_delta=0):\n",
    "#         self.patience = patience\n",
    "#         self.min_delta = min_delta\n",
    "#         self.counter = 0\n",
    "#         self.best_loss = None\n",
    "#         self.early_stop = False\n",
    "\n",
    "#     def __call__(self, val_loss):\n",
    "#         if self.best_loss is None:\n",
    "#             self.best_loss = val_loss\n",
    "#         elif val_loss > self.best_loss - self.min_delta:\n",
    "#             self.counter += 1\n",
    "#             if self.counter >= self.patience:\n",
    "#                 self.early_stop = True\n",
    "#         else:\n",
    "#             self.best_loss = val_loss\n",
    "#             self.counter = 0\n",
    "\n",
    "# # Training function for Continual Learning\n",
    "# def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, patience=5, min_delta=0):\n",
    "#     early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "\n",
    "#         for inputs, labels in train_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         train_loss = running_loss / len(train_loader)\n",
    "#         train_accuracy = 100 * correct / total\n",
    "\n",
    "#         # Validation phase\n",
    "#         model.eval()\n",
    "#         val_running_loss = 0.0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in val_loader:\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 val_running_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs.data, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#         val_loss = val_running_loss / len(val_loader)\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "#               f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "#               f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "#         # Early stopping\n",
    "#         early_stopping(val_loss)\n",
    "#         if early_stopping.early_stop:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Testing function to evaluate the model on the test set\n",
    "# def test_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     test_correct = 0\n",
    "#     test_total = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in test_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             test_total += labels.size(0)\n",
    "#             test_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "#     test_accuracy = 100 * test_correct / test_total\n",
    "#     print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# # Save function\n",
    "# def save_model(model, client_idx, save_dir):\n",
    "#     if not os.path.exists(save_dir):\n",
    "#         os.makedirs(save_dir)\n",
    "#     model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "#     torch.save(model.state_dict(), model_path)\n",
    "#     print(f'Model for Client {client_idx} saved at {model_path}')\n",
    "\n",
    "# # Prepare the training and validation loaders for each client\n",
    "# batch_size = 256\n",
    "# num_workers = 4\n",
    "# num_epochs = 5  # Set the number of epochs for each CL round\n",
    "\n",
    "# # Define the directory to save the models (for 16 clients)\n",
    "# save_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/'\n",
    "\n",
    "# # Iterate over all clients and perform incremental training (CL)\n",
    "# for client_idx, client_batches in enumerate(client_splits_cl):\n",
    "#     print(f\"\\nStarting Continual Learning for Client {client_idx + 1}...\")\n",
    "    \n",
    "#     # Initialize a new model for each client\n",
    "#     model = prepare_model().to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#     # Train on each batch incrementally\n",
    "#     for batch_idx, batch in enumerate(client_batches):\n",
    "#         print(f\"Training on Batch {batch_idx + 1}/{len(client_batches)}\")\n",
    "        \n",
    "#         # Create DataLoader for the current batch\n",
    "#         train_loader = DataLoader(batch, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "        \n",
    "#         # Use the same validation set for all batches\n",
    "#         model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs)\n",
    "    \n",
    "#     # After training on all batches, test the model on the test set\n",
    "#     print(f\"Testing model for Client {client_idx + 1} after Continual Learning...\")\n",
    "#     test_model(model, test_loader)\n",
    "\n",
    "#     # Save the model for each client in the 16 clients directory\n",
    "#     save_model(model, client_idx + 1, save_dir)\n",
    "\n",
    "# print(\"Continual Learning, testing, and saving models for all clients completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Function to create DataLoaders for each client\n",
    "def create_client_loaders(dataset, num_clients=16, batch_size=256, val_split=0.1):\n",
    "    \"\"\"Split dataset into `num_clients` parts and create train/validation loaders.\"\"\"\n",
    "    client_loaders = []\n",
    "    val_loaders = []\n",
    "    \n",
    "    # Split the dataset randomly into `num_clients` equal parts\n",
    "    client_datasets = random_split(dataset, [len(dataset) // num_clients] * num_clients)\n",
    "    \n",
    "    for client_dataset in client_datasets:\n",
    "        # Further split each client's dataset into train and validation sets\n",
    "        train_size = int((1 - val_split) * len(client_dataset))\n",
    "        val_size = len(client_dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(client_dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create DataLoaders for the client\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        client_loaders.append(train_loader)\n",
    "        val_loaders.append(val_loader)\n",
    "    \n",
    "    return client_loaders, val_loaders\n",
    "\n",
    "# Assuming `dataset_train` is your training dataset\n",
    "train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/raid/home/somayeh.shami/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model for Client 1 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_1_model.pth\n",
      "Loaded model for Client 2 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_2_model.pth\n",
      "Loaded model for Client 3 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_3_model.pth\n",
      "Loaded model for Client 4 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_4_model.pth\n",
      "Loaded model for Client 5 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_5_model.pth\n",
      "Loaded model for Client 6 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_6_model.pth\n",
      "Loaded model for Client 7 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_7_model.pth\n",
      "Loaded model for Client 8 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_8_model.pth\n",
      "Loaded model for Client 9 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_9_model.pth\n",
      "Loaded model for Client 10 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_10_model.pth\n",
      "Loaded model for Client 11 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_11_model.pth\n",
      "Loaded model for Client 12 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_12_model.pth\n",
      "Loaded model for Client 13 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_13_model.pth\n",
      "Loaded model for Client 14 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_14_model.pth\n",
      "Loaded model for Client 15 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_15_model.pth\n",
      "Loaded model for Client 16 from /raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/client_16_model.pth\n",
      "\n",
      "--- Federated Learning Round 1 ---\n",
      "\n",
      "Training client 1 with the global model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 134\u001b[0m\n\u001b[1;32m    131\u001b[0m train_loaders, val_loaders \u001b[38;5;241m=\u001b[39m create_client_loaders(dataset_train, num_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Apply Federated Learning and test the global model\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m global_model \u001b[38;5;241m=\u001b[39m \u001b[43mapply_federated_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 100\u001b[0m, in \u001b[0;36mapply_federated_learning\u001b[0;34m(cl_models, train_loaders, val_loaders, test_loader, num_clients, num_epochs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m client_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_clients):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining client \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with the global model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     client_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tune_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     client_state_dicts\u001b[38;5;241m.\u001b[39mappend(client_state_dict)\n\u001b[1;32m    103\u001b[0m avg_state_dict \u001b[38;5;241m=\u001b[39m federated_averaging(client_state_dicts)\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mfine_tune_client\u001b[0;34m(global_model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     47\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     48\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     51\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:362\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 362\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:917\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m     )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[0;32m--> 917\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    920\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Path where client models are saved\n",
    "saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models_16clients/'\n",
    "\n",
    "# Model Preparation (ResNet18 for CIFAR-100)\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "# Function to load a model from a file\n",
    "def load_client_model(client_idx, save_dir):\n",
    "    model = prepare_model().to(device)\n",
    "    model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f'Loaded model for Client {client_idx} from {model_path}')\n",
    "    return model\n",
    "\n",
    "# Function to average client models (FL aggregation)\n",
    "def federated_averaging(state_dicts):\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "    return avg_state_dict\n",
    "\n",
    "# Training function for each client after receiving the global model\n",
    "def fine_tune_client(global_model, train_loader, val_loader, num_epochs=5):  # Reduced to 5 epochs to save time\n",
    "    model = global_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Validation phase after each epoch\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    return model.state_dict()  # Return the fine-tuned state_dict\n",
    "\n",
    "# Step 6: Federated Learning (FL) after Continual Learning (CL)\n",
    "def apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader, num_clients=8, num_epochs=10):  # Updated for 16 clients\n",
    "    global_model = prepare_model().to(device)\n",
    "\n",
    "    # Collect state dicts (trained weights) from all client models\n",
    "    state_dicts = [model.state_dict() for model in cl_models]\n",
    "\n",
    "    # Perform federated averaging to create a global model\n",
    "    avg_state_dict = federated_averaging(state_dicts)\n",
    "    global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "    for round in range(num_epochs):\n",
    "        print(f'\\n--- Federated Learning Round {round + 1} ---')\n",
    "        client_state_dicts = []\n",
    "\n",
    "        for client_idx in range(num_clients):\n",
    "            print(f'\\nTraining client {client_idx + 1} with the global model')\n",
    "            client_state_dict = fine_tune_client(global_model, train_loaders[client_idx], val_loaders[client_idx], num_epochs=5)\n",
    "            client_state_dicts.append(client_state_dict)\n",
    "\n",
    "        avg_state_dict = federated_averaging(client_state_dicts)\n",
    "        global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "        test_accuracy = test_global_model(global_model, test_loader)\n",
    "        print(f'Test Accuracy after Round {round + 1}: {test_accuracy:.2f}%')\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Function to test the global model\n",
    "def test_global_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    return test_accuracy\n",
    "\n",
    "# Load the models saved after Continual Learning (CL)\n",
    "cl_models = [load_client_model(client_idx, saved_models_dir) for client_idx in range(1, 17)]  #  clients\n",
    "\n",
    "# Prepare the dataset splits for each client\n",
    "train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=8)\n",
    "\n",
    "# Apply Federated Learning and test the global model\n",
    "global_model = apply_federated_learning(cl_models, train_loaders, val_loaders, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Path where client models are saved\n",
    "saved_models_dir = '/raid/home/somayeh.shami/project/somayeh_workspace/federated_learning/cl_models/'\n",
    "\n",
    "# Model Preparation (ResNet18 for CIFAR-100)\n",
    "def prepare_model(num_classes=100, use_dropout=False, dropout_prob=0.2):\n",
    "    \"\"\"Load a pre-trained Resnet18 model and modify it for CIFAR100 with optional dropout.\"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    if use_dropout:\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "    else:\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model\n",
    "\n",
    "# Function to load a model from a file\n",
    "def load_client_model(client_idx, save_dir):\n",
    "    model = prepare_model().to(device)\n",
    "    model_path = os.path.join(save_dir, f'client_{client_idx}_model.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f'Loaded model for Client {client_idx} from {model_path}')\n",
    "    return model\n",
    "\n",
    "# Function to average client models (FL aggregation)\n",
    "def federated_averaging(state_dicts):\n",
    "    avg_state_dict = {}\n",
    "    for key in state_dicts[0].keys():\n",
    "        avg_state_dict[key] = sum(state_dict[key] for state_dict in state_dicts) / len(state_dicts)\n",
    "    return avg_state_dict\n",
    "\n",
    "# Training function for each client after receiving the global model\n",
    "def fine_tune_client(global_model, train_loader, val_loader, num_epochs=10):\n",
    "    model = global_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "    # Fine-tune the model for the client\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    return model.state_dict()  # Return the fine-tuned state_dict\n",
    "\n",
    "# Step 6: Federated Learning (FL) after each round of Continual Learning (CL)\n",
    "def apply_federated_learning_after_each_batch(client_splits_cl, num_clients=8, num_epochs=10):\n",
    "    # Initialize a global model\n",
    "    global_model = prepare_model().to(device)\n",
    "\n",
    "    for batch_idx in range(len(client_splits_cl[0])):  # Iterate over batches\n",
    "        print(f'\\n--- Training and Federated Learning after Batch {batch_idx + 1} ---')\n",
    "        \n",
    "        client_state_dicts = []\n",
    "\n",
    "        for client_idx in range(num_clients):\n",
    "            print(f'\\nTraining client {client_idx + 1} on Batch {batch_idx + 1}')\n",
    "            \n",
    "            # Create DataLoader for the current batch\n",
    "            train_loader = DataLoader(client_splits_cl[client_idx][batch_idx], batch_size=256, shuffle=True)\n",
    "            \n",
    "            # Fine-tune client model with the current global model\n",
    "            client_state_dict = fine_tune_client(global_model, train_loader, val_loader, num_epochs=10)\n",
    "            client_state_dicts.append(client_state_dict)\n",
    "\n",
    "        # Perform federated averaging after this batch for all clients\n",
    "        avg_state_dict = federated_averaging(client_state_dicts)\n",
    "        global_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "        # Optionally test the global model after each batch\n",
    "        test_accuracy = test_global_model(global_model, test_loader)\n",
    "        print(f'Test Accuracy after Batch {batch_idx + 1}: {test_accuracy:.2f}%')\n",
    "\n",
    "    return global_model\n",
    "\n",
    "\n",
    "# Function to test the global model\n",
    "def test_global_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * test_correct / test_total\n",
    "    return test_accuracy\n",
    "\n",
    "# Load the models saved after Continual Learning (CL)\n",
    "cl_models = [load_client_model(client_idx, saved_models_dir) for client_idx in range(1, 9)]\n",
    "\n",
    "# Prepare the dataset splits for each client\n",
    "train_loaders, val_loaders = create_client_loaders(dataset_train, num_clients=8)\n",
    "\n",
    "# Apply Federated Learning and test the global model\n",
    "global_model = apply_federated_learning_after_each_batch(client_splits_cl, num_clients=8, num_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
